{"posts":[{"title":"About Me","text":"Hello, and welcome to my blog. An introduction about myself is written here with my scarce English vocabulary. I am a 20-year-old college student majoring in Computer Science who dreamed about designing a computer architecture that would contribute to the development of advanced technology. I once was a Windows user but was obsessed with Linux immediately after my very first installation of Ubuntu for my scientific research requirement. After about 3 months, when I learnt about the architecture and components of computers as well as the Operating System in school, I started to switch from Ubuntu to Arch Linux, which, until now, is my first choice every time I turn the power on. My Arch Linux & Dwm I said I was passionate and wanted to design computer architectures. However, as a college student with limited choices of topic and professors, the project that I have conducted is related to another area, UAV (unmanned aerial vehicle) swam control, in which we proposed a de-centralised method and algorithm that enable a group of drones to explore an unknown environment and make decisions based on the algorithm that is embedded in every individual. Right now, I am involved in a project that develops a system with multiple technologies, which, from my point of view, shall be better for industrial rather than for scientific purposes since the idea and method we conducted are nothing special but rather old-school while the system that we design is somehow new to the industrial world to the best of our knowledge. All those experience of ‘doing lab’ and ‘debuging’ seems, from my point of view, to be a scientific skill training. That is, the knowledge is not the centric but the habit and lessons related to the insight of ‘doing lab’ is what I gains through it. Other experiences includes ACM and Web Backend Development will be written later.","link":"/2022/11/12/aboutme/"},{"title":"Deploy Hadoop with VirtualBox","text":"This post is a reminder of hadoop deployment on Arch Linux and virtualbox (with vagrant) Related Github Repo contains resource: system information Pre-Requirement VirtualBox vagrant virtualbox and vagrant Access to Internet The base OS we choose to deploy Hadoop Cluster is Ubuntu 16, thus we should be able to fetch “ubuntu/xenial64” from vagrant cloud. Also, the first step in your vm is to update and install necessary applications such as ssh, rsync and vim. Installation1. Ingredients checkup Make sure the Directory has the following structure 123456789101112131415161718192021.├── cache -- Files to replace in VM│ ├── core-site.xml -- replace /usr/local/hadoop/etc/hadoop/core-site.xml in VM│ ├── hadoop-2.9.0.tar.gz -- hadoop │ ├── hdfs-site.xml -- replace /usr/local/hadoop/etc/hadoop/hdfs-site.xml in VM │ ├── hosts -- replace /usr/hosts in VM│ ├── jdk-8u161-linux-x64.tar.gz -- jdk package│ ├── mapred-site.xml -- replace /usr/local/hadoop/etc/hadoop/mapred-site.xml│ ├── scala-2.11.8.tgz -- scala package│ ├── sources.list -- replace /etc/apt/sources.list -&gt; from https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/│ ├── spark-2.3.0-bin-hadoop2.7.tgz -- spark package│ └── yarn-site.xml -- replace /usr/local/hadoop/etc/hadoop/yarn-site.xml├── hadoop+spark集群平台搭建.pptx -- ppt instruction for Hadoop + spark on VMware├── Hadoop集群安装手册.pdf -- PDF instruction for Hadoop on VMware based on CentOS├── hadoop集群搭建.pptx -- ppt instruction for Hadoop on VMware based on Ubuntu -- Instruction for this Virtualbox Version├── img -- IMGs in this readme file├── init.sh -- Scripts to execute when VM first starts├── README.md -- This file└── Vagrantfile -- VM Configurations2 directories, 26 files 2. Check init.sh and Vagrantfile for certain configurations The default configuration for Virtual Machines is written in Vagrantfile. Modifications can be made by changing the code directly. Check up init.sh for more setups. 3. Execute Vagrant Up In the directory shown above, execute vagrant up to setup and boot your vm. 1vagrant up Commands to Execute It should take a while. So have a cup of tea and when everything is settled, check your virutal machine with either vagrant status or virtualbox user interface 1vagrant status vagrant status virtualbox user interface 4. ssh configurations use vagrant ssh master to enter master virtual machine. 1vagrant ssh master vagrant ssh master Append public keys to authorized_keys by cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys 1cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys Note that all three public keys have been generated and pasted in /vagrant/cache/authorized_keys by commands in init.sh and Vagrantfile. Ssh configuration should be done in both slaves as well. Varify ssh configuration by executing ssh slave1 in master virtual machine. You sohuld log into slave1 without entering password. log into slave1 without password after ssh configuration Deploy and Varify Hadoop1. Deploy Hadoop run hadoop namenode -format in master virtual machine to configure node information. 1hadoop namenode -format run start-all.sh to start deployment. Always remember to run stop-all.sh before virtual machine shutdown. 2. Varification run hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5 in directory /usr/local/hadoop/share/hadoop/mapreduce to varify.12cd /usr/local/hadoop/share/hadoop/mapreducehadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5 A number relatively close to pi is then presented. To be more accurate on the result, try running pi 10 10000 which takes a longer period. 3. User Interface Hadoop Environment Configuration on IP:50070 where IP is the static IP for Master. And on IP:8088 where IP is the static IP of Master and 8088 can be configured in those .xml files. UI","link":"/2022/11/15/deploy-hadoop/"},{"title":"Add Extra Memory to Thinkpad T14","text":"Working on hard projects with Hadoop cluster really runs out the limit of my poorly configured laptop. Therefore, I decide to add an extra memory card to it to boost the performance. 1. Check the hardware information Before cracking down the shell, we should figure out whether my laptop has a spare set for the extra memory and what kinds of memory card should I buy. To do that, we perform the following command to monitor the hardware status1sudo dmidecode memory &gt; memory.log command output Note: I’ve accidentally removed memory.log which generated before adding the extra memory, and this screenshot is taken after the card is inserted. According to the output, the maximum Capacity of Memory of my laptop is 32 GB, meaning that it is capable of accepting an extra memory of 16 GB. Another important information here is the speed. Because both memory cards will be accessed with the same bus, the slower speed could be a bottle net. I bought a memory card from Samsung with the same speed and data width for 300 RMB on Taobao. 2. Opening the Laptop Using tools from the shop, the laptop is rather easy to tear. The only thing to mention here is that, be gentle not to break the machine down and after insertion, do not fix the screw before the evaluation process succeeds. In fact, the work can be down with a screwdriver, and a student ID card. 3. Evaluation After insertion, boot the machine and enter the Operating System to check whether the extra Memory works well. Successfully Boot up Heavy Virtual Machines after Installation of the Additional 16GB Memory Card","link":"/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/"},{"title":"SSH Penetration Setup Memo","text":"Background We have deployed a web application demo on a computer whose network access is limited within school and cannot be accessed from devices outside. Plus that we know from the man page of ssh that ssh -R could perform a proxy that transfers data stream from one port to another, thus, we decide to make use of a server bought from Aliyun as a repeater, or bridge, that connects our clients to the application. How to do it For specification, we call the computer with the deployment of our web application as A and its port as AP while the server is called B, BP1, BP2. First on computer A 1ssh -fCNR `BP1`:localhost:`AP` -o ServerAliveInterval=60 serverName@serverIP On Server 1ssh -fCNL 0.0.0.0:`BP2`:localhost:`BP1` localhost It works like this Graph Access through internet Access","link":"/2022/12/01/ssh/"},{"title":"Setup Systemd for clash Proxy","text":"Since I live in China mainland, some specific websites can’t get accessed without a Vpn service running on the laptop. I managed to get one using clash and configuration. However, I have to run ./clash -d . manually every-time I need access, which is inconvenient. Since I’ve been using Linux, I did some search and managed to set up a system task that runs automatically after booting. Here is the Memo Add clash@.service in /usr/lib/systemd/system/ or in /etc/systemd/system/ Example 123456789101112[Unit]Description=A rule based proxy in Go for %i.After=network.target[Service]Type=simpleUser=%iRestart=on-abortExecStart=/home/christopher/.config/clash/clash[Install]WantedBy=multi-user.target Use systemctl to enable and start mission. Systemd Management","link":"/2022/12/09/Setup-Systemd-for-clash-Proxy/"},{"title":"Source Backup for Disk Update","text":"Background 作为大学生，只有一台 Thinkpad T14 Gen 1 的笔记本却安装了 Arch Linux，而由于课程需要仍保留了大部分 Windows 11 的系统，长期以来的使用，让 512 GB 的存储空间告急，于是打算更换一块 1T 的固态硬盘。在此对磁盘更换做一个记录。 准备文件分类 第一大类：系统文件， 如 Windows 等 Operating Systems 的文件 不考虑保存，但需要先查清除如何对 Windows 11 下的 Office 全家桶进行恢复。 第二大类：工程文件； 2-1. 项目、代码等 存入 github 代码库，将不必要保存的 tmp 文件删除 写好 README 文档 2-2. 安装包、dependency 丢弃，实际使用时再安装 第三大类：资源文件： 3-1. 课程文件等 打包按时间排序 项目类文件保存两份 3-2. 配置文件 重要配置打包或上传 github 3-3. 字体 不能打包，但需要确定安装和配置方案 3-3. 其他资源 写个 README 计划 文件备份 根据上述分类进行备份 2022-12-12 23:38 DONE 确定安装和磁盘分区 安装 512 GB 的 Windows 11 系统 和 512 GB 的 Linux 系统 Linux 安装 DWM + Xorg （需要先写好安装文档以便查看） 购置固态硬盘 选取三星 “980 Pro PCIe 4.0 NVMe M.2” 2022-12-12 23:38 次日到貨 对需要安装的系统和软件、配置写教程 待安装（配置）的内容 教程 备注 Arch Linux viseator的博客 dwm 拷贝文件即可, 应当包括 autostart dmenu 同上 vim / nvim 同上 中文输入法 中文輸入法等安裝 yay yay git repo clash 我的博客 navicat 已經完成拷貝 apifox vivado Vivado HLx 2019.2: All OS installer Single-File Download (TAR/GZIP - 26.55 GB) libfreenect2 libfreenect2 Github!! 十分重要 opencv yay 将就用一下先 virtualbox 我的博客","link":"/2022/12/12/Source-Backup-for-Disk-Update/"},{"title":"KinectV2 Camera Calibration and &#96;Yolov5&#96; Recognition","text":"Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution. BackgroundWe are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects); In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates. Project Structure123456789101112131415161718192021222324252627.├── build -- build Dir├── CMakeLists.txt -- Top Cmake Configuration├── data │ └── output.mp4 -- Output Data -&gt; stacks of imshown frames├── default.xml -- Default Configuration File (example)├── include │ ├── calibration.hpp -- Calibration -&gt; Future change: With Robot│ ├── define.hpp -- Define COLORS etc│ ├── dnn.hpp -- Use OpenCV DNN APIs│ ├── main.hpp -- Main Program│ └── settings.hpp -- Read Settings├── logsrc -- Log Helper by loguru│ ├── CMakeLists.txt│ ├── loguru.cpp│ └── loguru.hpp├── models -- Trained Yolov5 Modules│ ├── yolov5n.onnx│ ├── yolov5s.onnx│ ├── yolov5.xml│ └── yuki-bubu-2022-12-23.onnx ├── README.md └── src ├── calibration.cpp ├── dnn.cpp └── main.cpp21 directories, 87 files Dependency We use two libraries: Libfreenect2 and OpenCV. Note that: You should set freenect2_DIR and include freenect2_INCLUDE directions if you install libfreenect2 in custom directories. 12345# Set up libfreenect2# Set include dir and DIR for libfreenect2 if it is not installed globally# SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)# include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)find_package(freenect2 REQUIRED) Program Usage Install Dependencies shown above Run the following commands to build the project 123mkdir -p build &amp;&amp; cd buildcmake ..cmake --build . Plug in KinectV2 via USB (you might need a hub) run ./calibration to start program Developer Diary To meet the need, we have to conquer four difficulties. KinectV2 Connection OpenCV Calibration Object Detection Planes Transformation 1. KinectV2 ConnectionSince we develop the program on multiple Operating Systems (OS), we decide to take advantage of Libfreenect2 which is open-sourced and supports Linux, Windows and Mac-OS. To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine. To use KinectV2, we need the following steps: 1.1. Define Basic Variables, either globally or locally.12345libfreenect2::Freenect2 freenect2; // libfreenect2 entitylibfreenect2::PacketPipeline *pipeline; // libfreenect2 pipelinelibfreenect2::Freenect2Device *device; // devicelibfreenect2::SyncMultiFrameListener listener(libfreenect2::Frame::Color);libfreenect2::FrameMap frames; 1.2. Initialize the device via certain APIs12345678910111213141516171819202122/* -------------------- START Kinectv2 Initialization -------------------- */if(freenect2.enumerateDevices() == 0){ LOG_F(ERROR, &quot;no device connected!&quot;); return -1;} else { LOG_F(INFO, &quot;device connected&quot;);}string serial = freenect2.getDefaultDeviceSerialNumber();LOG_F(INFO, &quot;SEARIAL Number: %s&quot;,serial.c_str());pipeline = new libfreenect2::CpuPacketPipeline();device = freenect2.openDevice(serial, pipeline); if(device == 0){ LOG_F(ERROR, &quot;failed to open device: %s&quot;, serial.c_str()); return -1;} else { LOG_F(INFO, &quot;device opened&quot;);}kinect_shutdown = false;device-&gt;setColorFrameListener(&amp;listener);device-&gt;start();LOG_F(INFO, &quot;device serial: %s&quot; ,device-&gt;getSerialNumber().c_str());LOG_F(INFO, &quot;device firmware: %s&quot; ,device-&gt;getFirmwareVersion().c_str());/* -------------------- END Kinectv2 Initialization -------------------- */ 1.3. We shall start a Loop to receive frames from the Device12345678while(!kinect_shutdown){ if(!listener.waitForNewFrame(frames, timeout)) LOG_F(WARNING, &quot;Frame Received Failed after timeout: %d&quot;, timeout); libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color]; /* -------------------- START Frame Processing -------------------- */ /* -------------------- END Frame Processing -------------------- */ listener.release(frames);} 1.4. Before Exit, we need to manually stop and close the device12device-&gt;stop();device-&gt;close(); We must define a sigint_handler to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown. 1234567void sigint_handler(int s){ device-&gt;stop(); device-&gt;close(); exit(s);}// Usage signal(SIGINT, sigint_handler); // Savely Close the Device before sudden exit 1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing.1234cv::Mat(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).copyTo(kinect_mat);cv::flip(kinect_mat, kinect_mat, 1);rgb_mat = cv::Mat::zeros(kinect_mat.size(),CV_8UC3);mixChannels(kinect_mat, rgb_mat, {0,0,1,1,2,2}); Note that: libfreenect2::Frame contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a mixChannels() here to reduce the last one. 2. OpenCV CalibrationTo be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. YoloV5) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a Perspective-n-Point(aka PnP) problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally. However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task. We perform 4 steps to meet the need. 2.1. Collect multiple frames where the camera and chessboard are relatively still.12if(STATE == STATE_START_CALIBRATION) cali_frames.push_back(rgb_mat); Note that: We use STATE to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM). 2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system.123vector&lt;Point2f&gt; point_buff;int board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;int found = findChessboardCorners(rgb_mat, boardSize, point_buff, board_flag); Note that: We use PThread to accelerate the process, finding patterns in all collected frames at once. 2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.12345678910111213int found=0;for(int i=0;i&lt;size;++i){ void * ret; pthread_join(thread_ids[i], &amp;ret); runCalibrationRet retVal = *(runCalibrationRet*) ret; if(retVal.found){ if(!found) d2s = retVal.d2; else for(int j=0;j&lt;d2s.size();j++) d2s[j] += retVal.d2[j]; found ++; }} 123for(int i=0;i&lt;boardSize.height; ++i) for(int j=0;j&lt;boardSize.width; ++j) d3s.push_back(Point3f(j*squareSize, i*squareSize, 0)); Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system. 2.4. Wrap them up and perform solvePnP to get rvec and tvec1solvePnP(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec); Note that:camera_matrix and dist_coeffs are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it. 2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained.123456789int type_tv = tv.type();Mat rvf(3,3,type_tv);// Convert from vector rv(3x1) to matrix rotation(3x3)Rodrigues(rv, rvf);// The Inversed MatrixMat rvf_1(3,3,type_tv);invert(rvf, rvf_1, DECOMP_SVD);Mat Position = rvf_1 * (-tv);Point3f p(Position); Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: OpenCV Pnp reference. Also note that: Rodrigues is essential, the output, rvec is (3x1), reference: Rodrigues 3. Object DetectionAccording to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous YoloV5 Project. The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: Train on Custom Data). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs cv::dnn that load .onnx models and can run forward actions, or deduction. In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs. 3.1. Dataset CreationSimply follow steps described on this page. The output should be similar as below: 12345678.├── data.yaml├── README.dataset.txt├── README.roboflow.txt├── test├── train└── valid9 directories, 382 files 3.2 Train modelSimply put datasets in yolov5 directory and perform the following command and sit back to wait for the results 1python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt Note that: You should clone yolov5 repo before training and of course set up python environment. 3.3 Convert model to .onnxYoloV5 takes PyTorch as backend, thus, the models are saved as .pt format. However, cv::dnn prefers .onnx format. Thus, a conversion shall be performed. At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV dnn module. I have posted an Issue on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project. After the correction of Dependency, we perform the following command to export .onnx from .pt. 1python export.py --weights yolov5s.pt --include onnx 3.4 Load .onnx with OpenCV123cv::dnn::Net net = cv::dnn::readNetFromONNX(model_path);net.setPreferableBackend(cv::dnn::DNN_BACKEND_OPENCV);net.setPreferableTarget(cv::dnn::DNN_TARGET_CPU); 3.5 Format Input Data1234567cv::Mat blob;int col = frame.cols;int row = frame.rows;int _max = max(col, row);input_img = cv::Mat::zeros(_max, _max, CV_8UC3);frame.copyTo(input_img(cv::Rect(0, 0, col, row)));cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false); 3.6 Forward Network123net.setInput(input);std::vector&lt;cv::Mat&gt; outputs;net.forward(outputs, net.getUnconnectedOutLayersNames()); 3.7 Format the outputThe output of the model, as the result of network-forwarding, is defined as follows: 1234567 structure of `output.data`┌─┬─┬─┬─┬─┬────────┬─────────────────────►│0│1│2│3│4│5 ......│dimensions├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──► c: confidence Basically, it is an array that can be accessed via its address. The code is too large to be shown here. 3.8 Transfer Output to DetectionFor easy access, we transfer the output to the following data format. 12345struct Detection{ int class_id; // Result's class id float confidence; // Probability cv::Rect box; // Where it is}; 3.9 Draw Boxes around Targets12345678910int detection_size = output.size();for(int i=0;i&lt;detection_size;++i){ auto detection = output[i]; auto box = detection.box; auto class_id = detection.class_id; const auto color = color_list[class_id%color_list.size()]; cv::rectangle(frame, box, color, 2); cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED); cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);} 4. Plane TransformationFinally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: ResearchGate) Take the example of the transformation of Point $^1p_1$. The 3D position of it can be anywhere on the line of $O\\ ^1p_1$, not necessary be at Point $^wp_1$ unless we require the 3D position lies on a particular plane. Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground $XOY$ axis and not provide the height. Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: getPerspectiveTransformation. According to the definition below, the input src and dst must be vertices of a quadrangle. And the return value shall be the transformation matrix. 4.1 Calculate Transformation MatrixIn our project, we take 4 vertices of the chessboard to be the input. 123456789101112131415161718vector&lt;Point2f&gt; desk;for(auto d3:d3s) desk.push_back(Point2f(d3.x,d3.y));Point2f in[4];Point2f out[4];#define helper(_in, in_) \\_in[0] = in_[i0]; \\_in[1] = in_[i1]; \\_in[2] = in_[i2]; \\_in[3] = in_[i3]int i0 = 0, i1 = boardSize.width-1;int i2 = boardSize.width * (boardSize.height-1);int i3 = boardSize.width * boardSize.height - 1;helper(in, d2s);helper(out, desk);#undef helper// NOTE: According to reference (Opencv), // getPerspectiveTransform takes quadrangle vertices in the source imagepix23D = getPerspectiveTransform(in, out); 4.2 Calculate Corresponding 3D PositionperspectiveTransformation have already done for us. 1234vector&lt;Point2f&gt; out;vector&lt;Point2f&gt; in; in.push_back(p2);cv::perspectiveTransform(in, out, pix23D);Point3f ret = Point3f(out[0].x,out[0].y,0); SummaryTill now, the Project is half-way finished and seems pretty simple, only takes advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan. The next aim of our project would be human-skeleton detection and action deduction with it. And finally, takes some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.","link":"/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"}],"tags":[{"name":"DIY","slug":"DIY","link":"/tags/DIY/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"记录","slug":"记录","link":"/tags/%E8%AE%B0%E5%BD%95/"},{"name":"Hadoop","slug":"Hadoop","link":"/tags/Hadoop/"},{"name":"Big Data","slug":"Big-Data","link":"/tags/Big-Data/"},{"name":"課程作業","slug":"課程作業","link":"/tags/%E8%AA%B2%E7%A8%8B%E4%BD%9C%E6%A5%AD/"},{"name":"Hardware","slug":"Hardware","link":"/tags/Hardware/"},{"name":"Performance Improve","slug":"Performance-Improve","link":"/tags/Performance-Improve/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"learn","slug":"learn","link":"/tags/learn/"},{"name":"VPN","slug":"VPN","link":"/tags/VPN/"},{"name":"Clash","slug":"Clash","link":"/tags/Clash/"},{"name":"Systemd","slug":"Systemd","link":"/tags/Systemd/"},{"name":"KinectV2","slug":"KinectV2","link":"/tags/KinectV2/"},{"name":"Opencv","slug":"Opencv","link":"/tags/Opencv/"}],"categories":[{"name":"Post","slug":"Post","link":"/categories/Post/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"}],"pages":[]}
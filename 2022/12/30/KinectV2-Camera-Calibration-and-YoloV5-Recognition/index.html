<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>KinectV2 Camera Calibration and `Yolov5` Recognition | Christopher Liu's 小窩</title><meta name="author" content="Christopher Liu"><meta name="copyright" content="Christopher Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera">
<meta property="og:type" content="article">
<meta property="og:title" content="KinectV2 Camera Calibration and &#96;Yolov5&#96; Recognition">
<meta property="og:url" content="https://chrisvicky.github.io/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/index.html">
<meta property="og:site_name" content="Christopher Liu&#39;s 小窩">
<meta property="og:description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chrisvicky.github.io/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png">
<meta property="article:published_time" content="2022-12-29T16:00:00.000Z">
<meta property="article:modified_time" content="2023-01-05T06:28:51.463Z">
<meta property="article:author" content="Christopher Liu">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="记录">
<meta property="article:tag" content="KinectV2">
<meta property="article:tag" content="Opencv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chrisvicky.github.io/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chrisvicky.github.io/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'KinectV2 Camera Calibration and `Yolov5` Recognition',
  isPost: true,
  isHome: false,
  isHighlightShrink: undefined,
  isToc: true,
  postUpdate: '2023-01-05 14:28:51'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css"> <style> @font-face { font-family: SmileySans; src: url('/butterflypatch/fonts/SmileySans.ttf') format('truetype'); }
@font-face { font-family: UbuntuNerd; src: url('/butterflypatch/fonts/Ubuntu Light Nerd Font Complete.ttf') format('truetype'); }
@font-face { font-family: WenKai; src: url('/butterflypatch/fonts/LXGWWenKaiMono-Regular.ttf') format('truetype'); }
@font-face { font-family: LibertinusSans; src: url('/butterflypatch/fonts/LibertinusSans-Regular.otf') format('truetype'); } </style> <meta name="generator" content="Hexo 6.3.0"></head><body><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js/themes/blue/pace-theme-minimal.min.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/myimg/yukinoshita_1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/2022/11/12/aboutme"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Christopher Liu's 小窩</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/2022/11/12/aboutme"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">KinectV2 Camera Calibration and `Yolov5` Recognition</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-29T16:00:00.000Z" title="Created 2022-12-30 00:00:00">2022-12-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-01-05T06:28:51.463Z" title="Updated 2023-01-05 14:28:51">2023-01-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Post/">Post</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>14min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="KinectV2 Camera Calibration and `Yolov5` Recognition"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><ul>
<li>Project Repo: <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/KinectV2-calibration-and-Yolov5-recognition">KinectV2 Camera Calibration and <code>Yolov5</code> Recognition</a></li>
<li>2022-12-30 19:11</li>
<li>This is a subproject from <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/camera-position-solution">camera-position-solution</a>.</li>
</ul>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>We are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects);</p>
<center>
  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="/img/image_2023-01-04-00-07-18.png"><br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">KinectV2</div>
</center>

<p>In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates.</p>
<span id="more"></span>

<p><img src="/img/image_2022-12-30-19-10-06.jpg" alt="KinectV2"></p>
<h2 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build                         -- build Dir</span><br><span class="line">├── CMakeLists.txt                -- Top Cmake Configuration</span><br><span class="line">├── data                          </span><br><span class="line">│   └── output.mp4                -- Output Data -&gt; stacks of imshown frames</span><br><span class="line">├── default.xml                   -- Default Configuration File (example)</span><br><span class="line">├── include </span><br><span class="line">│   ├── calibration.hpp           -- Calibration -&gt; Future change: With Robot</span><br><span class="line">│   ├── define.hpp                -- Define COLORS etc</span><br><span class="line">│   ├── dnn.hpp                   -- Use OpenCV DNN APIs</span><br><span class="line">│   ├── main.hpp                  -- Main Program</span><br><span class="line">│   └── settings.hpp              -- Read Settings</span><br><span class="line">├── logsrc                        -- Log Helper by loguru</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── loguru.cpp</span><br><span class="line">│   └── loguru.hpp</span><br><span class="line">├── models                        -- Trained Yolov5 Modules</span><br><span class="line">│   ├── yolov5n.onnx</span><br><span class="line">│   ├── yolov5s.onnx</span><br><span class="line">│   ├── yolov5.xml</span><br><span class="line">│   └── yuki-bubu-2022-12-23.onnx </span><br><span class="line">├── README.md                     </span><br><span class="line">└── src                           </span><br><span class="line">    ├── calibration.cpp</span><br><span class="line">    ├── dnn.cpp</span><br><span class="line">    └── main.cpp</span><br><span class="line">21 directories, 87 files</span><br></pre></td></tr></table></figure>

<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><p>  We use two libraries: <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> and <a target="_blank" rel="noopener" href="https://github.com/opencv/opencv">OpenCV</a>.</p>
<blockquote>
<p>Note that: You should set <code>freenect2_DIR</code> and include <code>freenect2_INCLUDE</code> directions if you install <code>libfreenect2</code> in custom directories.</p>
</blockquote>
  <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up libfreenect2</span></span><br><span class="line"><span class="comment"># Set include dir and DIR for libfreenect2 if it is not installed globally</span></span><br><span class="line"><span class="comment"># SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)</span></span><br><span class="line"><span class="comment"># include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)</span></span><br><span class="line"><span class="keyword">find_package</span>(freenect2 REQUIRED)</span><br></pre></td></tr></table></figure>

<h2 id="Program-Usage"><a href="#Program-Usage" class="headerlink" title="Program Usage"></a>Program Usage</h2><ol>
<li><p>Install Dependencies shown above</p>
</li>
<li><p>Run the following commands to build the project</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure></li>
<li><p>Plug in KinectV2 via USB (you might need a hub)<br><img src="/img/image_2022-12-30-19-11-30.png" alt="Hub"></p>
</li>
<li><p>run <code>./calibration</code> to start program<br><img src="/img/image_2022-12-30-21-19-15.png" alt="Screenshot"></p>
</li>
</ol>
<h2 id="Developer-Diary"><a href="#Developer-Diary" class="headerlink" title="Developer Diary"></a>Developer Diary</h2><p>  To meet the need, we have to conquer four difficulties.</p>
<ol>
<li>KinectV2 Connection</li>
<li>OpenCV Calibration</li>
<li>Object Detection</li>
<li>Planes Transformation</li>
</ol>
<h3 id="1-KinectV2-Connection"><a href="#1-KinectV2-Connection" class="headerlink" title="1. KinectV2 Connection"></a>1. KinectV2 Connection</h3><p>Since we develop the program on multiple Operating Systems (OS), we decide to take advantage of <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> which is open-sourced and supports Linux, Windows and Mac-OS. </p>
<blockquote>
<p>To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine.</p>
</blockquote>
<p>To use KinectV2, we need the following steps:</p>
<h4 id="1-1-Define-Basic-Variables-either-globally-or-locally"><a href="#1-1-Define-Basic-Variables-either-globally-or-locally" class="headerlink" title="1.1. Define Basic Variables, either globally or locally."></a>1.1. Define Basic Variables, either globally or locally.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">libfreenect2::Freenect2 freenect2;      <span class="comment">// libfreenect2 entity</span></span><br><span class="line">libfreenect2::PacketPipeline *pipeline; <span class="comment">// libfreenect2 pipeline</span></span><br><span class="line">libfreenect2::Freenect2Device *device;  <span class="comment">// device</span></span><br><span class="line"><span class="function">libfreenect2::SyncMultiFrameListener <span class="title">listener</span><span class="params">(libfreenect2::Frame::Color)</span></span>;</span><br><span class="line">libfreenect2::FrameMap frames;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-Initialize-the-device-via-certain-APIs"><a href="#1-2-Initialize-the-device-via-certain-APIs" class="headerlink" title="1.2. Initialize the device via certain APIs"></a>1.2. Initialize the <code>device</code> via certain APIs</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* -------------------- START Kinectv2 Initialization -------------------- */</span></span><br><span class="line"><span class="keyword">if</span>(freenect2.<span class="built_in">enumerateDevices</span>() == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;no device connected!&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device connected&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">string serial = freenect2.<span class="built_in">getDefaultDeviceSerialNumber</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;SEARIAL Number: %s&quot;</span>,serial.<span class="built_in">c_str</span>());</span><br><span class="line">pipeline = <span class="keyword">new</span> libfreenect2::<span class="built_in">CpuPacketPipeline</span>();</span><br><span class="line">device = freenect2.<span class="built_in">openDevice</span>(serial, pipeline); <span class="keyword">if</span>(device == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;failed to open device: %s&quot;</span>, serial.<span class="built_in">c_str</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device opened&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">kinect_shutdown = <span class="literal">false</span>;</span><br><span class="line">device-&gt;<span class="built_in">setColorFrameListener</span>(&amp;listener);</span><br><span class="line">device-&gt;<span class="built_in">start</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device serial: %s&quot;</span> ,device-&gt;<span class="built_in">getSerialNumber</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device firmware: %s&quot;</span> ,device-&gt;<span class="built_in">getFirmwareVersion</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">/* -------------------- END Kinectv2 Initialization -------------------- */</span></span><br></pre></td></tr></table></figure>

<h4 id="1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><a href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device" class="headerlink" title="1.3. We shall start a Loop to receive frames from the Device"></a>1.3. We shall start a Loop to receive frames from the Device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!kinect_shutdown)&#123;</span><br><span class="line">  <span class="keyword">if</span>(!listener.<span class="built_in">waitForNewFrame</span>(frames, timeout))</span><br><span class="line">    <span class="built_in">LOG_F</span>(WARNING, <span class="string">&quot;Frame Received Failed after timeout: %d&quot;</span>, timeout);</span><br><span class="line">  libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color];</span><br><span class="line">  <span class="comment">/* -------------------- START Frame Processing -------------------- */</span></span><br><span class="line">  <span class="comment">/* -------------------- END Frame Processing -------------------- */</span></span><br><span class="line">  listener.<span class="built_in">release</span>(frames);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><a href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device" class="headerlink" title="1.4. Before Exit, we need to manually stop and close the device"></a>1.4. Before Exit, we need to manually stop and close the device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">device-&gt;<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>We must define a <code>sigint_handler</code> to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown.</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="type">int</span> s)</span></span>&#123;</span><br><span class="line">  device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">  device-&gt;<span class="built_in">close</span>();</span><br><span class="line">  <span class="built_in">exit</span>(s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Usage </span></span><br><span class="line"><span class="built_in">signal</span>(SIGINT, sigint_handler); <span class="comment">// Savely Close the Device before sudden exit</span></span><br></pre></td></tr></table></figure>

<h4 id="1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><a href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing" class="headerlink" title="1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing."></a>1.5. To take advantage of OpenCV APIs, we convert <code>libfreenect2::Frame</code> to <code>cv::Mat</code> right at the beginning of <code>Frame Processing</code>.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">Mat</span>(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).<span class="built_in">copyTo</span>(kinect_mat);</span><br><span class="line">cv::<span class="built_in">flip</span>(kinect_mat, kinect_mat, <span class="number">1</span>);</span><br><span class="line">rgb_mat = cv::Mat::<span class="built_in">zeros</span>(kinect_mat.<span class="built_in">size</span>(),CV_8UC3);</span><br><span class="line"><span class="built_in">mixChannels</span>(kinect_mat, rgb_mat, &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: <code>libfreenect2::Frame</code> contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a <code>mixChannels()</code> here to reduce the last one.</p>
</blockquote>
<hr>
<h3 id="2-OpenCV-Calibration"><a href="#2-OpenCV-Calibration" class="headerlink" title="2. OpenCV Calibration"></a>2. OpenCV Calibration</h3><p>To be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5</a>) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">Perspective-n-Point(aka <code>PnP</code>)</a> problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally.</p>
<p>However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task.</p>
<p>We perform 4 steps to meet the need.</p>
<h4 id="2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><a href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still" class="headerlink" title="2.1. Collect multiple frames where the camera and chessboard are relatively still."></a>2.1. Collect multiple frames where the camera and chessboard are relatively still.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(STATE == STATE_START_CALIBRATION)</span><br><span class="line">  cali_frames.<span class="built_in">push_back</span>(rgb_mat);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>STATE</code> to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM).</p>
</blockquote>
<h4 id="2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system"><a href="#2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system" class="headerlink" title="2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system."></a>2.2. Run <code>findPattern</code> to obtain feature points’ position in 2D-pixel-coordinate-system.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; point_buff;</span><br><span class="line"><span class="type">int</span> board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;</span><br><span class="line"><span class="type">int</span> found = <span class="built_in">findChessboardCorners</span>(rgb_mat, boardSize, point_buff, board_flag);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>PThread</code> to accelerate the process, finding patterns in all collected frames at once.</p>
</blockquote>
<h4 id="2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><a href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information" class="headerlink" title="2.3. Collect all 2D information and calculate 3D-world-coordinate-system information."></a>2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> found=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;++i)&#123;</span><br><span class="line">  <span class="type">void</span> * ret;</span><br><span class="line">  <span class="built_in">pthread_join</span>(thread_ids[i], &amp;ret);</span><br><span class="line">  runCalibrationRet retVal = *(runCalibrationRet*) ret;</span><br><span class="line">  <span class="keyword">if</span>(retVal.found)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!found)</span><br><span class="line">      d2s = retVal.d2;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;d2s.<span class="built_in">size</span>();j++) d2s[j] += retVal.d2[j];</span><br><span class="line">    found ++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;boardSize.height; ++i)</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;boardSize.width; ++j)</span><br><span class="line">    d3s.<span class="built_in">push_back</span>(<span class="built_in">Point3f</span>(j*squareSize, i*squareSize, <span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system.</p>
</blockquote>
<h4 id="2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><a href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec" class="headerlink" title="2.4. Wrap them up and perform solvePnP to get rvec and tvec"></a>2.4. Wrap them up and perform <code>solvePnP</code> to get <code>rvec</code> and <code>tvec</code></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">solvePnP</span>(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that:<code>camera_matrix</code> and <code>dist_coeffs</code> are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it.</p>
</blockquote>
<h4 id="2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><a href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained" class="headerlink" title="2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained."></a>2.5. To obtain camera position, we still need another step that takes both <code>rvec</code> and <code>tvec</code> as input and <code>camera_position</code> would be obtained.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> type_tv = tv.<span class="built_in">type</span>();</span><br><span class="line"><span class="function">Mat <span class="title">rvf</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="comment">// Convert from vector rv(3x1) to matrix rotation(3x3)</span></span><br><span class="line"><span class="built_in">Rodrigues</span>(rv, rvf);</span><br><span class="line"><span class="comment">// The Inversed Matrix</span></span><br><span class="line"><span class="function">Mat <span class="title">rvf_1</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="built_in">invert</span>(rvf, rvf_1, DECOMP_SVD);</span><br><span class="line">Mat Position = rvf_1 * (-tv);</span><br><span class="line"><span class="function">Point3f <span class="title">p</span><span class="params">(Position)</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">OpenCV <code>Pnp</code> reference</a>.</p>
</blockquote>
<blockquote>
<p>Also note that: <code>Rodrigues</code> is essential, the output, <code>rvec</code> is (3x1), reference: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac">Rodrigues</a></p>
</blockquote>
<hr>
<h3 id="3-Object-Detection"><a href="#3-Object-Detection" class="headerlink" title="3. Object Detection"></a>3. Object Detection</h3><p>According to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5 Project</a>.</p>
<p>The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train on Custom Data</a>). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs <code>cv::dnn</code> that load <code>.onnx</code> models and can run forward actions, or deduction.</p>
<p>In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs.</p>
<h4 id="3-1-Dataset-Creation"><a href="#3-1-Dataset-Creation" class="headerlink" title="3.1. Dataset Creation"></a>3.1. Dataset Creation</h4><p>Simply follow steps described on <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">this page</a>. The output should be similar as below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data.yaml</span><br><span class="line">├── README.dataset.txt</span><br><span class="line">├── README.roboflow.txt</span><br><span class="line">├── test</span><br><span class="line">├── train</span><br><span class="line">└── valid</span><br><span class="line">9 directories, 382 files</span><br></pre></td></tr></table></figure>

<h4 id="3-2-Train-model"><a href="#3-2-Train-model" class="headerlink" title="3.2 Train model"></a>3.2 Train model</h4><p>Simply put datasets in yolov5 directory and perform the following command and sit back to wait for the results</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: You should clone yolov5 repo before training and of course set up python environment. </p>
</blockquote>
<h4 id="3-3-Convert-model-to-onnx"><a href="#3-3-Convert-model-to-onnx" class="headerlink" title="3.3 Convert model to .onnx"></a>3.3 Convert model to <code>.onnx</code></h4><p>YoloV5 takes PyTorch as backend, thus, the models are saved as <code>.pt</code> format. However, <code>cv::dnn</code> prefers <code>.onnx</code> format. Thus, a conversion shall be performed.</p>
<p>At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV <code>dnn</code> module. I have posted an <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/10575">Issue</a> on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project.</p>
<p>After the correction of Dependency, we perform the following command to export <code>.onnx</code> from <code>.pt</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --weights yolov5s.pt --include onnx</span><br></pre></td></tr></table></figure>

<h4 id="3-4-Load-onnx-with-OpenCV"><a href="#3-4-Load-onnx-with-OpenCV" class="headerlink" title="3.4 Load .onnx with OpenCV"></a>3.4 Load <code>.onnx</code> with OpenCV</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv::dnn::Net net = cv::dnn::<span class="built_in">readNetFromONNX</span>(model_path);</span><br><span class="line">net.<span class="built_in">setPreferableBackend</span>(cv::dnn::DNN_BACKEND_OPENCV);</span><br><span class="line">net.<span class="built_in">setPreferableTarget</span>(cv::dnn::DNN_TARGET_CPU);</span><br></pre></td></tr></table></figure>

<h4 id="3-5-Format-Input-Data"><a href="#3-5-Format-Input-Data" class="headerlink" title="3.5 Format Input Data"></a>3.5 Format Input Data</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat blob;</span><br><span class="line">int col = frame.cols;</span><br><span class="line">int row = frame.rows;</span><br><span class="line">int _max = max(col, row);</span><br><span class="line">input_img = cv::Mat::zeros(_max, _max, CV_8UC3);</span><br><span class="line">frame.copyTo(input_img(cv::Rect(0, 0, col, row)));</span><br><span class="line">cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false);</span><br></pre></td></tr></table></figure>

<h4 id="3-6-Forward-Network"><a href="#3-6-Forward-Network" class="headerlink" title="3.6 Forward Network"></a>3.6 Forward Network</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.setInput(input);</span><br><span class="line">std::vector&lt;cv::Mat&gt; outputs;</span><br><span class="line">net.forward(outputs, net.getUnconnectedOutLayersNames());</span><br></pre></td></tr></table></figure>

<h4 id="3-7-Format-the-output"><a href="#3-7-Format-the-output" class="headerlink" title="3.7 Format the output"></a>3.7 Format the output</h4><p>The output of the model, as the result of network-forwarding, is defined as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> structure of `output.data`</span><br><span class="line">┌─┬─┬─┬─┬─┬────────┬─────────────────────►</span><br><span class="line">│0│1│2│3│4│5 ......│dimensions</span><br><span class="line">├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►</span><br><span class="line">│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..</span><br><span class="line">└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──►</span><br><span class="line"> c: confidence</span><br></pre></td></tr></table></figure>
<p>Basically, it is an array that can be accessed via its address. The code is too large to be shown here.</p>
<h4 id="3-8-Transfer-Output-to-Detection"><a href="#3-8-Transfer-Output-to-Detection" class="headerlink" title="3.8 Transfer Output to Detection"></a>3.8 Transfer Output to <code>Detection</code></h4><p>For easy access, we transfer the output to the following data format.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Detection&#123;</span><br><span class="line">	int       class_id;   // Result&#x27;s class id</span><br><span class="line">	float     confidence; // Probability</span><br><span class="line">	cv::Rect  box;        // Where it is</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="3-9-Draw-Boxes-around-Targets"><a href="#3-9-Draw-Boxes-around-Targets" class="headerlink" title="3.9 Draw Boxes around Targets"></a>3.9 Draw Boxes around Targets</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int detection_size = output.size();</span><br><span class="line">for(int i=0;i&lt;detection_size;++i)&#123;</span><br><span class="line">	auto detection = output[i];</span><br><span class="line">	auto box = detection.box;</span><br><span class="line">	auto class_id = detection.class_id;</span><br><span class="line">	const auto color = color_list[class_id%color_list.size()];</span><br><span class="line">	cv::rectangle(frame, box, color, 2);</span><br><span class="line">	cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED);</span><br><span class="line">	cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-Plane-Transformation"><a href="#4-Plane-Transformation" class="headerlink" title="4. Plane Transformation"></a>4. Plane Transformation</h3><p>Finally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: <a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Display-of-Various-Coordinate-Systems-for-a-Computer-Vision-System-i-i-i-i-1-i_fig2_337311806">ResearchGate</a>)</p>
<p><img src="/img/image_2022-12-31-00-10-18.png" alt="Computer Vision Model"></p>
<p>Take the example of the transformation of Point $^1p_1$. The 3D position of it can be anywhere on the line of $O\ ^1p_1$, not necessary be at Point $^wp_1$ unless we require the 3D position lies on a particular plane.</p>
<p>Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground $XOY$ axis and not provide the height.</p>
<p>Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae"><code>getPerspectiveTransformation</code></a>. According to the definition below, the input <code>src</code> and <code>dst</code> must be vertices of a quadrangle. And the return value shall be the transformation matrix.</p>
<p><img src="/img/image_2022-12-31-00-19-10.png" alt="`getPerspectiveTransform` definition"></p>
<h4 id="4-1-Calculate-Transformation-Matrix"><a href="#4-1-Calculate-Transformation-Matrix" class="headerlink" title="4.1 Calculate Transformation Matrix"></a>4.1 Calculate Transformation Matrix</h4><p>In our project, we take 4 vertices of the chessboard to be the input.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; desk;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> d3:d3s) desk.<span class="built_in">push_back</span>(<span class="built_in">Point2f</span>(d3.x,d3.y));</span><br><span class="line">Point2f in[<span class="number">4</span>];</span><br><span class="line">Point2f out[<span class="number">4</span>];</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> helper(_in, in_) \</span></span><br><span class="line"><span class="meta">_in[0] = in_[i0]; \</span></span><br><span class="line"><span class="meta">_in[1] = in_[i1]; \</span></span><br><span class="line"><span class="meta">_in[2] = in_[i2]; \</span></span><br><span class="line"><span class="meta">_in[3] = in_[i3]</span></span><br><span class="line"><span class="type">int</span> i0 = <span class="number">0</span>, i1 = boardSize.width<span class="number">-1</span>;</span><br><span class="line"><span class="type">int</span> i2 = boardSize.width * (boardSize.height<span class="number">-1</span>);</span><br><span class="line"><span class="type">int</span> i3 = boardSize.width * boardSize.height - <span class="number">1</span>;</span><br><span class="line"><span class="built_in">helper</span>(in, d2s);</span><br><span class="line"><span class="built_in">helper</span>(out, desk);</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> helper</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> According to reference (Opencv), </span></span><br><span class="line"><span class="comment">// getPerspectiveTransform takes quadrangle vertices in the source image</span></span><br><span class="line">pix23D = <span class="built_in">getPerspectiveTransform</span>(in, out);</span><br></pre></td></tr></table></figure>

<h4 id="4-2-Calculate-Corresponding-3D-Position"><a href="#4-2-Calculate-Corresponding-3D-Position" class="headerlink" title="4.2 Calculate Corresponding 3D Position"></a>4.2 Calculate Corresponding 3D Position</h4><p><code>perspectiveTransformation</code> have already done for us.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; out;</span><br><span class="line">vector&lt;Point2f&gt; in; in.push_back(p2);</span><br><span class="line">cv::perspectiveTransform(in, out, pix23D);</span><br><span class="line">Point3f ret = Point3f(out[0].x,out[0].y,0);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Till now, the Project is half-way finished and seems pretty simple, only taking advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan. </p>
<p>The next aim of our project would be human-skeleton detection and action deduction with it. And finally, adding some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://chrisvicky.github.io">Christopher Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://chrisvicky.github.io/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/">https://chrisvicky.github.io/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Linux/">Linux</a><a class="post-meta__tags" href="/tags/%E8%AE%B0%E5%BD%95/">记录</a><a class="post-meta__tags" href="/tags/KinectV2/">KinectV2</a><a class="post-meta__tags" href="/tags/Opencv/">Opencv</a></div><div class="post_share"><div class="social-share" data-image="/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/16/An-Image-is-Worth-16x16-Words/"><img class="prev-cover" src="/myimg/ViTModel.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Paper Review: An Image is Worth 16x16 Words</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/12/Source-Backup-for-Disk-Update/"><img class="next-cover" src="/myimg/AprilisLie.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Source Backup for Disk Update</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/12/Source-Backup-for-Disk-Update/" title="Source Backup for Disk Update"><img class="cover" src="/myimg/AprilisLie.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-12</div><div class="title">Source Backup for Disk Update</div></div></a></div><div><a href="/2022/12/09/Setup-Systemd-for-clash-Proxy/" title="Setup Systemd for clash Proxy"><img class="cover" src="/myimg/Clash.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">Setup Systemd for clash Proxy</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/myimg/yukinoshita_1.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Christopher Liu</div><div class="author-info__description">Stay Hungry, Stay Foolish</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ChrisVicky"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ChrisVicky" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:chrisckeyliu@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">1.</span> <span class="toc-text">Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Project-Structure"><span class="toc-number">2.</span> <span class="toc-text">Project Structure</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dependency"><span class="toc-number">3.</span> <span class="toc-text">Dependency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Program-Usage"><span class="toc-number">4.</span> <span class="toc-text">Program Usage</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Developer-Diary"><span class="toc-number">5.</span> <span class="toc-text">Developer Diary</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-KinectV2-Connection"><span class="toc-number">5.1.</span> <span class="toc-text">1. KinectV2 Connection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-Define-Basic-Variables-either-globally-or-locally"><span class="toc-number">5.1.1.</span> <span class="toc-text">1.1. Define Basic Variables, either globally or locally.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Initialize-the-device-via-certain-APIs"><span class="toc-number">5.1.2.</span> <span class="toc-text">1.2. Initialize the device via certain APIs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><span class="toc-number">5.1.3.</span> <span class="toc-text">1.3. We shall start a Loop to receive frames from the Device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><span class="toc-number">5.1.4.</span> <span class="toc-text">1.4. Before Exit, we need to manually stop and close the device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><span class="toc-number">5.1.5.</span> <span class="toc-text">1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-OpenCV-Calibration"><span class="toc-number">5.2.</span> <span class="toc-text">2. OpenCV Calibration</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><span class="toc-number">5.2.1.</span> <span class="toc-text">2.1. Collect multiple frames where the camera and chessboard are relatively still.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Run-findPattern-to-obtain-feature-points%E2%80%99-position-in-2D-pixel-coordinate-system"><span class="toc-number">5.2.2.</span> <span class="toc-text">2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><span class="toc-number">5.2.3.</span> <span class="toc-text">2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><span class="toc-number">5.2.4.</span> <span class="toc-text">2.4. Wrap them up and perform solvePnP to get rvec and tvec</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><span class="toc-number">5.2.5.</span> <span class="toc-text">2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained.</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Object-Detection"><span class="toc-number">5.3.</span> <span class="toc-text">3. Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Dataset-Creation"><span class="toc-number">5.3.1.</span> <span class="toc-text">3.1. Dataset Creation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Train-model"><span class="toc-number">5.3.2.</span> <span class="toc-text">3.2 Train model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-Convert-model-to-onnx"><span class="toc-number">5.3.3.</span> <span class="toc-text">3.3 Convert model to .onnx</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-Load-onnx-with-OpenCV"><span class="toc-number">5.3.4.</span> <span class="toc-text">3.4 Load .onnx with OpenCV</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-5-Format-Input-Data"><span class="toc-number">5.3.5.</span> <span class="toc-text">3.5 Format Input Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-6-Forward-Network"><span class="toc-number">5.3.6.</span> <span class="toc-text">3.6 Forward Network</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-7-Format-the-output"><span class="toc-number">5.3.7.</span> <span class="toc-text">3.7 Format the output</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-8-Transfer-Output-to-Detection"><span class="toc-number">5.3.8.</span> <span class="toc-text">3.8 Transfer Output to Detection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-9-Draw-Boxes-around-Targets"><span class="toc-number">5.3.9.</span> <span class="toc-text">3.9 Draw Boxes around Targets</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Plane-Transformation"><span class="toc-number">5.4.</span> <span class="toc-text">4. Plane Transformation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-Calculate-Transformation-Matrix"><span class="toc-number">5.4.1.</span> <span class="toc-text">4.1 Calculate Transformation Matrix</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-Calculate-Corresponding-3D-Position"><span class="toc-number">5.4.2.</span> <span class="toc-text">4.2 Calculate Corresponding 3D Position</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">6.</span> <span class="toc-text">Summary</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/16/An-Image-is-Worth-16x16-Words/" title="Paper Review: An Image is Worth 16x16 Words"><img src="/myimg/ViTModel.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Paper Review: An Image is Worth 16x16 Words"/></a><div class="content"><a class="title" href="/2023/01/16/An-Image-is-Worth-16x16-Words/" title="Paper Review: An Image is Worth 16x16 Words">Paper Review: An Image is Worth 16x16 Words</a><time datetime="2023-01-15T16:10:27.000Z" title="Created 2023-01-16 00:10:27">2023-01-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/" title="KinectV2 Camera Calibration and `Yolov5` Recognition"><img src="/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="KinectV2 Camera Calibration and `Yolov5` Recognition"/></a><div class="content"><a class="title" href="/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/" title="KinectV2 Camera Calibration and `Yolov5` Recognition">KinectV2 Camera Calibration and `Yolov5` Recognition</a><time datetime="2022-12-29T16:00:00.000Z" title="Created 2022-12-30 00:00:00">2022-12-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/Source-Backup-for-Disk-Update/" title="Source Backup for Disk Update"><img src="/myimg/AprilisLie.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Source Backup for Disk Update"/></a><div class="content"><a class="title" href="/2022/12/12/Source-Backup-for-Disk-Update/" title="Source Backup for Disk Update">Source Backup for Disk Update</a><time datetime="2022-12-11T16:00:00.000Z" title="Created 2022-12-12 00:00:00">2022-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/09/Setup-Systemd-for-clash-Proxy/" title="Setup Systemd for clash Proxy"><img src="/myimg/Clash.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Setup Systemd for clash Proxy"/></a><div class="content"><a class="title" href="/2022/12/09/Setup-Systemd-for-clash-Proxy/" title="Setup Systemd for clash Proxy">Setup Systemd for clash Proxy</a><time datetime="2022-12-09T12:23:09.000Z" title="Created 2022-12-09 20:23:09">2022-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/01/ssh/" title="SSH Penetration Setup Memo"><img src="/myimg/ssh-post-cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SSH Penetration Setup Memo"/></a><div class="content"><a class="title" href="/2022/12/01/ssh/" title="SSH Penetration Setup Memo">SSH Penetration Setup Memo</a><time datetime="2022-12-01T00:05:28.000Z" title="Created 2022-12-01 08:05:28">2022-12-01</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Christopher Liu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, Welcome to My 小窩</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadGiscus () {
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  const config = Object.assign({
    src: 'https://giscus.app/client.js',
    'data-repo': 'ChrisVicky/blog-discussion',
    'data-repo-id': 'R_kgDOItlfgg',
    'data-category-id': 'DIC_kwDOItlfgs4CTYM1',
    'data-mapping': 'pathname',
    'data-theme': nowTheme,
    'data-reactions-enabled': '1',
    crossorigin: 'anonymous',
    async: true
  },null)

  let ele = document.createElement('script')
  for (let key in config) {
    ele.setAttribute(key, config[key])
  }
  document.getElementById('giscus-wrap').insertAdjacentElement('afterbegin',ele)
}

function changeGiscusTheme () {
  const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'light'

  function sendMessage(message) {
    const iframe = document.querySelector('iframe.giscus-frame');
    if (!iframe) return;
    iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
  }

  sendMessage({
    setConfig: {
      theme: theme
    }
  });
}

if ('Giscus' === 'Giscus' || !true) {
  if (true) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
  else loadGiscus()
} else {
  function loadOtherComment () {
    loadGiscus()
  }
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.6" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>
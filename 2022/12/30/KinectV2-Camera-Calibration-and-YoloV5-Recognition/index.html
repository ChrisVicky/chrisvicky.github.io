<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>KinectV2 Camera Calibration and `Yolov5` Recognition - Christopher Liu&#039;s Personal Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Christopher Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Christopher Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera"><meta property="og:type" content="blog"><meta property="og:title" content="KinectV2 Camera Calibration and `Yolov5` Recognition"><meta property="og:url" content="http://example.com/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"><meta property="og:site_name" content="Christopher Liu&#039;s Personal Blog"><meta property="og:description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png"><meta property="article:published_time" content="2022-12-29T16:00:00.000Z"><meta property="article:modified_time" content="2022-12-31T03:33:59.985Z"><meta property="article:author" content="Christopher Liu"><meta property="article:tag" content="Linux"><meta property="article:tag" content="记录"><meta property="article:tag" content="KinectV2"><meta property="article:tag" content="Opencv"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"},"headline":"KinectV2 Camera Calibration and `Yolov5` Recognition","image":["http://example.com/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png"],"datePublished":"2022-12-29T16:00:00.000Z","dateModified":"2022-12-31T03:33:59.985Z","author":{"@type":"Person","name":"Christopher Liu"},"publisher":{"@type":"Organization","name":"Christopher Liu's Personal Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera"}</script><link rel="canonical" href="http://example.com/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/dracula.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Christopher Liu&#039;s Personal Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/2022/11/12/aboutme">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/chrisvicky"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/myimg/KinectV2-Camera-Calibration-and-Yolov5-Recognition.png" alt="KinectV2 Camera Calibration and `Yolov5` Recognition"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-12-29T16:00:00.000Z" title="12/30/2022, 12:00:00 AM">2022-12-30</time>发表</span><span class="level-item"><a class="link-muted" href="/categories/Post/">Post</a></span><span class="level-item">15 分钟读完 (大约2275个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">KinectV2 Camera Calibration and `Yolov5` Recognition</h1><div class="content"><ul>
<li>Project Repo: <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/KinectV2-calibration-and-Yolov5-recognition">KinectV2 Camera Calibration and <code>Yolov5</code> Recognition</a></li>
<li>2022-12-30 19:11</li>
<li>This is a subproject from <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/camera-position-solution">camera-position-solution</a>.</li>
</ul>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>We are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects);</p>
<p>In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates.</p>
<span id="more"></span>

<p><img src="/img/image_2022-12-30-19-10-06.jpg" alt="KinectV2"></p>
<h2 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build                         -- build Dir</span><br><span class="line">├── CMakeLists.txt                -- Top Cmake Configuration</span><br><span class="line">├── data                          </span><br><span class="line">│   └── output.mp4                -- Output Data -&gt; stacks of imshown frames</span><br><span class="line">├── default.xml                   -- Default Configuration File (example)</span><br><span class="line">├── include </span><br><span class="line">│   ├── calibration.hpp           -- Calibration -&gt; Future change: With Robot</span><br><span class="line">│   ├── define.hpp                -- Define COLORS etc</span><br><span class="line">│   ├── dnn.hpp                   -- Use OpenCV DNN APIs</span><br><span class="line">│   ├── main.hpp                  -- Main Program</span><br><span class="line">│   └── settings.hpp              -- Read Settings</span><br><span class="line">├── logsrc                        -- Log Helper by loguru</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── loguru.cpp</span><br><span class="line">│   └── loguru.hpp</span><br><span class="line">├── models                        -- Trained Yolov5 Modules</span><br><span class="line">│   ├── yolov5n.onnx</span><br><span class="line">│   ├── yolov5s.onnx</span><br><span class="line">│   ├── yolov5.xml</span><br><span class="line">│   └── yuki-bubu-2022-12-23.onnx </span><br><span class="line">├── README.md                     </span><br><span class="line">└── src                           </span><br><span class="line">    ├── calibration.cpp</span><br><span class="line">    ├── dnn.cpp</span><br><span class="line">    └── main.cpp</span><br><span class="line">21 directories, 87 files</span><br></pre></td></tr></table></figure>

<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><p>  We use two libraries: <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> and <a target="_blank" rel="noopener" href="https://github.com/opencv/opencv">OpenCV</a>.</p>
<blockquote>
<p>Note that: You should set <code>freenect2_DIR</code> and include <code>freenect2_INCLUDE</code> directions if you install <code>libfreenect2</code> in custom directories.</p>
</blockquote>
  <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up libfreenect2</span></span><br><span class="line"><span class="comment"># Set include dir and DIR for libfreenect2 if it is not installed globally</span></span><br><span class="line"><span class="comment"># SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)</span></span><br><span class="line"><span class="comment"># include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)</span></span><br><span class="line"><span class="keyword">find_package</span>(freenect2 REQUIRED)</span><br></pre></td></tr></table></figure>

<h2 id="Program-Usage"><a href="#Program-Usage" class="headerlink" title="Program Usage"></a>Program Usage</h2><ol>
<li><p>Install Dependencies shown above</p>
</li>
<li><p>Run the following commands to build the project</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure></li>
<li><p>Plug in KinectV2 via USB (you might need a hub)<br><img src="/img/image_2022-12-30-19-11-30.png" alt="Hub"></p>
</li>
<li><p>run <code>./calibration</code> to start program<br><img src="/img/image_2022-12-30-21-19-15.png" alt="Screenshot"></p>
</li>
</ol>
<h2 id="Developer-Diary"><a href="#Developer-Diary" class="headerlink" title="Developer Diary"></a>Developer Diary</h2><p>  To meet the need, we have to conquer four difficulties.</p>
<ol>
<li>KinectV2 Connection</li>
<li>OpenCV Calibration</li>
<li>Object Detection</li>
<li>Planes Transformation</li>
</ol>
<h3 id="1-KinectV2-Connection"><a href="#1-KinectV2-Connection" class="headerlink" title="1. KinectV2 Connection"></a>1. KinectV2 Connection</h3><p>Since we develop the program on multiple Operating Systems (OS), we decide to take advantage of <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> which is open-sourced and supports Linux, Windows and Mac-OS. </p>
<blockquote>
<p>To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine.</p>
</blockquote>
<p>To use KinectV2, we need the following steps:</p>
<h4 id="1-1-Define-Basic-Variables-either-globally-or-locally"><a href="#1-1-Define-Basic-Variables-either-globally-or-locally" class="headerlink" title="1.1. Define Basic Variables, either globally or locally."></a>1.1. Define Basic Variables, either globally or locally.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">libfreenect2::Freenect2 freenect2;      <span class="comment">// libfreenect2 entity</span></span><br><span class="line">libfreenect2::PacketPipeline *pipeline; <span class="comment">// libfreenect2 pipeline</span></span><br><span class="line">libfreenect2::Freenect2Device *device;  <span class="comment">// device</span></span><br><span class="line"><span class="function">libfreenect2::SyncMultiFrameListener <span class="title">listener</span><span class="params">(libfreenect2::Frame::Color)</span></span>;</span><br><span class="line">libfreenect2::FrameMap frames;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-Initialize-the-device-via-certain-APIs"><a href="#1-2-Initialize-the-device-via-certain-APIs" class="headerlink" title="1.2. Initialize the device via certain APIs"></a>1.2. Initialize the <code>device</code> via certain APIs</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* -------------------- START Kinectv2 Initialization -------------------- */</span></span><br><span class="line"><span class="keyword">if</span>(freenect2.<span class="built_in">enumerateDevices</span>() == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;no device connected!&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device connected&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">string serial = freenect2.<span class="built_in">getDefaultDeviceSerialNumber</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;SEARIAL Number: %s&quot;</span>,serial.<span class="built_in">c_str</span>());</span><br><span class="line">pipeline = <span class="keyword">new</span> libfreenect2::<span class="built_in">CpuPacketPipeline</span>();</span><br><span class="line">device = freenect2.<span class="built_in">openDevice</span>(serial, pipeline); <span class="keyword">if</span>(device == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;failed to open device: %s&quot;</span>, serial.<span class="built_in">c_str</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device opened&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">kinect_shutdown = <span class="literal">false</span>;</span><br><span class="line">device-&gt;<span class="built_in">setColorFrameListener</span>(&amp;listener);</span><br><span class="line">device-&gt;<span class="built_in">start</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device serial: %s&quot;</span> ,device-&gt;<span class="built_in">getSerialNumber</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device firmware: %s&quot;</span> ,device-&gt;<span class="built_in">getFirmwareVersion</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">/* -------------------- END Kinectv2 Initialization -------------------- */</span></span><br></pre></td></tr></table></figure>

<h4 id="1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><a href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device" class="headerlink" title="1.3. We shall start a Loop to receive frames from the Device"></a>1.3. We shall start a Loop to receive frames from the Device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!kinect_shutdown)&#123;</span><br><span class="line">  <span class="keyword">if</span>(!listener.<span class="built_in">waitForNewFrame</span>(frames, timeout))</span><br><span class="line">    <span class="built_in">LOG_F</span>(WARNING, <span class="string">&quot;Frame Received Failed after timeout: %d&quot;</span>, timeout);</span><br><span class="line">  libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color];</span><br><span class="line">  <span class="comment">/* -------------------- START Frame Processing -------------------- */</span></span><br><span class="line">  <span class="comment">/* -------------------- END Frame Processing -------------------- */</span></span><br><span class="line">  listener.<span class="built_in">release</span>(frames);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><a href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device" class="headerlink" title="1.4. Before Exit, we need to manually stop and close the device"></a>1.4. Before Exit, we need to manually stop and close the device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">device-&gt;<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>We must define a <code>sigint_handler</code> to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown.</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="type">int</span> s)</span></span>&#123;</span><br><span class="line">  device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">  device-&gt;<span class="built_in">close</span>();</span><br><span class="line">  <span class="built_in">exit</span>(s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Usage </span></span><br><span class="line"><span class="built_in">signal</span>(SIGINT, sigint_handler); <span class="comment">// Savely Close the Device before sudden exit</span></span><br></pre></td></tr></table></figure>

<h4 id="1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><a href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing" class="headerlink" title="1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing."></a>1.5. To take advantage of OpenCV APIs, we convert <code>libfreenect2::Frame</code> to <code>cv::Mat</code> right at the beginning of <code>Frame Processing</code>.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">Mat</span>(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).<span class="built_in">copyTo</span>(kinect_mat);</span><br><span class="line">cv::<span class="built_in">flip</span>(kinect_mat, kinect_mat, <span class="number">1</span>);</span><br><span class="line">rgb_mat = cv::Mat::<span class="built_in">zeros</span>(kinect_mat.<span class="built_in">size</span>(),CV_8UC3);</span><br><span class="line"><span class="built_in">mixChannels</span>(kinect_mat, rgb_mat, &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: <code>libfreenect2::Frame</code> contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a <code>mixChannels()</code> here to reduce the last one.</p>
</blockquote>
<hr>
<h3 id="2-OpenCV-Calibration"><a href="#2-OpenCV-Calibration" class="headerlink" title="2. OpenCV Calibration"></a>2. OpenCV Calibration</h3><p>To be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5</a>) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">Perspective-n-Point(aka <code>PnP</code>)</a> problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally.</p>
<p>However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task.</p>
<p>We perform 4 steps to meet the need.</p>
<h4 id="2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><a href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still" class="headerlink" title="2.1. Collect multiple frames where the camera and chessboard are relatively still."></a>2.1. Collect multiple frames where the camera and chessboard are relatively still.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(STATE == STATE_START_CALIBRATION)</span><br><span class="line">  cali_frames.<span class="built_in">push_back</span>(rgb_mat);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>STATE</code> to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM).</p>
</blockquote>
<h4 id="2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system"><a href="#2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system" class="headerlink" title="2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system."></a>2.2. Run <code>findPattern</code> to obtain feature points’ position in 2D-pixel-coordinate-system.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; point_buff;</span><br><span class="line"><span class="type">int</span> board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;</span><br><span class="line"><span class="type">int</span> found = <span class="built_in">findChessboardCorners</span>(rgb_mat, boardSize, point_buff, board_flag);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>PThread</code> to accelerate the process, finding patterns in all collected frames at once.</p>
</blockquote>
<h4 id="2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><a href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information" class="headerlink" title="2.3. Collect all 2D information and calculate 3D-world-coordinate-system information."></a>2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> found=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;++i)&#123;</span><br><span class="line">  <span class="type">void</span> * ret;</span><br><span class="line">  <span class="built_in">pthread_join</span>(thread_ids[i], &amp;ret);</span><br><span class="line">  runCalibrationRet retVal = *(runCalibrationRet*) ret;</span><br><span class="line">  <span class="keyword">if</span>(retVal.found)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!found)</span><br><span class="line">      d2s = retVal.d2;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;d2s.<span class="built_in">size</span>();j++) d2s[j] += retVal.d2[j];</span><br><span class="line">    found ++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;boardSize.height; ++i)</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;boardSize.width; ++j)</span><br><span class="line">    d3s.<span class="built_in">push_back</span>(<span class="built_in">Point3f</span>(j*squareSize, i*squareSize, <span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system.</p>
</blockquote>
<h4 id="2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><a href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec" class="headerlink" title="2.4. Wrap them up and perform solvePnP to get rvec and tvec"></a>2.4. Wrap them up and perform <code>solvePnP</code> to get <code>rvec</code> and <code>tvec</code></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">solvePnP</span>(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that:<code>camera_matrix</code> and <code>dist_coeffs</code> are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it.</p>
</blockquote>
<h4 id="2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><a href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained" class="headerlink" title="2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained."></a>2.5. To obtain camera position, we still need another step that takes both <code>rvec</code> and <code>tvec</code> as input and <code>camera_position</code> would be obtained.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> type_tv = tv.<span class="built_in">type</span>();</span><br><span class="line"><span class="function">Mat <span class="title">rvf</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="comment">// Convert from vector rv(3x1) to matrix rotation(3x3)</span></span><br><span class="line"><span class="built_in">Rodrigues</span>(rv, rvf);</span><br><span class="line"><span class="comment">// The Inversed Matrix</span></span><br><span class="line"><span class="function">Mat <span class="title">rvf_1</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="built_in">invert</span>(rvf, rvf_1, DECOMP_SVD);</span><br><span class="line">Mat Position = rvf_1 * (-tv);</span><br><span class="line"><span class="function">Point3f <span class="title">p</span><span class="params">(Position)</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">OpenCV <code>Pnp</code> reference</a>.</p>
</blockquote>
<blockquote>
<p>Also note that: <code>Rodrigues</code> is essential, the output, <code>rvec</code> is (3x1), reference: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac">Rodrigues</a></p>
</blockquote>
<hr>
<h3 id="3-Object-Detection"><a href="#3-Object-Detection" class="headerlink" title="3. Object Detection"></a>3. Object Detection</h3><p>According to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5 Project</a>.</p>
<p>The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train on Custom Data</a>). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs <code>cv::dnn</code> that load <code>.onnx</code> models and can run forward actions, or deduction.</p>
<p>In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs.</p>
<h4 id="3-1-Dataset-Creation"><a href="#3-1-Dataset-Creation" class="headerlink" title="3.1. Dataset Creation"></a>3.1. Dataset Creation</h4><p>Simply follow steps described on <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">this page</a>. The output should be similar as below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data.yaml</span><br><span class="line">├── README.dataset.txt</span><br><span class="line">├── README.roboflow.txt</span><br><span class="line">├── test</span><br><span class="line">├── train</span><br><span class="line">└── valid</span><br><span class="line">9 directories, 382 files</span><br></pre></td></tr></table></figure>

<h4 id="3-2-Train-model"><a href="#3-2-Train-model" class="headerlink" title="3.2 Train model"></a>3.2 Train model</h4><p>Simply put datasets in yolov5 directory and perform the following command and sit back to wait for the results</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: You should clone yolov5 repo before training and of course set up python environment. </p>
</blockquote>
<h4 id="3-3-Convert-model-to-onnx"><a href="#3-3-Convert-model-to-onnx" class="headerlink" title="3.3 Convert model to .onnx"></a>3.3 Convert model to <code>.onnx</code></h4><p>YoloV5 takes PyTorch as backend, thus, the models are saved as <code>.pt</code> format. However, <code>cv::dnn</code> prefers <code>.onnx</code> format. Thus, a conversion shall be performed.</p>
<p>At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV <code>dnn</code> module. I have posted an <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/10575">Issue</a> on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project.</p>
<p>After the correction of Dependency, we perform the following command to export <code>.onnx</code> from <code>.pt</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --weights yolov5s.pt --include onnx</span><br></pre></td></tr></table></figure>

<h4 id="3-4-Load-onnx-with-OpenCV"><a href="#3-4-Load-onnx-with-OpenCV" class="headerlink" title="3.4 Load .onnx with OpenCV"></a>3.4 Load <code>.onnx</code> with OpenCV</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv::dnn::Net net = cv::dnn::<span class="built_in">readNetFromONNX</span>(model_path);</span><br><span class="line">net.<span class="built_in">setPreferableBackend</span>(cv::dnn::DNN_BACKEND_OPENCV);</span><br><span class="line">net.<span class="built_in">setPreferableTarget</span>(cv::dnn::DNN_TARGET_CPU);</span><br></pre></td></tr></table></figure>

<h4 id="3-5-Format-Input-Data"><a href="#3-5-Format-Input-Data" class="headerlink" title="3.5 Format Input Data"></a>3.5 Format Input Data</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat blob;</span><br><span class="line">int col = frame.cols;</span><br><span class="line">int row = frame.rows;</span><br><span class="line">int _max = max(col, row);</span><br><span class="line">input_img = cv::Mat::zeros(_max, _max, CV_8UC3);</span><br><span class="line">frame.copyTo(input_img(cv::Rect(0, 0, col, row)));</span><br><span class="line">cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false);</span><br></pre></td></tr></table></figure>

<h4 id="3-6-Forward-Network"><a href="#3-6-Forward-Network" class="headerlink" title="3.6 Forward Network"></a>3.6 Forward Network</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.setInput(input);</span><br><span class="line">std::vector&lt;cv::Mat&gt; outputs;</span><br><span class="line">net.forward(outputs, net.getUnconnectedOutLayersNames());</span><br></pre></td></tr></table></figure>

<h4 id="3-7-Format-the-output"><a href="#3-7-Format-the-output" class="headerlink" title="3.7 Format the output"></a>3.7 Format the output</h4><p>The output of the model, as the result of network-forwarding, is defined as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> structure of `output.data`</span><br><span class="line">┌─┬─┬─┬─┬─┬────────┬─────────────────────►</span><br><span class="line">│0│1│2│3│4│5 ......│dimensions</span><br><span class="line">├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►</span><br><span class="line">│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..</span><br><span class="line">└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──►</span><br><span class="line"> c: confidence</span><br></pre></td></tr></table></figure>
<p>Basically, it is an array that can be accessed via its address. The code is too large to be shown here.</p>
<h4 id="3-8-Transfer-Output-to-Detection"><a href="#3-8-Transfer-Output-to-Detection" class="headerlink" title="3.8 Transfer Output to Detection"></a>3.8 Transfer Output to <code>Detection</code></h4><p>For easy access, we transfer the output to the following data format.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Detection&#123;</span><br><span class="line">	int       class_id;   // Result&#x27;s class id</span><br><span class="line">	float     confidence; // Probability</span><br><span class="line">	cv::Rect  box;        // Where it is</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="3-9-Draw-Boxes-around-Targets"><a href="#3-9-Draw-Boxes-around-Targets" class="headerlink" title="3.9 Draw Boxes around Targets"></a>3.9 Draw Boxes around Targets</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int detection_size = output.size();</span><br><span class="line">for(int i=0;i&lt;detection_size;++i)&#123;</span><br><span class="line">	auto detection = output[i];</span><br><span class="line">	auto box = detection.box;</span><br><span class="line">	auto class_id = detection.class_id;</span><br><span class="line">	const auto color = color_list[class_id%color_list.size()];</span><br><span class="line">	cv::rectangle(frame, box, color, 2);</span><br><span class="line">	cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED);</span><br><span class="line">	cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-Plane-Transformation"><a href="#4-Plane-Transformation" class="headerlink" title="4. Plane Transformation"></a>4. Plane Transformation</h3><p>Finally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: <a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Display-of-Various-Coordinate-Systems-for-a-Computer-Vision-System-i-i-i-i-1-i_fig2_337311806">ResearchGate</a>)</p>
<p><img src="/img/image_2022-12-31-00-10-18.png" alt="Computer Vision Model"></p>
<p>Take the example of the transformation of Point $^1p_1$. The 3D position of it can be anywhere on the line of $O\ ^1p_1$, not necessary be at Point $^wp_1$ unless we require the 3D position lies on a particular plane.</p>
<p>Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground $XOY$ axis and not provide the height.</p>
<p>Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae"><code>getPerspectiveTransformation</code></a>. According to the definition below, the input <code>src</code> and <code>dst</code> must be vertices of a quadrangle. And the return value shall be the transformation matrix.</p>
<p><img src="/img/image_2022-12-31-00-19-10.png" alt="`getPerspectiveTransform` definition"></p>
<h4 id="4-1-Calculate-Transformation-Matrix"><a href="#4-1-Calculate-Transformation-Matrix" class="headerlink" title="4.1 Calculate Transformation Matrix"></a>4.1 Calculate Transformation Matrix</h4><p>In our project, we take 4 vertices of the chessboard to be the input.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; desk;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> d3:d3s) desk.<span class="built_in">push_back</span>(<span class="built_in">Point2f</span>(d3.x,d3.y));</span><br><span class="line">Point2f in[<span class="number">4</span>];</span><br><span class="line">Point2f out[<span class="number">4</span>];</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> helper(_in, in_) \</span></span><br><span class="line"><span class="meta">_in[0] = in_[i0]; \</span></span><br><span class="line"><span class="meta">_in[1] = in_[i1]; \</span></span><br><span class="line"><span class="meta">_in[2] = in_[i2]; \</span></span><br><span class="line"><span class="meta">_in[3] = in_[i3]</span></span><br><span class="line"><span class="type">int</span> i0 = <span class="number">0</span>, i1 = boardSize.width<span class="number">-1</span>;</span><br><span class="line"><span class="type">int</span> i2 = boardSize.width * (boardSize.height<span class="number">-1</span>);</span><br><span class="line"><span class="type">int</span> i3 = boardSize.width * boardSize.height - <span class="number">1</span>;</span><br><span class="line"><span class="built_in">helper</span>(in, d2s);</span><br><span class="line"><span class="built_in">helper</span>(out, desk);</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> helper</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> According to reference (Opencv), </span></span><br><span class="line"><span class="comment">// getPerspectiveTransform takes quadrangle vertices in the source image</span></span><br><span class="line">pix23D = <span class="built_in">getPerspectiveTransform</span>(in, out);</span><br></pre></td></tr></table></figure>

<h4 id="4-2-Calculate-Corresponding-3D-Position"><a href="#4-2-Calculate-Corresponding-3D-Position" class="headerlink" title="4.2 Calculate Corresponding 3D Position"></a>4.2 Calculate Corresponding 3D Position</h4><p><code>perspectiveTransformation</code> have already done for us.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; out;</span><br><span class="line">vector&lt;Point2f&gt; in; in.push_back(p2);</span><br><span class="line">cv::perspectiveTransform(in, out, pix23D);</span><br><span class="line">Point3f ret = Point3f(out[0].x,out[0].y,0);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Till now, the Project is half-way finished and seems pretty simple, only takes advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan. </p>
<p>The next aim of our project would be human-skeleton detection and action deduction with it. And finally, takes some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>KinectV2 Camera Calibration and `Yolov5` Recognition</p><p><a href="http://example.com/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/">http://example.com/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Christopher Liu</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2022-12-30</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-12-31</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Linux/">Linux</a><a class="link-muted mr-2" rel="tag" href="/tags/%E8%AE%B0%E5%BD%95/">记录</a><a class="link-muted mr-2" rel="tag" href="/tags/KinectV2/">KinectV2</a><a class="link-muted mr-2" rel="tag" href="/tags/Opencv/">Opencv</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/12/12/Source-Backup-for-Disk-Update/"><span class="level-item">Source Backup for Disk Update</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/myimg/yukinoshita_1.png" alt="Christopher Liu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Christopher Liu</p><p class="is-size-6 is-block">College Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Chengdu, Sichuan, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/chrisvicky" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/chrisvicky"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Background"><span class="level-left"><span class="level-item">1</span><span class="level-item">Background</span></span></a></li><li><a class="level is-mobile" href="#Project-Structure"><span class="level-left"><span class="level-item">2</span><span class="level-item">Project Structure</span></span></a></li><li><a class="level is-mobile" href="#Dependency"><span class="level-left"><span class="level-item">3</span><span class="level-item">Dependency</span></span></a></li><li><a class="level is-mobile" href="#Program-Usage"><span class="level-left"><span class="level-item">4</span><span class="level-item">Program Usage</span></span></a></li><li><a class="level is-mobile" href="#Developer-Diary"><span class="level-left"><span class="level-item">5</span><span class="level-item">Developer Diary</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-KinectV2-Connection"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">1. KinectV2 Connection</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-Define-Basic-Variables-either-globally-or-locally"><span class="level-left"><span class="level-item">5.1.1</span><span class="level-item">1.1. Define Basic Variables, either globally or locally.</span></span></a></li><li><a class="level is-mobile" href="#1-2-Initialize-the-device-via-certain-APIs"><span class="level-left"><span class="level-item">5.1.2</span><span class="level-item">1.2. Initialize the device via certain APIs</span></span></a></li><li><a class="level is-mobile" href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><span class="level-left"><span class="level-item">5.1.3</span><span class="level-item">1.3. We shall start a Loop to receive frames from the Device</span></span></a></li><li><a class="level is-mobile" href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><span class="level-left"><span class="level-item">5.1.4</span><span class="level-item">1.4. Before Exit, we need to manually stop and close the device</span></span></a></li><li><a class="level is-mobile" href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><span class="level-left"><span class="level-item">5.1.5</span><span class="level-item">1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing.</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-OpenCV-Calibration"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">2. OpenCV Calibration</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><span class="level-left"><span class="level-item">5.2.1</span><span class="level-item">2.1. Collect multiple frames where the camera and chessboard are relatively still.</span></span></a></li><li><a class="level is-mobile" href="#2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system"><span class="level-left"><span class="level-item">5.2.2</span><span class="level-item">2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system.</span></span></a></li><li><a class="level is-mobile" href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><span class="level-left"><span class="level-item">5.2.3</span><span class="level-item">2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</span></span></a></li><li><a class="level is-mobile" href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><span class="level-left"><span class="level-item">5.2.4</span><span class="level-item">2.4. Wrap them up and perform solvePnP to get rvec and tvec</span></span></a></li><li><a class="level is-mobile" href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><span class="level-left"><span class="level-item">5.2.5</span><span class="level-item">2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained.</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-Object-Detection"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">3. Object Detection</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-Dataset-Creation"><span class="level-left"><span class="level-item">5.3.1</span><span class="level-item">3.1. Dataset Creation</span></span></a></li><li><a class="level is-mobile" href="#3-2-Train-model"><span class="level-left"><span class="level-item">5.3.2</span><span class="level-item">3.2 Train model</span></span></a></li><li><a class="level is-mobile" href="#3-3-Convert-model-to-onnx"><span class="level-left"><span class="level-item">5.3.3</span><span class="level-item">3.3 Convert model to .onnx</span></span></a></li><li><a class="level is-mobile" href="#3-4-Load-onnx-with-OpenCV"><span class="level-left"><span class="level-item">5.3.4</span><span class="level-item">3.4 Load .onnx with OpenCV</span></span></a></li><li><a class="level is-mobile" href="#3-5-Format-Input-Data"><span class="level-left"><span class="level-item">5.3.5</span><span class="level-item">3.5 Format Input Data</span></span></a></li><li><a class="level is-mobile" href="#3-6-Forward-Network"><span class="level-left"><span class="level-item">5.3.6</span><span class="level-item">3.6 Forward Network</span></span></a></li><li><a class="level is-mobile" href="#3-7-Format-the-output"><span class="level-left"><span class="level-item">5.3.7</span><span class="level-item">3.7 Format the output</span></span></a></li><li><a class="level is-mobile" href="#3-8-Transfer-Output-to-Detection"><span class="level-left"><span class="level-item">5.3.8</span><span class="level-item">3.8 Transfer Output to Detection</span></span></a></li><li><a class="level is-mobile" href="#3-9-Draw-Boxes-around-Targets"><span class="level-left"><span class="level-item">5.3.9</span><span class="level-item">3.9 Draw Boxes around Targets</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-Plane-Transformation"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">4. Plane Transformation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#4-1-Calculate-Transformation-Matrix"><span class="level-left"><span class="level-item">5.4.1</span><span class="level-item">4.1 Calculate Transformation Matrix</span></span></a></li><li><a class="level is-mobile" href="#4-2-Calculate-Corresponding-3D-Position"><span class="level-left"><span class="level-item">5.4.2</span><span class="level-item">4.2 Calculate Corresponding 3D Position</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">6</span><span class="level-item">Summary</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Diary/"><span class="level-start"><span class="level-item">Diary</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Post/"><span class="level-start"><span class="level-item">Post</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-29T16:00:00.000Z">2022-12-30</time></p><p class="title"><a href="/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/">KinectV2 Camera Calibration and `Yolov5` Recognition</a></p><p class="categories"><a href="/categories/Post/">Post</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-11T16:00:00.000Z">2022-12-12</time></p><p class="title"><a href="/2022/12/12/Source-Backup-for-Disk-Update/">Source Backup for Disk Update</a></p><p class="categories"><a href="/categories/Post/">Post</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-09T12:23:09.000Z">2022-12-09</time></p><p class="title"><a href="/2022/12/09/Setup-Systemd-for-clash-Proxy/">Setup Systemd for clash Proxy</a></p><p class="categories"><a href="/categories/Post/">Post</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-12-01T00:05:28.000Z">2022-12-01</time></p><p class="title"><a href="/2022/12/01/ssh/">SSH Penetration Setup Memo</a></p><p class="categories"><a href="/categories/Post/">Post</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-17T16:00:00.000Z">2022-11-18</time></p><p class="title"><a href="/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/">Add Extra Memory to Thinkpad T14</a></p><p class="categories"><a href="/categories/Post/">Post</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Big-Data/"><span class="tag">Big Data</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Clash/"><span class="tag">Clash</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIY/"><span class="tag">DIY</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hadoop/"><span class="tag">Hadoop</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hardware/"><span class="tag">Hardware</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KinectV2/"><span class="tag">KinectV2</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Opencv/"><span class="tag">Opencv</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Performance-Improve/"><span class="tag">Performance Improve</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Systemd/"><span class="tag">Systemd</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VPN/"><span class="tag">VPN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/learn/"><span class="tag">learn</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ssh/"><span class="tag">ssh</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AA%B2%E7%A8%8B%E4%BD%9C%E6%A5%AD/"><span class="tag">課程作業</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%B0%E5%BD%95/"><span class="tag">记录</span><span class="tag">2</span></a></div></div></div></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">十二月 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/11/"><span class="level-start"><span class="level-item">十一月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Christopher Liu&#039;s Personal Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 Christopher Liu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/chrisvicky"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>
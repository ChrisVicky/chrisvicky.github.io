<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Paper Review: An Image is Worth 16x16 Words</title>
      <link href="/2023/01/16/An-Image-is-Worth-16x16-Words/"/>
      <url>/2023/01/16/An-Image-is-Worth-16x16-Words/</url>
      
        <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2023-01-15-22-28-11.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Title</div></center><ul><li>cite: Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</li><li>available online: <a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></li><li>Date: 2023-01-15 22:07</li></ul><h2 id="Brief-summary"><a href="#Brief-summary" class="headerlink" title="Brief summary"></a>Brief summary</h2><p><img src="/img/image_2023-01-15-22-17-58.png" alt="image_2023-01-15-22-17-58"></p><ul><li><p>What is the problem the paper is trying to solve?</p><ul><li>How can we reduce CNNs in transformer models in image classification tasks?</li><li>How to treat an Image as a sequence of data?</li></ul></li><li><p>What are the key ideas of the paper? Key insights?</p><ul><li>A new way to treat an image sequencially.</li><li>An image is divided into 16x16 2D patches and are transfered sequencially into an out-of-box self-attention transformer encoder, followed by MLP Heads which result in classification predictions.</li></ul></li><li><p>What is the key contribution to the literature at the time it was written?</p><ul><li>First to reduce entirely the CNNs in usage of transformer in image classification tasks and propose a pure transformer model called Vision Transformer (ViT).</li><li>Maybe the first to propose a sequencial treatment on an image.</li></ul></li><li><p>What are the most important things you take out from it?</p><ul><li>A possible way to treat an image as a sequence of data while preserving some relative information.</li><li>Designing easy-to-use models that utilize out-of-box interfaces could be a good way of conducting creative methods that solve difficult problems.</li></ul></li></ul><h2 id="Strengths-Most-Important-Ones"><a href="#Strengths-Most-Important-Ones" class="headerlink" title="Strengths (Most Important Ones)"></a>Strengths (Most Important Ones)</h2><ul><li>Does the paper solve the problem well?<ol><li>First of all, it is true that the solution, or more precisely, the new model proposed can be a possible solution that treat an image in a sequencial way.</li><li>Second, the computational latency can be low in this case since the division is as small as 16x16, and therefore the computational complexity can be relatively at a low level.</li></ol></li></ul><h2 id="Weaknesses-Most-Important-Ones"><a href="#Weaknesses-Most-Important-Ones" class="headerlink" title="Weaknesses (Most Important Ones)"></a>Weaknesses (Most Important Ones)</h2><ul><li>Room for improvement.<ol><li>The way to treat the image as a sequence of data is too straight-forward. The information on the boaders between patches can be lost.</li><li>The patches are produced with a fixed number of sequences, making the model difficult to perform on images with hight resolution, and since nowadays people have a preference of taking high resolution pictures, the methods proposed may not be practical in reallife without further improvements.</li><li>Also, the model requires a large quantity of data for training, significally improving the training costs.</li></ol></li></ul><h2 id="How-can-we-do-better-Your-ideas-and-thoughts"><a href="#How-can-we-do-better-Your-ideas-and-thoughts" class="headerlink" title="How can we do better? Your ideas and thoughts."></a>How can we do better? Your ideas and thoughts.</h2><ol><li><p>To include more relative information between patches, I guess one way is to introduce an additional division that stands on the boarder of the original one. A possible implementation is shown below.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">┌───┬───┬───┐   ┌─┬───────┬─┐    ┌─┬─┬───┬─┬─┐</span><br><span class="line">│   │   │   │   ├─┼───────┼─┤    ├─┼─┼───┼─┼─┤</span><br><span class="line">├───┼───┼───┤   │ │       │ │    ├─┼─┼───┼─┼─┤</span><br><span class="line">│   │   │   │ + │ │       │ │ =&gt; │ │ │   │ │ │</span><br><span class="line">├───┼───┼───┤   │ │       │ │    ├─┼─┼───┼─┼─┤</span><br><span class="line">│   │   │   │   ├─┼───────┼─┤    ├─┼─┼───┼─┼─┤</span><br><span class="line">└───┴───┴───┘   └─┴───────┴─┘    └─┴─┴───┴─┴─┘</span><br><span class="line">      A               B                C</span><br></pre></td></tr></table></figure><blockquote><p>Where <code>A</code> is the method in the paper and <code>B</code> is the additional sequence to provide extra information. But again, this additional patche can further worsen the situation of high overall computational costs.</p></blockquote></li><li><p>The high computational costs in high resolution image processing may be decreased by parellel hardware devices.</p></li></ol><h2 id="What-have-you-learnt-x2F-enjoyed-x2F-disliked-in-the-paper-Why"><a href="#What-have-you-learnt-x2F-enjoyed-x2F-disliked-in-the-paper-Why" class="headerlink" title="What have you learnt&#x2F;enjoyed&#x2F;disliked in the paper? Why?"></a>What have you learnt&#x2F;enjoyed&#x2F;disliked in the paper? Why?</h2><ul><li>The most impressive thing I take away from this paper is the methodology that the model is designed to utilize the API of original Transformer model which has been implemented and optimized for efficient performance. This way of design is simple, yet creative and meaningful.</li></ul><!-- !! No more than Half a pape -->]]></content>
      
      
      <categories>
          
          <category> PaperReview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tranformer </tag>
            
            <tag> AI </tag>
            
            <tag> ViT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KinectV2 Camera Calibration and `Yolov5` Recognition</title>
      <link href="/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"/>
      <url>/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/</url>
      
        <content type="html"><![CDATA[<ul><li>Project Repo: <a href="https://github.com/ChrisVicky/KinectV2-calibration-and-Yolov5-recognition">KinectV2 Camera Calibration and <code>Yolov5</code> Recognition</a></li><li>2022-12-30 19:11</li><li>This is a subproject from <a href="https://github.com/ChrisVicky/camera-position-solution">camera-position-solution</a>.</li></ul><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>We are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects);</p><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-01-04-00-07-18.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">KinectV2</div></center><p>In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates.</p><span id="more"></span><p><img src="/img/image_2022-12-30-19-10-06.jpg" alt="KinectV2"></p><h2 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build                         -- build Dir</span><br><span class="line">├── CMakeLists.txt                -- Top Cmake Configuration</span><br><span class="line">├── data                          </span><br><span class="line">│   └── output.mp4                -- Output Data -&gt; stacks of imshown frames</span><br><span class="line">├── default.xml                   -- Default Configuration File (example)</span><br><span class="line">├── include </span><br><span class="line">│   ├── calibration.hpp           -- Calibration -&gt; Future change: With Robot</span><br><span class="line">│   ├── define.hpp                -- Define COLORS etc</span><br><span class="line">│   ├── dnn.hpp                   -- Use OpenCV DNN APIs</span><br><span class="line">│   ├── main.hpp                  -- Main Program</span><br><span class="line">│   └── settings.hpp              -- Read Settings</span><br><span class="line">├── logsrc                        -- Log Helper by loguru</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── loguru.cpp</span><br><span class="line">│   └── loguru.hpp</span><br><span class="line">├── models                        -- Trained Yolov5 Modules</span><br><span class="line">│   ├── yolov5n.onnx</span><br><span class="line">│   ├── yolov5s.onnx</span><br><span class="line">│   ├── yolov5.xml</span><br><span class="line">│   └── yuki-bubu-2022-12-23.onnx </span><br><span class="line">├── README.md                     </span><br><span class="line">└── src                           </span><br><span class="line">    ├── calibration.cpp</span><br><span class="line">    ├── dnn.cpp</span><br><span class="line">    └── main.cpp</span><br><span class="line">21 directories, 87 files</span><br></pre></td></tr></table></figure><h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><p>  We use two libraries: <a href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> and <a href="https://github.com/opencv/opencv">OpenCV</a>.</p><blockquote><p>Note that: You should set <code>freenect2_DIR</code> and include <code>freenect2_INCLUDE</code> directions if you install <code>libfreenect2</code> in custom directories.</p></blockquote>  <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up libfreenect2</span></span><br><span class="line"><span class="comment"># Set include dir and DIR for libfreenect2 if it is not installed globally</span></span><br><span class="line"><span class="comment"># SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)</span></span><br><span class="line"><span class="comment"># include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)</span></span><br><span class="line"><span class="keyword">find_package</span>(freenect2 REQUIRED)</span><br></pre></td></tr></table></figure><h2 id="Program-Usage"><a href="#Program-Usage" class="headerlink" title="Program Usage"></a>Program Usage</h2><ol><li><p>Install Dependencies shown above</p></li><li><p>Run the following commands to build the project</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure></li><li><p>Plug in KinectV2 via USB (you might need a hub)<br><img src="/img/image_2022-12-30-19-11-30.png" alt="Hub"></p></li><li><p>run <code>./calibration</code> to start program<br><img src="/img/image_2022-12-30-21-19-15.png" alt="Screenshot"></p></li></ol><h2 id="Developer-Diary"><a href="#Developer-Diary" class="headerlink" title="Developer Diary"></a>Developer Diary</h2><p>  To meet the need, we have to conquer four difficulties.</p><ol><li>KinectV2 Connection</li><li>OpenCV Calibration</li><li>Object Detection</li><li>Planes Transformation</li></ol><h3 id="1-KinectV2-Connection"><a href="#1-KinectV2-Connection" class="headerlink" title="1. KinectV2 Connection"></a>1. KinectV2 Connection</h3><p>Since we develop the program on multiple Operating Systems (OS), we decide to take advantage of <a href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> which is open-sourced and supports Linux, Windows and Mac-OS. </p><blockquote><p>To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine.</p></blockquote><p>To use KinectV2, we need the following steps:</p><h4 id="1-1-Define-Basic-Variables-either-globally-or-locally"><a href="#1-1-Define-Basic-Variables-either-globally-or-locally" class="headerlink" title="1.1. Define Basic Variables, either globally or locally."></a>1.1. Define Basic Variables, either globally or locally.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">libfreenect2::Freenect2 freenect2;      <span class="comment">// libfreenect2 entity</span></span><br><span class="line">libfreenect2::PacketPipeline *pipeline; <span class="comment">// libfreenect2 pipeline</span></span><br><span class="line">libfreenect2::Freenect2Device *device;  <span class="comment">// device</span></span><br><span class="line"><span class="function">libfreenect2::SyncMultiFrameListener <span class="title">listener</span><span class="params">(libfreenect2::Frame::Color)</span></span>;</span><br><span class="line">libfreenect2::FrameMap frames;</span><br></pre></td></tr></table></figure><h4 id="1-2-Initialize-the-device-via-certain-APIs"><a href="#1-2-Initialize-the-device-via-certain-APIs" class="headerlink" title="1.2. Initialize the device via certain APIs"></a>1.2. Initialize the <code>device</code> via certain APIs</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* -------------------- START Kinectv2 Initialization -------------------- */</span></span><br><span class="line"><span class="keyword">if</span>(freenect2.<span class="built_in">enumerateDevices</span>() == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;no device connected!&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device connected&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">string serial = freenect2.<span class="built_in">getDefaultDeviceSerialNumber</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;SEARIAL Number: %s&quot;</span>,serial.<span class="built_in">c_str</span>());</span><br><span class="line">pipeline = <span class="keyword">new</span> libfreenect2::<span class="built_in">CpuPacketPipeline</span>();</span><br><span class="line">device = freenect2.<span class="built_in">openDevice</span>(serial, pipeline); <span class="keyword">if</span>(device == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;failed to open device: %s&quot;</span>, serial.<span class="built_in">c_str</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device opened&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">kinect_shutdown = <span class="literal">false</span>;</span><br><span class="line">device-&gt;<span class="built_in">setColorFrameListener</span>(&amp;listener);</span><br><span class="line">device-&gt;<span class="built_in">start</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device serial: %s&quot;</span> ,device-&gt;<span class="built_in">getSerialNumber</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device firmware: %s&quot;</span> ,device-&gt;<span class="built_in">getFirmwareVersion</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">/* -------------------- END Kinectv2 Initialization -------------------- */</span></span><br></pre></td></tr></table></figure><h4 id="1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><a href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device" class="headerlink" title="1.3. We shall start a Loop to receive frames from the Device"></a>1.3. We shall start a Loop to receive frames from the Device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!kinect_shutdown)&#123;</span><br><span class="line">  <span class="keyword">if</span>(!listener.<span class="built_in">waitForNewFrame</span>(frames, timeout))</span><br><span class="line">    <span class="built_in">LOG_F</span>(WARNING, <span class="string">&quot;Frame Received Failed after timeout: %d&quot;</span>, timeout);</span><br><span class="line">  libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color];</span><br><span class="line">  <span class="comment">/* -------------------- START Frame Processing -------------------- */</span></span><br><span class="line">  <span class="comment">/* -------------------- END Frame Processing -------------------- */</span></span><br><span class="line">  listener.<span class="built_in">release</span>(frames);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><a href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device" class="headerlink" title="1.4. Before Exit, we need to manually stop and close the device"></a>1.4. Before Exit, we need to manually stop and close the device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">device-&gt;<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure><blockquote><p>We must define a <code>sigint_handler</code> to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown.</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="type">int</span> s)</span></span>&#123;</span><br><span class="line">  device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">  device-&gt;<span class="built_in">close</span>();</span><br><span class="line">  <span class="built_in">exit</span>(s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Usage </span></span><br><span class="line"><span class="built_in">signal</span>(SIGINT, sigint_handler); <span class="comment">// Savely Close the Device before sudden exit</span></span><br></pre></td></tr></table></figure><h4 id="1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><a href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing" class="headerlink" title="1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing."></a>1.5. To take advantage of OpenCV APIs, we convert <code>libfreenect2::Frame</code> to <code>cv::Mat</code> right at the beginning of <code>Frame Processing</code>.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">Mat</span>(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).<span class="built_in">copyTo</span>(kinect_mat);</span><br><span class="line">cv::<span class="built_in">flip</span>(kinect_mat, kinect_mat, <span class="number">1</span>);</span><br><span class="line">rgb_mat = cv::Mat::<span class="built_in">zeros</span>(kinect_mat.<span class="built_in">size</span>(),CV_8UC3);</span><br><span class="line"><span class="built_in">mixChannels</span>(kinect_mat, rgb_mat, &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: <code>libfreenect2::Frame</code> contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a <code>mixChannels()</code> here to reduce the last one.</p></blockquote><hr><h3 id="2-OpenCV-Calibration"><a href="#2-OpenCV-Calibration" class="headerlink" title="2. OpenCV Calibration"></a>2. OpenCV Calibration</h3><p>To be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. <a href="https://github.com/ultralytics/yolov5">YoloV5</a>) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a <a href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">Perspective-n-Point(aka <code>PnP</code>)</a> problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally.</p><p>However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task.</p><p>We perform 4 steps to meet the need.</p><h4 id="2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><a href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still" class="headerlink" title="2.1. Collect multiple frames where the camera and chessboard are relatively still."></a>2.1. Collect multiple frames where the camera and chessboard are relatively still.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(STATE == STATE_START_CALIBRATION)</span><br><span class="line">  cali_frames.<span class="built_in">push_back</span>(rgb_mat);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: We use <code>STATE</code> to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM).</p></blockquote><h4 id="2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system"><a href="#2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system" class="headerlink" title="2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system."></a>2.2. Run <code>findPattern</code> to obtain feature points’ position in 2D-pixel-coordinate-system.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; point_buff;</span><br><span class="line"><span class="type">int</span> board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;</span><br><span class="line"><span class="type">int</span> found = <span class="built_in">findChessboardCorners</span>(rgb_mat, boardSize, point_buff, board_flag);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: We use <code>PThread</code> to accelerate the process, finding patterns in all collected frames at once.</p></blockquote><h4 id="2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><a href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information" class="headerlink" title="2.3. Collect all 2D information and calculate 3D-world-coordinate-system information."></a>2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> found=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;++i)&#123;</span><br><span class="line">  <span class="type">void</span> * ret;</span><br><span class="line">  <span class="built_in">pthread_join</span>(thread_ids[i], &amp;ret);</span><br><span class="line">  runCalibrationRet retVal = *(runCalibrationRet*) ret;</span><br><span class="line">  <span class="keyword">if</span>(retVal.found)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!found)</span><br><span class="line">      d2s = retVal.d2;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;d2s.<span class="built_in">size</span>();j++) d2s[j] += retVal.d2[j];</span><br><span class="line">    found ++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;boardSize.height; ++i)</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;boardSize.width; ++j)</span><br><span class="line">    d3s.<span class="built_in">push_back</span>(<span class="built_in">Point3f</span>(j*squareSize, i*squareSize, <span class="number">0</span>));</span><br></pre></td></tr></table></figure><blockquote><p>Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system.</p></blockquote><h4 id="2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><a href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec" class="headerlink" title="2.4. Wrap them up and perform solvePnP to get rvec and tvec"></a>2.4. Wrap them up and perform <code>solvePnP</code> to get <code>rvec</code> and <code>tvec</code></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">solvePnP</span>(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec);</span><br></pre></td></tr></table></figure><blockquote><p>Note that:<code>camera_matrix</code> and <code>dist_coeffs</code> are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it.</p></blockquote><h4 id="2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><a href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained" class="headerlink" title="2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained."></a>2.5. To obtain camera position, we still need another step that takes both <code>rvec</code> and <code>tvec</code> as input and <code>camera_position</code> would be obtained.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> type_tv = tv.<span class="built_in">type</span>();</span><br><span class="line"><span class="function">Mat <span class="title">rvf</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="comment">// Convert from vector rv(3x1) to matrix rotation(3x3)</span></span><br><span class="line"><span class="built_in">Rodrigues</span>(rv, rvf);</span><br><span class="line"><span class="comment">// The Inversed Matrix</span></span><br><span class="line"><span class="function">Mat <span class="title">rvf_1</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="built_in">invert</span>(rvf, rvf_1, DECOMP_SVD);</span><br><span class="line">Mat Position = rvf_1 * (-tv);</span><br><span class="line"><span class="function">Point3f <span class="title">p</span><span class="params">(Position)</span></span>;</span><br></pre></td></tr></table></figure><blockquote><p>Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: <a href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">OpenCV <code>Pnp</code> reference</a>.</p></blockquote><blockquote><p>Also note that: <code>Rodrigues</code> is essential, the output, <code>rvec</code> is (3x1), reference: <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac">Rodrigues</a></p></blockquote><hr><h3 id="3-Object-Detection"><a href="#3-Object-Detection" class="headerlink" title="3. Object Detection"></a>3. Object Detection</h3><p>According to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous <a href="https://github.com/ultralytics/yolov5">YoloV5 Project</a>.</p><p>The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: <a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train on Custom Data</a>). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs <code>cv::dnn</code> that load <code>.onnx</code> models and can run forward actions, or deduction.</p><p>In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs.</p><h4 id="3-1-Dataset-Creation"><a href="#3-1-Dataset-Creation" class="headerlink" title="3.1. Dataset Creation"></a>3.1. Dataset Creation</h4><p>Simply follow steps described on <a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">this page</a>. The output should be similar as below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data.yaml</span><br><span class="line">├── README.dataset.txt</span><br><span class="line">├── README.roboflow.txt</span><br><span class="line">├── test</span><br><span class="line">├── train</span><br><span class="line">└── valid</span><br><span class="line">9 directories, 382 files</span><br></pre></td></tr></table></figure><h4 id="3-2-Train-model"><a href="#3-2-Train-model" class="headerlink" title="3.2 Train model"></a>3.2 Train model</h4><p>Simply put datasets in yolov5 directory and perform the following command and sit back to wait for the results</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure><blockquote><p>Note that: You should clone yolov5 repo before training and of course set up python environment. </p></blockquote><h4 id="3-3-Convert-model-to-onnx"><a href="#3-3-Convert-model-to-onnx" class="headerlink" title="3.3 Convert model to .onnx"></a>3.3 Convert model to <code>.onnx</code></h4><p>YoloV5 takes PyTorch as backend, thus, the models are saved as <code>.pt</code> format. However, <code>cv::dnn</code> prefers <code>.onnx</code> format. Thus, a conversion shall be performed.</p><p>At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV <code>dnn</code> module. I have posted an <a href="https://github.com/ultralytics/yolov5/issues/10575">Issue</a> on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project.</p><p>After the correction of Dependency, we perform the following command to export <code>.onnx</code> from <code>.pt</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --weights yolov5s.pt --include onnx</span><br></pre></td></tr></table></figure><h4 id="3-4-Load-onnx-with-OpenCV"><a href="#3-4-Load-onnx-with-OpenCV" class="headerlink" title="3.4 Load .onnx with OpenCV"></a>3.4 Load <code>.onnx</code> with OpenCV</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv::dnn::Net net = cv::dnn::<span class="built_in">readNetFromONNX</span>(model_path);</span><br><span class="line">net.<span class="built_in">setPreferableBackend</span>(cv::dnn::DNN_BACKEND_OPENCV);</span><br><span class="line">net.<span class="built_in">setPreferableTarget</span>(cv::dnn::DNN_TARGET_CPU);</span><br></pre></td></tr></table></figure><h4 id="3-5-Format-Input-Data"><a href="#3-5-Format-Input-Data" class="headerlink" title="3.5 Format Input Data"></a>3.5 Format Input Data</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat blob;</span><br><span class="line">int col = frame.cols;</span><br><span class="line">int row = frame.rows;</span><br><span class="line">int _max = max(col, row);</span><br><span class="line">input_img = cv::Mat::zeros(_max, _max, CV_8UC3);</span><br><span class="line">frame.copyTo(input_img(cv::Rect(0, 0, col, row)));</span><br><span class="line">cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false);</span><br></pre></td></tr></table></figure><h4 id="3-6-Forward-Network"><a href="#3-6-Forward-Network" class="headerlink" title="3.6 Forward Network"></a>3.6 Forward Network</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.setInput(input);</span><br><span class="line">std::vector&lt;cv::Mat&gt; outputs;</span><br><span class="line">net.forward(outputs, net.getUnconnectedOutLayersNames());</span><br></pre></td></tr></table></figure><h4 id="3-7-Format-the-output"><a href="#3-7-Format-the-output" class="headerlink" title="3.7 Format the output"></a>3.7 Format the output</h4><p>The output of the model, as the result of network-forwarding, is defined as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> structure of `output.data`</span><br><span class="line">┌─┬─┬─┬─┬─┬────────┬─────────────────────►</span><br><span class="line">│0│1│2│3│4│5 ......│dimensions</span><br><span class="line">├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►</span><br><span class="line">│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..</span><br><span class="line">└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──►</span><br><span class="line"> c: confidence</span><br></pre></td></tr></table></figure><p>Basically, it is an array that can be accessed via its address. The code is too large to be shown here.</p><h4 id="3-8-Transfer-Output-to-Detection"><a href="#3-8-Transfer-Output-to-Detection" class="headerlink" title="3.8 Transfer Output to Detection"></a>3.8 Transfer Output to <code>Detection</code></h4><p>For easy access, we transfer the output to the following data format.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Detection&#123;</span><br><span class="line">int       class_id;   // Result&#x27;s class id</span><br><span class="line">float     confidence; // Probability</span><br><span class="line">cv::Rect  box;        // Where it is</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="3-9-Draw-Boxes-around-Targets"><a href="#3-9-Draw-Boxes-around-Targets" class="headerlink" title="3.9 Draw Boxes around Targets"></a>3.9 Draw Boxes around Targets</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int detection_size = output.size();</span><br><span class="line">for(int i=0;i&lt;detection_size;++i)&#123;</span><br><span class="line">auto detection = output[i];</span><br><span class="line">auto box = detection.box;</span><br><span class="line">auto class_id = detection.class_id;</span><br><span class="line">const auto color = color_list[class_id%color_list.size()];</span><br><span class="line">cv::rectangle(frame, box, color, 2);</span><br><span class="line">cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED);</span><br><span class="line">cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="4-Plane-Transformation"><a href="#4-Plane-Transformation" class="headerlink" title="4. Plane Transformation"></a>4. Plane Transformation</h3><p>Finally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: <a href="https://www.researchgate.net/figure/Display-of-Various-Coordinate-Systems-for-a-Computer-Vision-System-i-i-i-i-1-i_fig2_337311806">ResearchGate</a>)</p><p><img src="/img/image_2022-12-31-00-10-18.png" alt="Computer Vision Model"></p><p>Take the example of the transformation of Point $^1p_1$. The 3D position of it can be anywhere on the line of $O\ ^1p_1$, not necessary be at Point $^wp_1$ unless we require the 3D position lies on a particular plane.</p><p>Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground $XOY$ axis and not provide the height.</p><p>Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: <a href="https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae"><code>getPerspectiveTransformation</code></a>. According to the definition below, the input <code>src</code> and <code>dst</code> must be vertices of a quadrangle. And the return value shall be the transformation matrix.</p><p><img src="/img/image_2022-12-31-00-19-10.png" alt="`getPerspectiveTransform` definition"></p><h4 id="4-1-Calculate-Transformation-Matrix"><a href="#4-1-Calculate-Transformation-Matrix" class="headerlink" title="4.1 Calculate Transformation Matrix"></a>4.1 Calculate Transformation Matrix</h4><p>In our project, we take 4 vertices of the chessboard to be the input.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; desk;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> d3:d3s) desk.<span class="built_in">push_back</span>(<span class="built_in">Point2f</span>(d3.x,d3.y));</span><br><span class="line">Point2f in[<span class="number">4</span>];</span><br><span class="line">Point2f out[<span class="number">4</span>];</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> helper(_in, in_) \</span></span><br><span class="line"><span class="meta">_in[0] = in_[i0]; \</span></span><br><span class="line"><span class="meta">_in[1] = in_[i1]; \</span></span><br><span class="line"><span class="meta">_in[2] = in_[i2]; \</span></span><br><span class="line"><span class="meta">_in[3] = in_[i3]</span></span><br><span class="line"><span class="type">int</span> i0 = <span class="number">0</span>, i1 = boardSize.width<span class="number">-1</span>;</span><br><span class="line"><span class="type">int</span> i2 = boardSize.width * (boardSize.height<span class="number">-1</span>);</span><br><span class="line"><span class="type">int</span> i3 = boardSize.width * boardSize.height - <span class="number">1</span>;</span><br><span class="line"><span class="built_in">helper</span>(in, d2s);</span><br><span class="line"><span class="built_in">helper</span>(out, desk);</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> helper</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> According to reference (Opencv), </span></span><br><span class="line"><span class="comment">// getPerspectiveTransform takes quadrangle vertices in the source image</span></span><br><span class="line">pix23D = <span class="built_in">getPerspectiveTransform</span>(in, out);</span><br></pre></td></tr></table></figure><h4 id="4-2-Calculate-Corresponding-3D-Position"><a href="#4-2-Calculate-Corresponding-3D-Position" class="headerlink" title="4.2 Calculate Corresponding 3D Position"></a>4.2 Calculate Corresponding 3D Position</h4><p><code>perspectiveTransformation</code> have already done for us.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; out;</span><br><span class="line">vector&lt;Point2f&gt; in; in.push_back(p2);</span><br><span class="line">cv::perspectiveTransform(in, out, pix23D);</span><br><span class="line">Point3f ret = Point3f(out[0].x,out[0].y,0);</span><br></pre></td></tr></table></figure><hr><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Till now, the Project is half-way finished and seems pretty simple, only taking advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan. </p><p>The next aim of our project would be human-skeleton detection and action deduction with it. And finally, adding some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.</p>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 记录 </tag>
            
            <tag> KinectV2 </tag>
            
            <tag> Opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Source Backup for Disk Update</title>
      <link href="/2022/12/12/Source-Backup-for-Disk-Update/"/>
      <url>/2022/12/12/Source-Backup-for-Disk-Update/</url>
      
        <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li>作为大学生，只有一台 Thinkpad T14 Gen 1 的笔记本却安装了 Arch Linux，而由于课程需要仍保留了大部分 Windows 11 的系统，长期以来的使用，让 512 GB 的存储空间告急，于是打算更换一块 1T 的固态硬盘。在此对磁盘更换做一个记录。</li></ul><span id="more"></span><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><h3 id="文件分类"><a href="#文件分类" class="headerlink" title="文件分类"></a>文件分类</h3><ul><li><p>第一大类：系统文件，</p><ul><li>如 Windows 等 Operating Systems 的文件</li><li>不考虑保存，但需要先查清除如何对 Windows 11 下的 Office 全家桶进行恢复。</li></ul></li><li><p>第二大类：工程文件；</p><ul><li>2-1. 项目、代码等<ul><li>存入 github 代码库，将不必要保存的 tmp 文件删除</li><li>写好 README 文档</li></ul></li><li>2-2. 安装包、dependency<ul><li>丢弃，实际使用时再安装</li></ul></li></ul></li><li><p>第三大类：资源文件：</p><ul><li>3-1. 课程文件等<ul><li>打包按时间排序</li><li>项目类文件保存两份</li></ul></li><li>3-2. 配置文件<ul><li>重要配置打包或上传 github</li></ul></li><li>3-3. 字体<ul><li>不能打包，但需要确定安装和配置方案</li></ul></li><li>3-3. 其他资源<ul><li>写个 README</li></ul></li></ul></li></ul><h3 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h3><ol><li><p>文件备份</p><ul><li>根据上述分类进行备份</li><li>2022-12-12 23:38 DONE</li></ul></li><li><p>确定安装和磁盘分区</p><ul><li>安装 512 GB 的 Windows 11 系统 和 512 GB 的 Linux 系统 </li><li>Linux 安装 DWM + Xorg （需要先写好安装文档以便查看）</li></ul></li><li><p>购置固态硬盘</p><ul><li>选取三星 “980 Pro PCIe 4.0 NVMe M.2”</li><li>2022-12-12 23:38 次日到貨</li></ul></li><li><p>对需要安装的系统和软件、配置写教程</p></li></ol><table><thead><tr><th>待安装（配置）的内容</th><th>教程</th><th>备注</th></tr></thead><tbody><tr><td>Arch Linux</td><td><a href="https://www.viseator.com/2017/05/17/arch_install/">viseator的博客</a></td><td></td></tr><tr><td>dwm</td><td></td><td>拷贝文件即可, 应当包括 autostart</td></tr><tr><td>dmenu</td><td></td><td>同上</td></tr><tr><td>vim &#x2F; nvim</td><td></td><td>同上</td></tr><tr><td>中文输入法</td><td><a href="https://www.viseator.com/2017/07/02/arch_more/">中文輸入法等安裝</a></td><td></td></tr><tr><td>yay</td><td><a href="https://aur.archlinux.org/yay.git">yay git repo</a></td><td></td></tr><tr><td>clash</td><td><a href="https://chrisvicky.github.io/2022/12/09/Setup-Systemd-for-clash-Proxy/">我的博客</a></td><td></td></tr><tr><td>navicat</td><td></td><td>已經完成拷貝</td></tr><tr><td>apifox</td><td></td><td></td></tr><tr><td>vivado</td><td><a href="https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/vivado-design-tools/archive.html">Vivado HLx 2019.2: All OS installer Single-File Download (TAR&#x2F;GZIP - 26.55 GB)</a></td><td></td></tr><tr><td>libfreenect2</td><td><a href="https://github.com/OpenKinect/libfreenect2">libfreenect2 Github</a>!!</td><td>十分重要</td></tr><tr><td>opencv</td><td></td><td>yay 将就用一下先</td></tr><tr><td>virtualbox</td><td><a href="https://chrisvicky.github.io/2022/11/15/deploy-hadoop/">我的博客</a></td><td></td></tr></tbody></table><hr>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Setup Systemd for clash Proxy</title>
      <link href="/2022/12/09/Setup-Systemd-for-clash-Proxy/"/>
      <url>/2022/12/09/Setup-Systemd-for-clash-Proxy/</url>
      
        <content type="html"><![CDATA[<ul><li>Since I live in China mainland, some specific websites can’t get accessed without a <code>Vpn</code> service running on the laptop. I managed to get one using <a href="https://github.com/Dreamacro/clash">clash</a> and <a href="ikuuu.live">configuration</a>. However, I have to run <code>./clash -d .</code> manually every-time I need access, which is inconvenient. Since I’ve been using Linux, I did some search and managed to set up a system task that runs automatically after booting. Here is the Memo</li></ul><span id="more"></span><ol><li>Add <code>clash@.service</code> in <code>/usr/lib/systemd/system/</code> or in <code>/etc/systemd/system/</code></li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2022-12-09-20-23-23.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Example</div></center><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=A rule based proxy in Go for %i.</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=%i</span><br><span class="line">Restart=on-abort</span><br><span class="line">ExecStart=/home/christopher/.config/clash/clash</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Use <code>systemctl</code> to enable and start mission.</li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2022-12-09-20-38-10.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Systemd Management</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> VPN </tag>
            
            <tag> Clash </tag>
            
            <tag> Systemd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSH Penetration Setup Memo</title>
      <link href="/2022/12/01/ssh/"/>
      <url>/2022/12/01/ssh/</url>
      
        <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li>We have deployed a web application demo on a computer whose network access is limited within school and cannot be accessed from devices outside. Plus that we know from the man page of ssh that ssh -R could perform a proxy that transfers data stream from one port to another, thus, we decide to make use of a server bought from <a href="https://aliyun.com/">Aliyun</a> as a repeater, or bridge, that connects our clients to the application.</li></ul><span id="more"></span><h2 id="How-to-do-it"><a href="#How-to-do-it" class="headerlink" title="How to do it"></a>How to do it</h2><ul><li><p>For specification, we call the computer with the deployment of our web application as <code>A</code> and its port as <code>AP</code> while the server is called <code>B</code>, <code>BP1</code>, <code>BP2</code>.</p></li><li><p>First on computer <code>A</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -fCNR `BP1`:localhost:`AP` -o ServerAliveInterval=60 serverName@serverIP</span><br></pre></td></tr></table></figure></li><li><p>On Server</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -fCNL 0.0.0.0:`BP2`:localhost:`BP1` localhost</span><br></pre></td></tr></table></figure></li><li><p>It works like this</p><!-- ``` --><!--                       Server B                                    Our Computer in School --><!--                 ┌──────────────────────┐                    ┌───────────────────────────┐ --><!--                 │                      │                    │                           │ --><!--       ─────► ┌──┴──┐ ──────────────► ┌─┴───┐ ──────────► ┌──┴─┐  bind                   │ --><!-- requests     │ BP2 │  inside Server  │ BP1 │  Via SSH    │ AP │ ◄────► Web Application  │ --><!--       ◄───── └──┬──┘ ◄────────────── └─┬───┘ ◄────────── └──┬─┘                         │ --><!--                 │                      │                    │                           │ --><!--                 └──────────────────────┘                    └───────────────────────────┘ --><!----><!-- ``` --></li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-12-01-16-33-31.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Graph</div></center><ul><li>Access through internet<center><img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"  src="/img/image_2022-12-01-16-23-35.png"><br><div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Access</div></center></li></ul>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh </tag>
            
            <tag> learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add Extra Memory to Thinkpad T14</title>
      <link href="/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/"/>
      <url>/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/</url>
      
        <content type="html"><![CDATA[<p>Working on hard projects with Hadoop cluster really runs out the limit of my poorly configured laptop. Therefore, I decide to add an extra memory card to it to boost the performance.</p><span id="more"></span><h2 id="1-Check-the-hardware-information"><a href="#1-Check-the-hardware-information" class="headerlink" title="1. Check the hardware information"></a>1. Check the hardware information</h2><ul><li>Before cracking down the shell, we should figure out whether my laptop has a spare set for the extra memory and what kinds of memory card should I buy.</li><li>To do that, we perform the following command to monitor the hardware status<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dmidecode memory &gt; memory.log</span><br></pre></td></tr></table></figure></li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-12-06-21-38-11.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">command output</div></center><blockquote><p>Note: I’ve accidentally removed <code>memory.log</code> which generated before adding the extra memory, and this screenshot is taken after the card is inserted.</p></blockquote><ul><li>According to the output, the maximum Capacity of Memory of my laptop is 32 GB, meaning that it is capable of accepting an extra memory of 16 GB.</li><li>Another important information here is the speed. Because both memory cards will be accessed with the same bus, the slower speed could be a bottle net.<blockquote><p>I bought a memory card from Samsung with the same speed and data width for 300 RMB on Taobao.</p></blockquote></li></ul><h2 id="2-Opening-the-Laptop"><a href="#2-Opening-the-Laptop" class="headerlink" title="2. Opening the Laptop"></a>2. Opening the Laptop</h2><ul><li>Using tools from the shop, the laptop is rather easy to tear. The only thing to mention here is that, be gentle not to break the machine down and after insertion, do not fix the screw before the evaluation process succeeds.</li></ul><blockquote><p>In fact, the work can be down with a screwdriver, and a student ID card.</p></blockquote><h2 id="3-Evaluation"><a href="#3-Evaluation" class="headerlink" title="3.  Evaluation"></a>3.  Evaluation</h2><ul><li>After insertion, boot the machine and enter the Operating System to check whether the extra Memory works well.</li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-18-11-20-48.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Successfully Boot up Heavy Virtual Machines after Installation of the Additional 16GB Memory Card</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DIY </tag>
            
            <tag> Hardware </tag>
            
            <tag> Performance Improve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deploy Hadoop with VirtualBox</title>
      <link href="/2022/11/15/deploy-hadoop/"/>
      <url>/2022/11/15/deploy-hadoop/</url>
      
        <content type="html"><![CDATA[<ul><li>This post is a reminder of hadoop deployment on Arch Linux and virtualbox (with vagrant)</li><li><a href="https://github.com/ChrisVicky/hadoop-vm">Related Github Repo contains resource:</a></li></ul><span id="more"></span><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-16-00-30-34.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">system information</div></center><hr><h2 id="Pre-Requirement"><a href="#Pre-Requirement" class="headerlink" title="Pre-Requirement"></a>Pre-Requirement</h2><ol><li>VirtualBox</li><li>vagrant</li></ol><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-23-47-12.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">virtualbox and vagrant</div></center><ol start="3"><li>Access to Internet<ul><li>The base OS we choose to deploy Hadoop Cluster is Ubuntu 16, thus we should be able to fetch “ubuntu&#x2F;xenial64” from <a href="https://app.vagrantup.com/boxes/search">vagrant cloud</a>.</li><li>Also, the first step in your vm is to update and install necessary applications such as <code>ssh</code>, <code>rsync</code> and <code>vim</code>.</li></ul></li></ol><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><h3 id="1-Ingredients-checkup"><a href="#1-Ingredients-checkup" class="headerlink" title="1. Ingredients checkup"></a>1. Ingredients checkup</h3><ul><li>Make sure the Directory has the following structure</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── cache                               -- Files to replace in VM</span><br><span class="line">│   ├── core-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/core-site.xml in VM</span><br><span class="line">│   ├── hadoop-2.9.0.tar.gz             -- hadoop </span><br><span class="line">│   ├── hdfs-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/hdfs-site.xml in VM </span><br><span class="line">│   ├── hosts                           -- replace /usr/hosts in VM</span><br><span class="line">│   ├── jdk-8u161-linux-x64.tar.gz      -- jdk package</span><br><span class="line">│   ├── mapred-site.xml                 -- replace /usr/local/hadoop/etc/hadoop/mapred-site.xml</span><br><span class="line">│   ├── scala-2.11.8.tgz                -- scala package</span><br><span class="line">│   ├── sources.list                    -- replace /etc/apt/sources.list -&gt; from https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</span><br><span class="line">│   ├── spark-2.3.0-bin-hadoop2.7.tgz   -- spark package</span><br><span class="line">│   └── yarn-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">├── hadoop+spark集群平台搭建.pptx       -- ppt instruction for Hadoop + spark on VMware</span><br><span class="line">├── Hadoop集群安装手册.pdf              -- PDF instruction for Hadoop on VMware based on CentOS</span><br><span class="line">├── hadoop集群搭建.pptx                 -- ppt instruction for Hadoop on VMware based on Ubuntu -- Instruction for this Virtualbox Version</span><br><span class="line">├── img                                 -- IMGs in this readme file</span><br><span class="line">├── init.sh                             -- Scripts to execute when VM first starts</span><br><span class="line">├── README.md                           -- This file</span><br><span class="line">└── Vagrantfile                         -- VM Configurations</span><br><span class="line"></span><br><span class="line">2 directories, 26 files</span><br></pre></td></tr></table></figure><h3 id="2-Check-init-sh-and-Vagrantfile-for-certain-configurations"><a href="#2-Check-init-sh-and-Vagrantfile-for-certain-configurations" class="headerlink" title="2. Check init.sh and Vagrantfile for certain configurations"></a>2. Check init.sh and Vagrantfile for certain configurations</h3><ol><li>The default configuration for Virtual Machines is written in <code>Vagrantfile</code>. Modifications can be made by changing the code directly.</li><li>Check up init.sh for more setups.</li></ol><h3 id="3-Execute-Vagrant-Up"><a href="#3-Execute-Vagrant-Up" class="headerlink" title="3. Execute Vagrant Up"></a>3. Execute Vagrant Up</h3><ul><li><p>In the directory shown above, execute <code>vagrant up</code> to setup and boot your vm.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant up </span><br></pre></td></tr></table></figure><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"      src="/img/image_2022-11-15-22-04-25.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Commands to Execute</div></center></li><li><p>It should take a while. So have a cup of tea and when everything is settled, check your virutal machine with either <code>vagrant status</code> or <code>virtualbox user interface</code></p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant status</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-07-10.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">vagrant status</div></center><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-08-01.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">virtualbox user interface</div></center><h3 id="4-ssh-configurations"><a href="#4-ssh-configurations" class="headerlink" title="4. ssh configurations"></a>4. ssh configurations</h3><ol><li><p>use <code>vagrant ssh master</code> to enter master virtual machine.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure><center> <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/image_2022-11-15-22-11-20.png"><br> <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">vagrant ssh master</div></center></li><li><p>Append public keys to authorized_keys by <code>cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><blockquote><p>Note that all three public keys have been generated and pasted in <code>/vagrant/cache/authorized_keys</code> by commands in <code>init.sh</code> and <code>Vagrantfile</code>.</p></blockquote></li><li><p>Ssh configuration should be done in both slaves as well.</p></li><li><p>Varify ssh configuration by executing <code>ssh slave1</code> in <code>master</code> virtual machine. You sohuld log into slave1 without entering password.</p><center> <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="/img/image_2022-11-15-22-22-03.png"><br> <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">log into slave1 without password after ssh configuration</div></center></li></ol><h2 id="Deploy-and-Varify-Hadoop"><a href="#Deploy-and-Varify-Hadoop" class="headerlink" title="Deploy and Varify Hadoop"></a>Deploy and Varify Hadoop</h2><h3 id="1-Deploy-Hadoop"><a href="#1-Deploy-Hadoop" class="headerlink" title="1. Deploy Hadoop"></a>1. Deploy Hadoop</h3><ul><li><p>run <code>hadoop namenode -format</code> in <code>master</code> virtual machine to configure node information.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure></li><li><p>run <code>start-all.sh</code> to start deployment. Always remember to run <code>stop-all.sh</code> before virtual machine shutdown.</p></li></ul><h3 id="2-Varification"><a href="#2-Varification" class="headerlink" title="2. Varification"></a>2. Varification</h3><ul><li>run <code>hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5</code> in directory <code>/usr/local/hadoop/share/hadoop/mapreduce</code> to varify.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5</span><br></pre></td></tr></table></figure><blockquote><p>A number relatively close to pi is then presented. To be more accurate on the result, try running <code>pi 10 10000</code> which takes a longer period.</p></blockquote></li></ul><h3 id="3-User-Interface"><a href="#3-User-Interface" class="headerlink" title="3. User Interface"></a>3. User Interface</h3><ul><li>Hadoop Environment Configuration on <code>IP:50070</code> where IP is the static IP for Master. And on <code>IP:8088</code> where IP is the static IP of Master and <code>8088</code> can be configured in those <code>.xml</code> files.</li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-33-19.png"><br><br>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-33-51.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">UI</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Big Data </tag>
            
            <tag> 課程作業 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About Me</title>
      <link href="/2022/11/12/aboutme/"/>
      <url>/2022/11/12/aboutme/</url>
      
        <content type="html"><![CDATA[<p>Hello, and welcome to my blog. An introduction about myself is written here with my scarce English vocabulary.</p><p>I am a 20-year-old college student majoring in Computer Science who dreamed about designing a computer architecture that would contribute to the development of advanced technology. I once was a Windows user but was obsessed with Linux immediately after my very first installation of Ubuntu for my scientific research requirement. After about 3 months, when I learnt about the architecture and components of computers as well as the Operating System in school, I started to switch from Ubuntu to Arch Linux, which, until now, is my first choice every time I turn the power on. </p><span id="more"></span><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-14-12-20-13.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">My Arch Linux & Dwm</div></center><p>I said I was passionate and wanted to design computer architectures. However, as a college student with limited choices of topic and professors, the project that I have conducted is related to another area, UAV (unmanned aerial vehicle) swam control, in which we proposed a de-centralised method and algorithm that enable a group of drones to explore an unknown environment and make decisions based on the algorithm that is embedded in every individual.</p><p>Right now, I am involved in a project that develops a system with multiple technologies, which, from my point of view, shall be better for industrial rather than for scientific purposes since the idea and method we conducted are nothing special but rather old-school while the system that we design is somehow new to the industrial world to the best of our knowledge.</p><p>All those experience of ‘doing lab’ and ‘debuging’ seems, from my point of view, to be a scientific skill training. That is, the knowledge is not the centric but the habit and lessons related to the insight of ‘doing lab’ is what I gains through it.</p><p>Other experiences includes ACM and Web Backend Development will be written later.</p>]]></content>
      
      
      <categories>
          
          <category> Diary </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>

<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>School List</title>
      <link href="/2023/09/25/School-List/"/>
      <url>/2023/09/25/School-List/</url>
      
        <content type="html"><![CDATA[<h1>Notes</h1><ul><li>Universities in Quebec will not be considered since French plays a vital part in immigration policies of the province.</li></ul><h1>Schools</h1><h2 id="university-of-toronto">University of Toronto</h2><h3 id="rankings">Rankings</h3><ul><li>#1 (Provincial)</li><li>#21 (QS)</li><li>#18 (US News)</li></ul><h3 id="location">Location</h3><ul><li>Toronto, Ontari</li></ul><h3 id="programs"><a href="https://www.sgs.utoronto.ca/programs/">Programs</a></h3><ul><li>General English Requirement<ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-51-39.png" alt="image_2023-09-23-20-51-39"></li></ul></li><li>General Regulations: <a href="https://sgs.calendar.utoronto.ca/general-regulations">https://sgs.calendar.utoronto.ca/general-regulations</a></li><li>NOTE: (2023-09-23) No GRE specifications are found in the Programs of <code>UofT</code>. The word <code>GRE</code> does not even appear in the webpage.</li></ul><h4 id="mscac-cs"><a href="https://www.sgs.utoronto.ca/programs/applied-computing/">MScAC - CS</a></h4><ul><li><strong>Length: 4 Sessions</strong></li><li>DDL: Dec/1/2023</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-19-42-54.png" alt="image_2023-09-23-19-42-54"></li></ul><h5 id="general-requirements">General Requirements</h5><ul><li>English<ul><li>TOEFL: 93/120 and 22/30 (W/S)</li><li>IELTS: 7.0 and 6.5</li></ul></li><li>Three letters of support</li></ul><h5 id="no-concentration"><strong>No Concentration</strong></h5><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-19-48-23.png" alt="image_2023-09-23-19-48-23"></li></ul><h5 id="ai"><strong>AI</strong></h5><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-19-53-40.png" alt="image_2023-09-23-19-53-40"></li><li>Three letters of reference from faculty and/or employers, with preference for at least <em>one such letter from a faculty member in Artificial Intelligence (AI)</em>.</li></ul><h5 id="ds"><strong>DS</strong></h5><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-06-25.png" alt="image_2023-09-23-20-06-25"></li></ul><h5 id="ds"><strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msup><mi>S</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">DS^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.688696em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></strong></h5><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-09-52.png" alt="image_2023-09-23-20-09-52"></li></ul><h4 id="msc-cs"><a href="https://www.sgs.utoronto.ca/programs/computer-science/">MSc - CS</a></h4><ul><li><strong>Length: 4 Sessions</strong></li><li>DDL: Dec/1/2022 <code>2023-09-23: Not yet Opened</code></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-26-31.png" alt="image_2023-09-23-20-26-31"></li><li>Three letters of support</li></ul><h4 id="masc-ece"><a href="https://www.sgs.utoronto.ca/programs/electrical-and-computer-engineering/">MASc - ECE</a></h4><ul><li><strong>Length: 6 Sessions</strong></li><li>DDL: Jan/4/2024</li><li>Master of Applied Science</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-41-24.png" alt="image_2023-09-23-20-41-24"></li></ul><h4 id="meng-ece"><a href="https://www.sgs.utoronto.ca/programs/electrical-and-computer-engineering/">MEng - ECE</a></h4><ul><li><strong>Length</strong><ul><li>3 Sessions</li><li>6 Sessions (Extened)</li></ul></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-20-40-56.png" alt="image_2023-09-23-20-40-56"></li></ul><h4 id="msc-statics"><a href="https://www.sgs.utoronto.ca/programs/statistics/">MSc - Statics</a></h4><ul><li><strong>Length: 3 Sessions</strong></li><li>Statistical Theory and Application,  Probability</li><li>DDL: Jan/22/2023 <code>2023-09-23: Not yet Opened</code></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-21-52-45.png" alt="image_2023-09-23-21-52-45"></li></ul><hr><h2 id="university-of-british-columbia">University of British Columbia</h2><h3 id="rankings">Rankings</h3><ul><li>#1 (Provincial)</li><li>#34 (QS)</li><li>#35 (US)</li></ul><h3 id="location">Location</h3><ul><li>Vancouver, British Columbia</li></ul><h3 id="programs"><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs">Programs</a></h3><ul><li>Overall English Requirements<ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-30-06.png" alt="image_2023-09-23-22-30-06"></li></ul></li></ul><h4 id="msc-cs"><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs/master-of-science-computer-science">MSc - CS</a></h4><ul><li><strong>Length: 2Y</strong></li><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li><li>Master of Science in Computer Science</li></ul><h5 id="english">English</h5><ul><li>100/22/21/21/22 - TOEFL/R/W/S/L</li><li>7/6.5 - IELTS</li><li>GRE NOT required</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-38-05.png" alt="image_2023-09-23-22-38-05"></li></ul><h4 id="master-of-data-sience"><a href="https://masterdatascience.ubc.ca/programs">Master of Data Sience</a></h4><ul><li><strong>Length: 10 Months</strong></li><li>DDL: <code>Nov/3/2023</code> <strong>to</strong> <code>Jan/31/2024</code></li><li><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs/master-of-data-science-computational-linguistics">Linguistics</a></li><li><a href="https://masterdatascience.ubc.ca/programs/vancouver">Vancouver</a></li><li><a href="https://masterdatascience.ubc.ca/programs/okanagan">Okanagan</a></li><li>NOTE: (2023-09-23) Detailed Requirements are Not Yet Revealed</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-59-28.png" alt="image_2023-09-23-22-59-28"></li></ul><h4 id="meng-ece"><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs/master-of-engineering-electrical-computer-engineering">MEng - ECE</a></h4><ul><li><strong>Length: 12 ~ 16 Months:</strong><ul><li><em>The MEng program typically takes 12-16 months to complete, but students have up to 5 years to complete the program if they choose.</em></li></ul></li><li>DDL: <code>Oct/15/2023</code> <strong>to</strong> <code>Jan/15/2024</code></li><li>Master of Engineering in Electrical and Computer Engineering</li><li>Official Site: <a href="https://ece.ubc.ca/graduate/programs/master-engineering/">https://ece.ubc.ca/graduate/programs/master-engineering/</a></li><li>The GRE is not required <strong>But Welcomed</strong>.</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-24-26.png" alt="image_2023-09-23-22-24-26"></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-25-10.png" alt="image_2023-09-23-22-25-10"></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-17-46.png" alt="image_2023-09-23-22-17-46"></li></ul><h4 id="masc-ece"><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs/master-of-applied-science-electrical-computer-engineering">MASc - ECE</a></h4><ul><li><strong>Length: 2 Years</strong></li><li>DDL: <code>Oct/15/2023</code> <strong>to</strong> <code>Jan/31/2024</code></li><li>More academic than MEng</li><li>Official Site: <a href="https://ece.ubc.ca/graduate/programs/master-applied-science/">https://ece.ubc.ca/graduate/programs/master-applied-science/</a></li><li>Master of Applied Science in Electrical and Computer Engineering</li><li>The GRE is not required <strong>But Welcomed</strong>.</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-33-02.png" alt="image_2023-09-23-22-33-02"></li></ul><h4 id="msc-statistics"><a href="https://www.grad.ubc.ca/prospective-students/graduate-degree-programs/master-of-science-statistics">MSc - Statistics</a></h4><ul><li><strong>Length: 1.96 Year</strong>:<ul><li><em>Based on 41 graduations between 2017 - 2020 the minimum time to completion is 1.66 years and the maximum time is 2.99 years with an average of 1.96 years of study. All calculations exclude leave times.</em></li></ul></li><li>DDL: <code>Oct/2/2023</code> to <code>Jan/7/2024</code></li><li>Master of Science in Statistics</li><li>The GRE is not required.</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-55-52.png" alt="image_2023-09-23-22-55-52"></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-23-22-57-16.png" alt="image_2023-09-23-22-57-16"></li></ul><hr><h2 id="mcgill-university">McGill University</h2><h3 id="rankings">Rankings</h3><ul><li>#1(Provincial)</li><li>#30(QS)</li><li>#54(US)</li></ul><h3 id="location">Location</h3><ul><li>Montreal, Quebec</li></ul><h3 id="programs"><a href="https://www.mcgill.ca/gradapplicants/programs">Programs</a></h3><ul><li>Issues with Supervisors are NOT mentioned in program description.</li><li>English Requirements<ul><li>86/20 - TOEFL</li><li>6.5 for all - IELTS</li></ul></li><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li></ul><h4 id="m-sc-cs-thesis"><a href="https://www.mcgill.ca/gradapplicants/program/computer-science-msc">M.Sc. - CS | Thesis</a></h4><ul><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li><li><strong>Length: ~2Y</strong><ul><li><em>The program is designed to take 18-24 months. Students have to register as full-term <a href="http://M.Sc">M.Sc</a>. students (thesis) for three terms (typically in Fall/Winter/Fall) and then often for one additional session (Winter).</em></li></ul></li><li>Official Site: <a href="https://www.cs.mcgill.ca/graduate/masters/mscthesis/">https://www.cs.mcgill.ca/graduate/masters/mscthesis/</a></li></ul><h4 id="m-sc-cs-non-thesis"><a href="https://www.mcgill.ca/gradapplicants/program/computer-science-msc">M.Sc. - CS | NON-Thesis</a></h4><ul><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li><li>Official Site: <a href="https://www.cs.mcgill.ca/graduate/masters/mscnonthesis/">https://www.cs.mcgill.ca/graduate/masters/mscnonthesis/</a></li><li>Additional credits can be ful-filled by course-work, internship or research.</li></ul><h4 id="m-sc-ece-thesis"><a href="https://www.mcgill.ca/gradapplicants/program/electrical-engineering-msc">M.Sc. - ECE | Thesis</a></h4><ul><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li><li><strong>Length: ~2Y</strong><ul><li><em>Full-time students must complete the degree within three (3) years of initial registration. However, it is possible to complete the program in one and one-half years.</em></li></ul></li><li>software engineering, intelligent systems</li><li>Official Site: <a href="https://www.mcgill.ca/ece/graduate/programinfo/mastereng">https://www.mcgill.ca/ece/graduate/programinfo/mastereng</a></li></ul><h4 id="m-eng-ece-non-thesis"><a href="https://www.mcgill.ca/gradapplicants/program/electrical-engineering-meng-non-thesis">M.Eng. - ECE | Non-Thesis</a></h4><ul><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Dec/15/2023</code></li><li>Official Site: <a href="https://www.mcgill.ca/ece/graduate/programinfo/master-engineering-non-thesis-project-based-option-retired">https://www.mcgill.ca/ece/graduate/programinfo/master-engineering-non-thesis-project-based-option-retired</a></li><li>software engineering, intelligent systems</li></ul><h4 id="m-a-math-statistics"><a href="https://www.mcgill.ca/gradapplicants/program/mathematics-statistics-ma">M.A. - Math &amp; Statistics</a></h4><ul><li><strong>Length: <em>NOT FOUND</em></strong></li><li>DDL: <code>Sep/15/2023</code> <strong>to</strong> <code>Jan/15/2023</code></li><li>Official Site: <a href="https://www.mcgill.ca/mathstat/graduate/prospective-students/admissions">https://www.mcgill.ca/mathstat/graduate/prospective-students/admissions</a></li><li>Statistics</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-18-27-04.png" alt="image_2023-09-24-18-27-04"></li></ul><h4 id="m-m-a-analystics"><a href="https://www.mcgill.ca/gradapplicants/program/analytics-mma">M.M.A. - Analystics</a></h4><ul><li><strong>Length: 1Y</strong></li></ul><hr><h2 id="university-of-alberta">University of Alberta</h2><h3 id="rankings">Rankings</h3><ul><li>#1 (Provincial)</li><li>#111 (QS)</li><li>#136 (US)</li></ul><h3 id="location">Location</h3><ul><li>Edmonton, Alberta</li></ul><h3 id="programs"><a href="https://www.ualberta.ca/graduate-programs/index.html">Programs</a></h3><ul><li>If you are applying for a thesis-based program, please note that some programs require you to identify a potential supervisor before applying.</li></ul><h4 id="ece"><a href="https://www.ualberta.ca/graduate-programs/electrical-and-computer-engineering.html">ECE</a></h4><ul><li>DDL: <code>May/1/2024</code></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-18-45-49.png" alt="image_2023-09-24-18-45-49"></li><li>a Curriculum Vitae</li><li>Three letters of reference</li></ul><h5 id="master-of-engineering"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=47656">Master of Engineering</a></h5><ul><li><strong>Length:</strong><ul><li>Typically: <strong>one and a half years to two years</strong>.</li><li>Max: <em>six years</em>.</li></ul></li><li>course-based</li></ul><h5 id="master-of-science"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=47653">Master of Science</a></h5><ul><li><strong>Length:</strong><ul><li>Typical time of two years is normally required.</li><li>Max: four years.</li></ul></li><li>thesis-based</li></ul><h4 id="cs"><a href="https://www.ualberta.ca/graduate-programs/computing-science.html">CS</a></h4><ul><li>DDL: <code>Dec/15/2023</code></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-19-12-11.png" alt="image_2023-09-24-19-12-11"></li><li>Three letters of reference and a CV.</li><li>Applicants to a thesis-based MSc are required to select a research area and name up to three professors as potential supervisors.</li></ul><h5 id="master-of-science"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=47585">Master of Science</a></h5><ul><li><strong>Length: 2Y</strong></li><li>Thesis &amp; Course - Based Versions</li></ul><h5 id="master-of-science-in-statistical-machine-learning"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=48148">Master of Science … in Statistical Machine Learning</a></h5><ul><li><strong>Length: 2Y</strong></li><li>There is no direct admission to the MSc with a specialization in Statistical Machine Learning. Applicants wishing to pursue this program should apply to the thesis-based MSc; they may apply to transfer to the SML program after one or two terms of study provided a supervisor is found.</li></ul><h4 id="math-statistical-science"><a href="https://www.ualberta.ca/graduate-programs/mathematical-and-statistical-sciences.html">Math &amp; Statistical Science</a></h4><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-19-25-39.png" alt="image_2023-09-24-19-25-39"></li><li>DDL: NOT FOUND</li><li>Applicants are <strong>encouraged</strong> to contact academic staff before applying and identify professors who would be willing to provide supervision.</li><li>Requirements<ul><li>A Curriculum Vitae</li><li>A brief (two pages maximum) Personal Statement</li><li>Three letters of reference</li><li>Publications (up to a maximum of three) (not required)</li><li>GRE scores (General and Mathematics) (not required)</li></ul></li></ul><h5 id="master-of-science-with-a-specialization-in-statistical-machine-learning"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=48232">Master of Science with a specialization in Statistical Machine Learning</a></h5><ul><li><strong>Length: 2Y</strong></li><li>Thesis-Based</li></ul><h5 id="master-of-science-with-a-specialization-in-statistics"><a href="https://calendar.ualberta.ca/preview_program.php?catoid=39&amp;poid=48233">Master of Science with a specialization in Statistics</a></h5><ul><li><strong>Length: 2Y for Thesis-based | 1Y for Course-based</strong></li></ul><hr><h2 id="university-of-waterloo">University of Waterloo</h2><h3 id="rankings">Rankings</h3><ul><li>#2 (Provincial)</li><li>#112 (QS)</li><li>#191 (US)</li></ul><h3 id="location">Location</h3><ul><li>Waterloo, Ontari</li></ul><h3 id="programs"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/grad-programs/results">Programs</a></h3><h4 id="mmath-cs"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/computer-science-mmath-waterloo">MMath - CS</a></h4><ul><li><strong>Length: 2Y</strong></li><li>DDL: <code>Dec/1/2023</code></li><li>Thesis</li><li>Supervisors: Finding (Not necessarily required)</li><li>TOEFL 93 (writing 22, speaking 22), IELTS 6.5 (writing 6.0, speaking 6.5)</li><li>3 Reference</li></ul><h4 id="masc-ece"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/electrical-and-computer-engineering-masc-waterloo">MASc - ECE</a></h4><ul><li><strong>Length: 2Y</strong></li><li>DDL: <code>Feb/1/2024</code></li><li>A supervisor is required to receive an offer of admission</li><li>Thesis</li><li>TOEFL 80 (writing 22, speaking 20, reading 20, listening 18), IELTS 6.5 (writing 6.0, speaking 6.0)</li><li>2 Reference</li></ul><h4 id="meng-co-op-ece"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/electrical-and-computer-engineering-meng-co-op-waterloo">MEng (Co-op) - ECE</a></h4><ul><li><strong>Length: 20 Months</strong></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-19-46-50.png" alt="image_2023-09-24-19-46-50"></li><li>course-work + co-operative</li><li>DDL: <code>Feb/1/2024</code></li><li>TOEFL 80 (writing 22, speaking 20, reading 20, listening 18), IELTS 6.5 (writing 6.0, speaking 6.0)</li></ul><h4 id="mmath-data-science"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/data-science-mmath-waterloo">MMath - Data Science</a></h4><ul><li>Thesis-Based</li><li>DDL: <code>Dec/15/2023</code></li><li><strong>Length: 2Y</strong></li><li>Supervisors: Finding (Not necessarily required)</li><li>TOEFL 90 (writing 25, speaking 25), IELTS 7.0 (writing 6.5, speaking 6.5)</li><li>3 references</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-19-50-07.png" alt="image_2023-09-24-19-50-07"></li></ul><h4 id="mdsai-co-op-ds-ai-co-op"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/data-science-and-artificial-intelligence-mdsai-co-op">MDSAI (Co-op) - DS + AI + Co-op</a></h4><ul><li><strong>Length: 16 Months</strong></li><li>Course work</li><li>??? TOEFL 100 (writing 26, speaking 26), IELTS 7.5 (writing 7.0, speaking 7.0)</li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-20-01-01.png" alt="image_2023-09-24-20-01-01"></li></ul><h4 id="mmath-statics"><a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/future-students/programs/statistics-mmath-waterloo">MMath - Statics</a></h4><ul><li>DDL: <code>Jan/15/2024</code></li><li><strong>Length: 24 Months</strong></li><li>Supervisors: Required (But Not necessarily required prior to the application)</li><li>Thesis</li><li>TOEFL 90 (writing 25, speaking 25), IELTS 7.0 (writing 6.5, speaking 6.5)</li></ul><hr><h2 id="western-university">Western University</h2><h3 id="rankings">Rankings</h3><ul><li>#3 (Provincial)</li><li>#114 (QS)</li><li>#300 (US)</li></ul><h3 id="location">Location</h3><ul><li>London, Ontari</li></ul><h3 id="programs"><a href="https://grad.uwo.ca/admissions/programs/index.cfm">Programs</a></h3><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-20-42-26.png" alt="image_2023-09-24-20-42-26"></li></ul><h4 id="msc-cs"><a href="https://grad.uwo.ca/admissions/programs/program.cfm?p=37">MSc - CS</a></h4><ul><li><strong>Length: 4 Terms</strong></li><li>Course-based, project-based or thesis-based</li><li>TOEFL: 92/20, IELTS: 6.5/6</li><li>DDL: <code>Feb/15/2024</code> (Mid-April Shall receive the Acceptance Notification)</li></ul><h4 id="mesc-ece"><a href="https://grad.uwo.ca/admissions/programs/program.cfm?p=39">MESc - ECE</a></h4><ul><li><strong>Length: 6 Terms</strong></li><li>Thesis-based</li><li>TOEFL: 86/20, IELTS: 6.5/6</li><li>DDL: <code>Jul/31/2024</code></li></ul><h4 id="msc-statistics"><a href="https://grad.uwo.ca/admissions/programs/program.cfm?p=140">MSc - Statistics</a></h4><ul><li>**Length: **<ul><li>3 Terms (project-based)</li><li>6 Terms (Thesis-based)</li></ul></li><li>TOEFL: 86/20; IELTS: 6.5/6</li><li>DDL: <code>Feb/15/2024</code></li></ul><h4 id="mda-data-science"><a href="https://grad.uwo.ca/admissions/programs/program.cfm?p=272">MDA - Data Science</a></h4><ul><li><strong>Length: 3 Terms</strong></li><li>IELTS: 7.0/6.5</li><li>Course-based</li></ul><hr><h2 id="université-de-montréal">Université de Montréal</h2><h3 id="rankings">Rankings</h3><ul><li>#2 (Provincial)</li><li>#141 (QS)</li><li>#156 (US)</li></ul><h3 id="location">Location</h3><ul><li>Montreal, Quebec</li></ul><h3 id="programs"><a href="https://admission.umontreal.ca/en/programs-of-study">Programs</a></h3><ul><li>Most Programs are <em>French Only</em></li></ul><hr><h2 id="university-of-calgary">University of Calgary</h2><h3 id="rankings">Rankings</h3><ul><li>#2 (Provincial)</li><li>#182 (QS)</li><li>#175 (US)</li></ul><h3 id="location">Location</h3><ul><li>Calgary, Alberta</li></ul><h3 id="programs"><a href="https://grad.ucalgary.ca/future-students/explore-programs">Programs</a></h3><ul><li>Supervisors: For Thesis-based only.<ul><li><em>A supervisor is required, but is not required prior to the start of the program</em></li></ul></li></ul><h4 id="meng-ece"><a href="https://grad.ucalgary.ca/future-students/explore-programs/electrical-and-computer-engineering-meng-course">MEng - ECE</a></h4><ul><li><strong>Length: 2Y</strong><ul><li><em>The program can often be completed in one to two years of full-time study</em></li></ul></li><li>course-based</li><li>TOEFL: 86/20; IELTS: 6.5/6.0</li><li>DDL: <code>Mar/1/2024</code><ul><li><em>We encourage you to apply early as this program receives a high volume of applications and reaches capacity quickly. We send offers to qualified applicants on a rolling basis.</em></li></ul></li></ul><h4 id="meng-ece-thesis"><a href="https://grad.ucalgary.ca/future-students/explore-programs/electrical-and-computer-engineering-meng-thesis">MEng - ECE - Thesis</a></h4><ul><li><strong>Length: 2Y</strong></li><li>Thesis-based</li><li>TOEFL: 86/20; IELTS: 6.5/6.0</li><li>DDL: 2023-09-25: <code>NOT Accepting Applications</code></li></ul><h4 id="msc-ece"><a href="https://grad.ucalgary.ca/future-students/explore-programs/electrical-and-computer-engineering-msc-thesis">MSc - ECE</a></h4><ul><li><strong>Length: 20Months</strong></li><li>Thesis-based</li><li>TOEFL: 86/20; IELTS: 6.5/6</li><li>DDL: <code>Jan/31/2024</code></li></ul><h4 id="msc-cs"><a href="https://grad.ucalgary.ca/future-students/explore-programs/computer-science-msc-thesis">MSc - CS</a></h4><ul><li><strong>Length: 2Y</strong></li><li>Thesis-based</li><li>GRE: Expected<ul><li><em>Special consideration will be given to those with GRE scores of at least 600 verbal, 750 quantitative, and 720 analytical (5.5 in the new format). Applicants from outside Canada are expected to apply with GRE scores.</em></li></ul></li><li>TOEFL: 97; IELTS: 7.0/6.5</li><li>DDL: <code>Jan/15/2024</code> (<code>Estimated</code>)</li></ul><h4 id="mdsa-ds-analystics"><a href="https://grad.ucalgary.ca/future-students/explore-programs/data-science-and-analytics-mdsa-course">MDSA - DS + Analystics</a></h4><ul><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-25-11-02-24.png" alt="image_2023-09-25-11-02-24"></li><li>course-based</li><li>Length: 1Y</li><li>IELTS: 6.5/6.0</li><li>DDL:<ul><li>For admission September 1:<ul><li>Canadian and Permanent Residents: July 3</li></ul></li><li>For admission January 1:<ul><li>Canadian and Permanent Residents: October 3</li><li>International: September 1</li></ul></li></ul></li></ul><h4 id="msc-statistics"><a href="https://grad.ucalgary.ca/future-students/explore-programs/math-and-statistics-msc-thesis">MSc - Statistics</a></h4><ul><li>Thesis-based</li><li>Length: 2Year</li><li>IELTS: 7.0/6.5</li><li>DDL: <code>Jan/15/2024</code></li><li>3 References</li></ul><h4 id="msc-statistics"><a href="https://grad.ucalgary.ca/future-students/explore-programs/math-and-statistics-msc-course">MSc - Statistics</a></h4><ul><li>course-based</li><li>Length: 1~2 Year</li><li>IELTS: 7.0/6.5</li><li>DDL: <code>Jan/15/2024</code></li><li>3 References</li></ul><hr><h2 id="mcmaster-university">McMaster University</h2><h3 id="rankings">Rankings</h3><ul><li>#4 (Provincial)</li><li>#189(QS)</li><li>#138 (US)</li></ul><h3 id="location">Location</h3><ul><li>Hamilton, Ontari</li></ul><h3 id="programs"><a href="https://gs.mcmaster.ca/programs/">Programs</a></h3><p>…</p><hr><h2 id="university-of-ottawa">University of Ottawa</h2><h3 id="rankings">Rankings</h3><ul><li>#5 (Provincial)</li><li>#203 (QS)</li><li>#215 (US)</li></ul><h3 id="location">Location</h3><ul><li>Ottawa, Ontari</li></ul><h3 id="programs"><a href="https://catalogue.uottawa.ca/en/programs/">Programs</a></h3><p>…</p><hr><h2 id="queen-s-university-at-kingston">Queen’s University at Kingston</h2><h3 id="rankings">Rankings</h3><ul><li>#6 (Provincial)</li><li>#209 (QS)</li><li>#429 (US)</li></ul><h3 id="location">Location</h3><ul><li>Kingston, Ontari</li></ul><h3 id="programs"><a href="https://www.queensu.ca/grad-postdoc/grad-studies/programs-degrees">Programs</a></h3><hr><h2 id="dalhousie-university">Dalhousie University</h2><h3 id="rankings">Rankings</h3><ul><li>#1 (Provincial)</li><li>#298 (QS)</li><li>#314 (US)</li></ul><h3 id="location">Location</h3><ul><li>Halifax, Nova Scotia</li></ul><h3 id="programs"><a href="https://www.dal.ca/academics/graduate_programs.html">Programs</a></h3><hr><h2 id="simon-fraser-university">Simon Fraser University</h2><h3 id="rankings">Rankings</h3><ul><li>#2 (Provincial)</li><li>#318 (QS)</li><li>#317 (US)</li></ul><h3 id="location">Location</h3><ul><li>Burnaby, British Columbia</li></ul><h3 id="programs"><a href="https://www.sfu.ca/gradstudies/apply/programs/alphabetic.html">Programs</a></h3><hr><h2 id="university-of-victoria">University of Victoria</h2><h3 id="rankings">Rankings</h3><ul><li>#3 (Provincial)</li><li>#322 (QS)</li><li>#324 (US)</li></ul><h3 id="location">Location</h3><ul><li>Victoria, British Columbia</li></ul><h3 id="programs"><a href="https://www.uvic.ca/graduate/programs/graduate-programs/index.php">Programs</a></h3><hr><h2 id="national-university-of-singapore">National University of Singapore</h2><h2 id="nanyang-technological-university-singapore">Nanyang Technological University, Singapore</h2><hr><h2 id="the-university-of-hong-kong">?The University of Hong Kong</h2><h2 id="the-chinese-university-of-hong-kong">?The Chinese University of Hong Kong</h2><h2 id="the-hong-kong-university-of-science-and-technology">?The Hong Kong University of Science and Technology</h2><hr><h1>Immigration Policies</h1><h2 id="canada">Canada</h2><ul><li>Federal Government - EE</li><li>Provincial Nomination</li></ul><h3 id="british-columbia">British Columbia</h3><ul><li><a href="https://www.welcomebc.ca/Study-in-B-C/Stay-in-B-C-After-Studying">https://www.welcomebc.ca/Study-in-B-C/Stay-in-B-C-After-Studying</a></li></ul><h3 id="ontari">Ontari</h3><ul><li><a href="https://www.ontario.ca/page/ontario-immigrant-nominee-program-oinp">https://www.ontario.ca/page/ontario-immigrant-nominee-program-oinp</a></li></ul><h3 id="quebec">Quebec</h3><ul><li><a href="https://www.quebec.ca/en/education/study-quebec/staying-after-studies">https://www.quebec.ca/en/education/study-quebec/staying-after-studies</a></li></ul><h3 id="nova-scotia">Nova Scotia</h3><ul><li><a href="https://novascotiaimmigration.com/study-here/after-you-graduate/">https://novascotiaimmigration.com/study-here/after-you-graduate/</a></li></ul><h3 id="alberta">Alberta</h3><ul><li><a href="https://study.alberta.ca/after-graduation/immigrate-to-canada/">https://study.alberta.ca/after-graduation/immigrate-to-canada/</a></li><li><img src="https://raw.githubusercontent.com/ChrisVicky/image-bed/main/2023-09/image_2023-09-24-21-54-55.png" alt="image_2023-09-24-21-54-55"></li></ul><h2 id="singapore">Singapore</h2><ul><li>…</li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>数据结构期末复习</title>
      <link href="/2023/06/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
      <url>/2023/06/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="100%"        src="/img/image_2023-06-11-19-44-33.png">    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">MindMap</div></center><h1>2023 年复习版</h1><ul><li>[ ] 2023-06-16 考试复习版</li></ul><h2 id="复习重点">复习重点</h2><h3 id="绪论"><a href="/notes/%E7%BB%AA%E8%AE%BA">绪论</a></h3><ul><li>[x] <a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD">基本概念和术语</a></li><li>[x] <a href="#%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%AE%A1%E7%AE%97">时间复杂度计算</a></li></ul><h3 id="线性表"><a href="/notes/%E7%BA%BF%E6%80%A7%E8%A1%A8">线性表</a></h3><ul><li>[x] <a href="#%E7%BA%BF%E6%80%A7%E8%A1%A8">线性表</a>、<a href="#%E9%A1%BA%E5%BA%8F%E8%A1%A8">顺序表</a>、<a href="#%E5%8D%95%E9%93%BE%E8%A1%A8">单链表</a>的基本操作和算法</li><li>[x] <a href="#%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8">循环链表</a>、<a href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E9%93%BE%E8%A1%A8%E6%93%8D%E4%BD%9C">双向循环链表操作</a></li></ul><h3 id="栈和队列"><a href="/notes/%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97">栈和队列</a></h3><ul><li>[x] 栈的定义、特点、基本操作和算法</li><li>[x] 队列的定义、特点、基本操作和算法</li><li>[x] 递归</li></ul><h3 id="数组和广义表"><a href="/notes/%E6%95%B0%E7%BB%84%E5%92%8C%E5%B9%BF%E4%B9%89%E8%A1%A8">数组和广义表</a></h3><ul><li>[x] 上三角矩阵、下三角矩阵、稀疏矩阵存储与表示</li><li>[x] 稀疏存储与表示（矩阵快速转置）</li></ul><h3 id="树和二叉树"><a href="/notes/%E6%A0%91%E5%92%8C%E4%BA%8C%E5%8F%89%E6%A0%91">树和二叉树</a></h3><ul><li>[x] 二叉树定义、性质</li><li>[x] 二叉树构造</li><li>[x] 二叉树遍历和算法（递归/非递归）</li><li>[x] 二叉树应用: 技术</li><li>[x] 树/森林 和 二叉树转换</li><li>[x] 哈夫曼树的构建<ul><li>[x] 哈夫曼编码，求加权路径长度</li></ul></li></ul><h3 id="图"><a href="/notes/%E5%9B%BE">图</a></h3><ul><li>[x] 图的定义、存储结构 !!</li><li>[x] 图的遍历（广搜，深搜）</li><li>[x] Prim 构建最小生成树</li><li>[x] 拓扑排序实现过程 !!</li><li>[x] 关键路径实现 !!<ul><li>[x] 给出各事件最早、最晚开始时间，路径上各活动最早、最晚开始时间</li></ul></li><li>[x] 最短路径实现过程</li></ul><h3 id="查找"><a href="/notes/%E6%9F%A5%E6%89%BE">查找</a></h3><ul><li>[x] 顺序表查找、折半查找</li><li>[x] 二叉排序树查找、插入、删除过程</li><li>[ ] 构造平衡二叉树</li><li>[ ] B- 树构建、实现 !!</li><li>[x] 基于哈希函数构建哈希表（散列表） !!<ul><li>[x] 除留余数法解决冲突</li><li>[x] 线性深测处理</li><li>[x] 二次深测处理</li></ul></li></ul><h3 id="内排序"><a href="/notes/%E6%8E%92%E5%BA%8F">内排序</a></h3><ul><li>[x] 直接插入、折半插入、希尔排序</li><li>[ ] 冒泡、快速排序</li><li>[ ] 直接选择、堆排序</li><li>[x] 归并排序</li></ul>]]></content>
      
      
      <categories>
          
          <category> Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 期末复习 </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 基本指令记录</title>
      <link href="/2023/02/16/Git-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/"/>
      <url>/2023/02/16/Git-%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1>Git 基本指令</h1><hr><h2 id="1-将远程仓库弄到本地">1. 将远程仓库弄到本地</h2><h3 id="1-0-将远程仓库克隆到本地-git-clone-xxx">1.0. 将远程仓库克隆到本地 <code>git clone xxx</code></h3><blockquote><p>当本地没有该仓库时</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone xxxx</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">xxxx: 仓库地址（可以是 `https://xxx.git`, 也可以是 ssh 连接 `git@githubxxx`）</span></span><br><span class="line">e.g.:  git clone git@github.com:ChrisVicky/TJU-2022-Socket-Computer-Network-Lab.git</span><br></pre></td></tr></table></figure><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-16-22-56-41.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">example</div></center><ul><li>会在本地生成一个目录，包含这个仓库</li></ul><h3 id="1-1-将远程同步到本地仓库-git-pull">1.1. 将远程同步到本地仓库 <code>git pull</code></h3><blockquote><ul><li>当本地有该仓库时</li><li>每次开始对该仓库内文件修改之前都需要进行</li></ul></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><h2 id="2-做一些修改">2. 做一些修改</h2><h2 id="3-查看仓库状态-git-status">3. 查看仓库状态 <code>git status</code></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">❯ git status </span><br><span class="line">On branch main ==&gt; 在主分支（*分支）</span><br><span class="line">Your branch is up to date with &#x27;origin/main&#x27;. ==&gt; 与远程的 `origin/main` 分支的情况 （*远程分支）</span><br><span class="line"></span><br><span class="line">Untracked files:  ==&gt; 哪些文件没有被 跟踪 （*track）</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">        something</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br><span class="line">  ==&gt; 总结当前的状况</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">❯ git status</span><br><span class="line">On branch main</span><br><span class="line">Your branch is up to date with &#x27;origin/main&#x27;.</span><br><span class="line"></span><br><span class="line">Changes to be committed:  ==&gt; 当前有没有被“提交的” 内容 （*commit）</span><br><span class="line">  (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage)</span><br><span class="line">        new file:   something</span><br></pre></td></tr></table></figure><blockquote><p>如何建立一个“版本”， git 是一个版本控制软件。一个版本就是一个 commit, 一个 commit 中需要 track 多个 changes, 每个 changes 和一个文件绑定</p></blockquote><pre><code>  ┌───────────┐     git add   ┌────────────┐    git commit  ┌───────────┐  │file change│──────────────►│file tracked│───────────────►│file commit│  └───────────┘               └────────────┘                └───────────┘                                                              新版本诞生</code></pre><h2 id="4-添加到仓库-git-add-file">4. 添加到仓库 <code>git add &lt;file&gt;</code></h2><h2 id="5-提交到仓库-git-commit">5. 提交到仓库 <code>git commit</code></h2><ul><li>需要对提交进行描述</li></ul><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-16-23-08-13.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">git commit</div></center><h2 id="6-提交到远程分支-git-push">6. 提交到远程分支 <code>git push</code></h2>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TJU-快速评教</title>
      <link href="/2023/02/15/TJU-%E5%BF%AB%E9%80%9F%E8%AF%84%E6%95%99%E8%84%9A%E6%9C%AC/"/>
      <url>/2023/02/15/TJU-%E5%BF%AB%E9%80%9F%E8%AF%84%E6%95%99%E8%84%9A%E6%9C%AC/</url>
      
        <content type="html"><![CDATA[<ul><li><a href="https://github.com/ChrisVicky/TJU-2023-Evaluator">GitHub Repo</a></li><li>本程序只是为了玩一玩 <a href="https://playwright.dev/"><code>PlayWright</code></a>，并不是为了提高评教速度。</li></ul><h2 id="使用说明">使用说明</h2><h3 id="1-校外使用-连接-tju-的-vpn">1.  <em>校外使用</em> 连接 TJU 的 VPN</h3><blockquote><p>校内登录无需此步骤</p></blockquote><ol><li>打开 <a href="https://www.sangfor.com.cn/"><code>easyconnect</code></a></li></ol><blockquote><p>Arch Linux 用户可以在 <code>aur</code> 上找到。</p></blockquote><ol start="2"><li>输入天津大学 VPN 地址 <code>https://vpn.tju.edu.cn</code>并点击按钮。</li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-13-21-08-36.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">输入天津大学VPN</div></center><ol start="3"><li>输入 VPN 帐号密码。</li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-13-21-08-48.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">输入帐号密码</div></center><ol start="4"><li>登录完成。</li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"      src="/img/image_2023-02-13-21-09-56.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">登录完成</div></center><h3 id="2-安装环境">2. 安装环境</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n tju</span><br><span class="line">conda activate tju</span><br><span class="line">conda install pip </span><br><span class="line">pip install -r requirement.txt</span><br><span class="line">playwright install chromium</span><br></pre></td></tr></table></figure><h3 id="3-查看并填写配置文件">3. 查看并填写配置文件</h3><ul><li>创建 <code>cfg</code> 文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp account.cfg.example account.cfg</span><br></pre></td></tr></table></figure><ul><li>修改之</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[account]</span><br><span class="line"># USERNAME: 学号</span><br><span class="line">USERNAME = xxxxxxxx</span><br><span class="line"># PASSWORD: 密码</span><br><span class="line">PASSWORD = xxxxxxxx</span><br><span class="line"></span><br><span class="line"># 是否启用 Headless Mode</span><br><span class="line"># True: 启用 Headless Mode --&gt; 不显示 UI</span><br><span class="line"># False: 不启用 Headless Mode  --&gt; 显示 UI</span><br><span class="line">HEADLESS = False</span><br></pre></td></tr></table></figure><h3 id="4-运行代码">4. 运行代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py</span><br></pre></td></tr></table></figure><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-13-21-26-22.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">执行效果</div></center><h2 id="关于-playwright">关于 <code>playwright</code></h2><ul><li><code>PlayWright</code> 有录制功能，能够快速生成脚本，提升开发速度。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m playwright codegen</span><br></pre></td></tr></table></figure><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-15-12-47-47.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Playwright Codegen</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TJU </tag>
            
            <tag> 评教 </tag>
            
            <tag> playwright </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Review: OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
      <link href="/2023/02/07/OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/"/>
      <url>/2023/02/07/OpenPose-Realtime-Multi-Person-2D-Pose-Estimation-using-Part-Affinity-Fields/</url>
      
        <content type="html"><![CDATA[<center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-07-13-07-25.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Title</div></center><ul><li>Cite: Cao Z, Simon T, Wei S E, et al. Realtime multi-person 2d pose estimation using part affinity fields[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7291-7299.</li><li>Available Online: <a href="https://arxiv.org/pdf/1812.08008.pdf">https://arxiv.org/pdf/1812.08008.pdf</a></li><li>Official Implementation: <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></li><li>Demo based on OpenCV Implementation: <a href="https://github.com/ChrisVicky/OpenCV-OpenPose-Demo">https://github.com/ChrisVicky/OpenCV-OpenPose-Demo</a></li><li>Date: 2023-02-07</li></ul><h2 id="brief-summary">Brief summary</h2><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-07-12-56-13.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Main Architecture</div></center><ul><li><p>What is the problem the paper is trying to solve?</p><ul><li>The Speed-up of Multi-Person Pose Estimation Process, the foundation of many behavior-recognizers.</li></ul></li><li><p>What are the key ideas of the paper? Key insights?</p><ul><li>Process all people at once.</li><li>Part Affinity Fields (PAF).</li></ul></li></ul><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-07-12-33-02.png"><br>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-02-07-12-34-55.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">PAF Definition</div></center><ul><li><p>What is the key contribution to the literature at the time it was written?</p><ul><li>The fastest and most accurate model that recognizes pose information is proposed and implemented by the authors.</li><li>A New Method that utilizing PAF is presented.</li></ul></li><li><p>What are the most important things you take out from it?</p><ul><li>The idea that using PAF to pair up feature points that belong to one person.</li><li>The Bottom-Top methodology could reduce redundancy produced by single person recognition in Top-Bottom Methods.</li></ul></li></ul><h2 id="strengths-most-important-ones">Strengths (Most Important Ones)</h2><ul><li>Does the paper solve the problem well?<ul><li>I suppose yes. The paper presents a higher accuracy compared to other models at that time. And According to my own implementation, the paper wins as well.</li></ul></li></ul><h2 id="weaknesses-most-important-ones">Weaknesses (Most Important Ones)</h2><ul><li>Room for improvement.<ul><li>The pair-up process currently is implemented according to an algorithm after the neural network which, in my opinion, could be merged in the network structure.</li></ul></li></ul><h2 id="how-can-we-do-better-your-ideas-and-thoughts">How can we do better? Your ideas and thoughts.</h2><ul><li>The method relies on a VGG Model that recognizes feature points which can be separated from the main part. Therefore, during video processing where there is a sequence of image data, a pipeline processing structure could be introduced to further improve the efficiency.</li><li>Also, since <em>VIT</em>, we may replace the Convolutional Neural Network with <strong>Transformer</strong>.</li></ul><h2 id="what-have-you-learnt-enjoyed-disliked-in-the-paper-why">What have you learnt/enjoyed/disliked in the paper? Why?</h2><ul><li>The bottom-up methodology is the most enjoyed idea. However, the feature extractor on the top still presents a Top-Bottom methodology that, in one hand, does reduce the difficulty of development, but on the other hand betrays the Bottom-Top methodology since the model is topped by a feature extractor.</li></ul>]]></content>
      
      
      <categories>
          
          <category> PaperReview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> OpenPose </tag>
            
            <tag> PaperReview </tag>
            
            <tag> Pose Estimation </tag>
            
            <tag> PAF </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paper Review: An Image is Worth 16x16 Words</title>
      <link href="/2023/01/16/An-Image-is-Worth-16x16-Words/"/>
      <url>/2023/01/16/An-Image-is-Worth-16x16-Words/</url>
      
        <content type="html"><![CDATA[<center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2023-01-15-22-28-11.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Title</div></center><ul><li>Cite: Dosovitskiy, Alexey, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.” arXiv preprint arXiv:2010.11929 (2020).</li><li>Available online: <a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></li><li>Date: 2023-01-15 22:07</li></ul><h2 id="brief-summary">Brief summary</h2><p><img src="img/image_2023-01-15-22-17-58.png" alt="image_2023-01-15-22-17-58"></p><ul><li><p>What is the problem the paper is trying to solve?</p><ul><li>How can we reduce CNNs in transformer models in image classification tasks?</li><li>How to treat an Image as a sequence of data?</li></ul></li><li><p>What are the key ideas of the paper? Key insights?</p><ul><li>A new way to treat an image sequentially.</li><li>An image is divided into 16x16 2D patches and are transferred sequentially into an out-of-box self-attention transformer encoder, followed by MLP Heads which result in classification predictions.</li></ul></li><li><p>What is the key contribution to the literature at the time it was written?</p><ul><li>First to reduce entirely the CNNs in usage of transformer in image classification tasks and propose a pure transformer model called Vision Transformer (ViT).</li><li>Maybe the first to propose a sequential treatment on an image.</li></ul></li><li><p>What are the most important things you take out from it?</p><ul><li>A possible way to treat an image as a sequence of data while preserving some relative information.</li><li>Designing easy-to-use models that utilize out-of-box interfaces could be a good way of conducting creative methods that solve difficult problems.</li></ul></li></ul><h2 id="strengths-most-important-ones">Strengths (Most Important Ones)</h2><ul><li>Does the paper solve the problem well?<ol><li>First, it is true that the solution, or more precisely, the new model proposed can be a possible solution that treat an image sequentially.</li><li>Second, the computational latency can be low in this case since the division is as small as 16x16, and therefore the computational complexity can be relatively at a low level.</li></ol></li></ul><h2 id="weaknesses-most-important-ones">Weaknesses (Most Important Ones)</h2><ul><li>Room for improvement.<ol><li>The way to treat the image as a sequence of data is too straight-forward. The information on the boarders between patches can be lost.</li><li>The patches are produced with a fixed number of sequences, making the model difficult to perform on images with height resolution, and since nowadays, people have a preference of taking high resolution pictures, the methods proposed may not be practical in real-life without further improvements.</li><li>Also, the model requires a large quantity of data for training, significantly improving the training costs.</li></ol></li></ul><h2 id="how-can-we-do-better-your-ideas-and-thoughts">How can we do better? Your ideas and thoughts.</h2><ol><li>To include more relative information between patches, I guess one way is to introduce an additional division that stands on the boarder of the original one. A possible implementation is shown below.</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">┌───┬───┬───┐   ┌─┬───────┬─┐    ┌─┬─┬───┬─┬─┐</span><br><span class="line">│   │   │   │   ├─┼───────┼─┤    ├─┼─┼───┼─┼─┤</span><br><span class="line">├───┼───┼───┤   │ │       │ │    ├─┼─┼───┼─┼─┤</span><br><span class="line">│   │   │   │ + │ │       │ │ =&gt; │ │ │   │ │ │</span><br><span class="line">├───┼───┼───┤   │ │       │ │    ├─┼─┼───┼─┼─┤</span><br><span class="line">│   │   │   │   ├─┼───────┼─┤    ├─┼─┼───┼─┼─┤</span><br><span class="line">└───┴───┴───┘   └─┴───────┴─┘    └─┴─┴───┴─┴─┘</span><br><span class="line">      A               B                C</span><br></pre></td></tr></table></figure><blockquote><p>Where <code>A</code> is the method in the paper and <code>B</code> is the additional sequence to provide extra information. But again, this additional patch can further worsen the situation of high overall computational costs.</p></blockquote><ol start="2"><li>The high computational costs in high resolution image processing may be decreased by parallel hardware devices.</li></ol><h2 id="what-have-you-learnt-enjoyed-disliked-in-the-paper-why">What have you learnt/enjoyed/disliked in the paper? Why?</h2><ul><li>The most impressive thing I take away from this paper is the methodology that the model is designed to utilize the API of original Transformer model which has been implemented and optimized for efficient performance. This way of design is simple, yet creative and meaningful.</li></ul><!-- !! No more than Half a pape -->]]></content>
      
      
      <categories>
          
          <category> PaperReview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> Tranformer </tag>
            
            <tag> ViT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KinectV2 Camera Calibration and `Yolov5` Recognition</title>
      <link href="/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/"/>
      <url>/2022/12/30/KinectV2-Camera-Calibration-and-YoloV5-Recognition/</url>
      
        <content type="html"><![CDATA[<ul><li>Project Repo: <a href="https://github.com/ChrisVicky/KinectV2-calibration-and-Yolov5-recognition">KinectV2 Camera Calibration and <code>Yolov5</code> Recognition</a></li><li>2022-12-30 19:11</li><li>This is a subproject from <a href="https://github.com/ChrisVicky/camera-position-solution">camera-position-solution</a>.</li></ul><h2 id="background">Background</h2><p>We are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects);</p><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2023-01-04-00-07-18.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">KinectV2</div></center><p>In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates.</p><span id="more"></span><p><img src="img/image_2022-12-30-19-10-06.jpg" alt="KinectV2"></p><h2 id="project-structure">Project Structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build                         -- build Dir</span><br><span class="line">├── CMakeLists.txt                -- Top Cmake Configuration</span><br><span class="line">├── data                          </span><br><span class="line">│   └── output.mp4                -- Output Data -&gt; stacks of imshown frames</span><br><span class="line">├── default.xml                   -- Default Configuration File (example)</span><br><span class="line">├── include </span><br><span class="line">│   ├── calibration.hpp           -- Calibration -&gt; Future change: With Robot</span><br><span class="line">│   ├── define.hpp                -- Define COLORS etc</span><br><span class="line">│   ├── dnn.hpp                   -- Use OpenCV DNN APIs</span><br><span class="line">│   ├── main.hpp                  -- Main Program</span><br><span class="line">│   └── settings.hpp              -- Read Settings</span><br><span class="line">├── logsrc                        -- Log Helper by loguru</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── loguru.cpp</span><br><span class="line">│   └── loguru.hpp</span><br><span class="line">├── models                        -- Trained Yolov5 Modules</span><br><span class="line">│   ├── yolov5n.onnx</span><br><span class="line">│   ├── yolov5s.onnx</span><br><span class="line">│   ├── yolov5.xml</span><br><span class="line">│   └── yuki-bubu-2022-12-23.onnx </span><br><span class="line">├── README.md                     </span><br><span class="line">└── src                           </span><br><span class="line">    ├── calibration.cpp</span><br><span class="line">    ├── dnn.cpp</span><br><span class="line">    └── main.cpp</span><br><span class="line">21 directories, 87 files</span><br></pre></td></tr></table></figure><h2 id="dependency">Dependency</h2><p>We use two libraries: <a href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> and <a href="https://github.com/opencv/opencv">OpenCV</a>.</p><blockquote><p>Note that: You should set <code>freenect2_DIR</code> and include <code>freenect2_INCLUDE</code> directions if you install <code>libfreenect2</code> in custom directories.</p></blockquote>  <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up libfreenect2</span></span><br><span class="line"><span class="comment"># Set include dir and DIR for libfreenect2 if it is not installed globally</span></span><br><span class="line"><span class="comment"># SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)</span></span><br><span class="line"><span class="comment"># include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)</span></span><br><span class="line"><span class="keyword">find_package</span>(freenect2 REQUIRED)</span><br></pre></td></tr></table></figure><h2 id="program-usage">Program Usage</h2><ol><li>Install Dependencies shown above</li><li>Run the following commands to build the project</li></ol>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure><ol start="3"><li><p>Plug in KinectV2 via USB (you might need a hub)<br><img src="img/image_2022-12-30-19-11-30.png" alt="Hub"></p></li><li><p>run <code>./calibration</code> to start program<br><img src="img/image_2022-12-30-21-19-15.png" alt="Screenshot"></p></li></ol><h2 id="developer-diary">Developer Diary</h2><p>To meet the need, we have to conquer four difficulties.</p><ol><li>KinectV2 Connection</li><li>OpenCV Calibration</li><li>Object Detection</li><li>Planes Transformation</li></ol><h3 id="1-kinectv2-connection">1. KinectV2 Connection</h3><p>Since we develop the program on multiple Operating Systems (OS), we decide to take advantage of <a href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> which is open-sourced and supports Linux, Windows and Mac-OS.</p><blockquote><p>To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine.</p></blockquote><p>To use KinectV2, we need the following steps:</p><h4 id="1-1-define-basic-variables-either-globally-or-locally">1.1. Define Basic Variables, either globally or locally.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">libfreenect2::Freenect2 freenect2;      <span class="comment">// libfreenect2 entity</span></span><br><span class="line">libfreenect2::PacketPipeline *pipeline; <span class="comment">// libfreenect2 pipeline</span></span><br><span class="line">libfreenect2::Freenect2Device *device;  <span class="comment">// device</span></span><br><span class="line"><span class="function">libfreenect2::SyncMultiFrameListener <span class="title">listener</span><span class="params">(libfreenect2::Frame::Color)</span></span>;</span><br><span class="line">libfreenect2::FrameMap frames;</span><br></pre></td></tr></table></figure><h4 id="1-2-initialize-the-device-via-certain-apis">1.2. Initialize the <code>device</code> via certain APIs</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* -------------------- START Kinectv2 Initialization -------------------- */</span></span><br><span class="line"><span class="keyword">if</span>(freenect2.<span class="built_in">enumerateDevices</span>() == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;no device connected!&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device connected&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">string serial = freenect2.<span class="built_in">getDefaultDeviceSerialNumber</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;SEARIAL Number: %s&quot;</span>,serial.<span class="built_in">c_str</span>());</span><br><span class="line">pipeline = <span class="keyword">new</span> libfreenect2::<span class="built_in">CpuPacketPipeline</span>();</span><br><span class="line">device = freenect2.<span class="built_in">openDevice</span>(serial, pipeline); <span class="keyword">if</span>(device == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;failed to open device: %s&quot;</span>, serial.<span class="built_in">c_str</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device opened&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">kinect_shutdown = <span class="literal">false</span>;</span><br><span class="line">device-&gt;<span class="built_in">setColorFrameListener</span>(&amp;listener);</span><br><span class="line">device-&gt;<span class="built_in">start</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device serial: %s&quot;</span> ,device-&gt;<span class="built_in">getSerialNumber</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device firmware: %s&quot;</span> ,device-&gt;<span class="built_in">getFirmwareVersion</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">/* -------------------- END Kinectv2 Initialization -------------------- */</span></span><br></pre></td></tr></table></figure><h4 id="1-3-we-shall-start-a-loop-to-receive-frames-from-the-device">1.3. We shall start a Loop to receive frames from the Device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!kinect_shutdown)&#123;</span><br><span class="line">  <span class="keyword">if</span>(!listener.<span class="built_in">waitForNewFrame</span>(frames, timeout))</span><br><span class="line">    <span class="built_in">LOG_F</span>(WARNING, <span class="string">&quot;Frame Received Failed after timeout: %d&quot;</span>, timeout);</span><br><span class="line">  libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color];</span><br><span class="line">  <span class="comment">/* -------------------- START Frame Processing -------------------- */</span></span><br><span class="line">  <span class="comment">/* -------------------- END Frame Processing -------------------- */</span></span><br><span class="line">  listener.<span class="built_in">release</span>(frames);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="1-4-before-exit-we-need-to-manually-stop-and-close-the-device">1.4. Before Exit, we need to manually stop and close the device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">device-&gt;<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure><blockquote><p>We must define a <code>sigint_handler</code> to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown.</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="type">int</span> s)</span></span>&#123;</span><br><span class="line">  device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">  device-&gt;<span class="built_in">close</span>();</span><br><span class="line">  <span class="built_in">exit</span>(s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Usage </span></span><br><span class="line"><span class="built_in">signal</span>(SIGINT, sigint_handler); <span class="comment">// Savely Close the Device before sudden exit</span></span><br></pre></td></tr></table></figure><h4 id="1-5-to-take-advantage-of-opencv-apis-we-convert-libfreenect2-frame-to-cv-mat-right-at-the-beginning-of-frame-processing">1.5. To take advantage of OpenCV APIs, we convert <code>libfreenect2::Frame</code> to <code>cv::Mat</code> right at the beginning of <code>Frame Processing</code>.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">Mat</span>(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).<span class="built_in">copyTo</span>(kinect_mat);</span><br><span class="line">cv::<span class="built_in">flip</span>(kinect_mat, kinect_mat, <span class="number">1</span>);</span><br><span class="line">rgb_mat = cv::Mat::<span class="built_in">zeros</span>(kinect_mat.<span class="built_in">size</span>(),CV_8UC3);</span><br><span class="line"><span class="built_in">mixChannels</span>(kinect_mat, rgb_mat, &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: <code>libfreenect2::Frame</code> contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a <code>mixChannels()</code> here to reduce the last one.</p></blockquote><hr><h3 id="2-opencv-calibration">2. OpenCV Calibration</h3><p>To be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. <a href="https://github.com/ultralytics/yolov5">YoloV5</a>) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a <a href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">Perspective-n-Point(aka <code>PnP</code>)</a> problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally.</p><p>However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task.</p><p>We perform 4 steps to meet the need.</p><h4 id="2-1-collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still">2.1. Collect multiple frames where the camera and chessboard are relatively still.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(STATE == STATE_START_CALIBRATION)</span><br><span class="line">  cali_frames.<span class="built_in">push_back</span>(rgb_mat);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: We use <code>STATE</code> to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM).</p></blockquote><h4 id="2-2-run-findpattern-to-obtain-feature-points-position-in-2d-pixel-coordinate-system">2.2. Run <code>findPattern</code> to obtain feature points’ position in 2D-pixel-coordinate-system.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; point_buff;</span><br><span class="line"><span class="type">int</span> board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;</span><br><span class="line"><span class="type">int</span> found = <span class="built_in">findChessboardCorners</span>(rgb_mat, boardSize, point_buff, board_flag);</span><br></pre></td></tr></table></figure><blockquote><p>Note that: We use <code>PThread</code> to accelerate the process, finding patterns in all collected frames at once.</p></blockquote><h4 id="2-3-collect-all-2d-information-and-calculate-3d-world-coordinate-system-information">2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> found=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;++i)&#123;</span><br><span class="line">  <span class="type">void</span> * ret;</span><br><span class="line">  <span class="built_in">pthread_join</span>(thread_ids[i], &amp;ret);</span><br><span class="line">  runCalibrationRet retVal = *(runCalibrationRet*) ret;</span><br><span class="line">  <span class="keyword">if</span>(retVal.found)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!found)</span><br><span class="line">      d2s = retVal.d2;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;d2s.<span class="built_in">size</span>();j++) d2s[j] += retVal.d2[j];</span><br><span class="line">    found ++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;boardSize.height; ++i)</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;boardSize.width; ++j)</span><br><span class="line">    d3s.<span class="built_in">push_back</span>(<span class="built_in">Point3f</span>(j*squareSize, i*squareSize, <span class="number">0</span>));</span><br></pre></td></tr></table></figure><blockquote><p>Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system.</p></blockquote><h4 id="2-4-wrap-them-up-and-perform-solvepnp-to-get-rvec-and-tvec">2.4. Wrap them up and perform <code>solvePnP</code> to get <code>rvec</code> and <code>tvec</code></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">solvePnP</span>(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec);</span><br></pre></td></tr></table></figure><blockquote><p>Note that:<code>camera_matrix</code> and <code>dist_coeffs</code> are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it.</p></blockquote><h4 id="2-5-to-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained">2.5. To obtain camera position, we still need another step that takes both <code>rvec</code> and <code>tvec</code> as input and <code>camera_position</code> would be obtained.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> type_tv = tv.<span class="built_in">type</span>();</span><br><span class="line"><span class="function">Mat <span class="title">rvf</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="comment">// Convert from vector rv(3x1) to matrix rotation(3x3)</span></span><br><span class="line"><span class="built_in">Rodrigues</span>(rv, rvf);</span><br><span class="line"><span class="comment">// The Inversed Matrix</span></span><br><span class="line"><span class="function">Mat <span class="title">rvf_1</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="built_in">invert</span>(rvf, rvf_1, DECOMP_SVD);</span><br><span class="line">Mat Position = rvf_1 * (-tv);</span><br><span class="line"><span class="function">Point3f <span class="title">p</span><span class="params">(Position)</span></span>;</span><br></pre></td></tr></table></figure><blockquote><p>Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: <a href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">OpenCV <code>Pnp</code> reference</a>.</p></blockquote><blockquote><p>Also note that: <code>Rodrigues</code> is essential, the output, <code>rvec</code> is (3x1), reference: <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac">Rodrigues</a></p></blockquote><hr><h3 id="3-object-detection">3. Object Detection</h3><p>According to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous <a href="https://github.com/ultralytics/yolov5">YoloV5 Project</a>.</p><p>The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: <a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train on Custom Data</a>). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs <code>cv::dnn</code> that load <code>.onnx</code> models and can run forward actions, or deduction.</p><p>In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs.</p><h4 id="3-1-dataset-creation">3.1. Dataset Creation</h4><p>Simply follow steps described on <a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">this page</a>. The output should be similar as below:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data.yaml</span><br><span class="line">├── README.dataset.txt</span><br><span class="line">├── README.roboflow.txt</span><br><span class="line">├── test</span><br><span class="line">├── train</span><br><span class="line">└── valid</span><br><span class="line">9 directories, 382 files</span><br></pre></td></tr></table></figure><h4 id="3-2-train-model">3.2 Train model</h4><p>Simply put datasets in yolov5 directory and perform the following command and sit back to wait for the results</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure><blockquote><p>Note that: You should clone yolov5 repo before training and of course set up python environment.</p></blockquote><h4 id="3-3-convert-model-to-onnx">3.3 Convert model to <code>.onnx</code></h4><p>YoloV5 takes PyTorch as backend, thus, the models are saved as <code>.pt</code> format. However, <code>cv::dnn</code> prefers <code>.onnx</code> format. Thus, a conversion shall be performed.</p><p>At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV <code>dnn</code> module. I have posted an <a href="https://github.com/ultralytics/yolov5/issues/10575">Issue</a> on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project.</p><p>After the correction of Dependency, we perform the following command to export <code>.onnx</code> from <code>.pt</code>.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --weights yolov5s.pt --include onnx</span><br></pre></td></tr></table></figure><h4 id="3-4-load-onnx-with-opencv">3.4 Load <code>.onnx</code> with OpenCV</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv::dnn::Net net = cv::dnn::<span class="built_in">readNetFromONNX</span>(model_path);</span><br><span class="line">net.<span class="built_in">setPreferableBackend</span>(cv::dnn::DNN_BACKEND_OPENCV);</span><br><span class="line">net.<span class="built_in">setPreferableTarget</span>(cv::dnn::DNN_TARGET_CPU);</span><br></pre></td></tr></table></figure><h4 id="3-5-format-input-data">3.5 Format Input Data</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat blob;</span><br><span class="line">int col = frame.cols;</span><br><span class="line">int row = frame.rows;</span><br><span class="line">int _max = max(col, row);</span><br><span class="line">input_img = cv::Mat::zeros(_max, _max, CV_8UC3);</span><br><span class="line">frame.copyTo(input_img(cv::Rect(0, 0, col, row)));</span><br><span class="line">cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false);</span><br></pre></td></tr></table></figure><h4 id="3-6-forward-network">3.6 Forward Network</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.setInput(input);</span><br><span class="line">std::vector&lt;cv::Mat&gt; outputs;</span><br><span class="line">net.forward(outputs, net.getUnconnectedOutLayersNames());</span><br></pre></td></tr></table></figure><h4 id="3-7-format-the-output">3.7 Format the output</h4><p>The output of the model, as the result of network-forwarding, is defined as follows:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> structure of `output.data`</span><br><span class="line">┌─┬─┬─┬─┬─┬────────┬─────────────────────►</span><br><span class="line">│0│1│2│3│4│5 ......│dimensions</span><br><span class="line">├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►</span><br><span class="line">│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..</span><br><span class="line">└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──►</span><br><span class="line"> c: confidence</span><br></pre></td></tr></table></figure><p>Basically, it is an array that can be accessed via its address. The code is too large to be shown here.</p><h4 id="3-8-transfer-output-to-detection">3.8 Transfer Output to <code>Detection</code></h4><p>For easy access, we transfer the output to the following data format.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Detection&#123;</span><br><span class="line">int       class_id;   // Result&#x27;s class id</span><br><span class="line">float     confidence; // Probability</span><br><span class="line">cv::Rect  box;        // Where it is</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="3-9-draw-boxes-around-targets">3.9 Draw Boxes around Targets</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int detection_size = output.size();</span><br><span class="line">for(int i=0;i&lt;detection_size;++i)&#123;</span><br><span class="line">auto detection = output[i];</span><br><span class="line">auto box = detection.box;</span><br><span class="line">auto class_id = detection.class_id;</span><br><span class="line">const auto color = color_list[class_id%color_list.size()];</span><br><span class="line">cv::rectangle(frame, box, color, 2);</span><br><span class="line">cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED);</span><br><span class="line">cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="4-plane-transformation">4. Plane Transformation</h3><p>Finally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: <a href="https://www.researchgate.net/figure/Display-of-Various-Coordinate-Systems-for-a-Computer-Vision-System-i-i-i-i-1-i_fig2_337311806">ResearchGate</a>)</p><p><img src="img/image_2022-12-31-00-10-18.png" alt="Computer Vision Model"></p><p>Take the example of the transformation of Point <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">^1p_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. The 3D position of it can be anywhere on the line of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><msup><mtext> </mtext><mn>1</mn></msup><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">O\ ^1p_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.008548em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord"><span class="mspace"> </span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, not necessary be at Point <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mi>w</mi></msup><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">^wp_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.858832em;vertical-align:-0.19444em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> unless we require the 3D position lies on a particular plane.</p><p>Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>O</mi><mi>Y</mi></mrow><annotation encoding="application/x-tex">XOY</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span> axis and not provide the height.</p><p>Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: <a href="https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae"><code>getPerspectiveTransformation</code></a>. According to the definition below, the input <code>src</code> and <code>dst</code> must be vertices of a quadrangle. And the return value shall be the transformation matrix.</p><p><img src="img/image_2022-12-31-00-19-10.png" alt=" definition"></p><h4 id="4-1-calculate-transformation-matrix">4.1 Calculate Transformation Matrix</h4><p>In our project, we take 4 vertices of the chessboard to be the input.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; desk;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> d3:d3s) desk.<span class="built_in">push_back</span>(<span class="built_in">Point2f</span>(d3.x,d3.y));</span><br><span class="line">Point2f in[<span class="number">4</span>];</span><br><span class="line">Point2f out[<span class="number">4</span>];</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> helper(_in, in_) \</span></span><br><span class="line"><span class="meta">_in[0] = in_[i0]; \</span></span><br><span class="line"><span class="meta">_in[1] = in_[i1]; \</span></span><br><span class="line"><span class="meta">_in[2] = in_[i2]; \</span></span><br><span class="line"><span class="meta">_in[3] = in_[i3]</span></span><br><span class="line"><span class="type">int</span> i0 = <span class="number">0</span>, i1 = boardSize.width<span class="number">-1</span>;</span><br><span class="line"><span class="type">int</span> i2 = boardSize.width * (boardSize.height<span class="number">-1</span>);</span><br><span class="line"><span class="type">int</span> i3 = boardSize.width * boardSize.height - <span class="number">1</span>;</span><br><span class="line"><span class="built_in">helper</span>(in, d2s);</span><br><span class="line"><span class="built_in">helper</span>(out, desk);</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> helper</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> According to reference (Opencv), </span></span><br><span class="line"><span class="comment">// getPerspectiveTransform takes quadrangle vertices in the source image</span></span><br><span class="line">pix23D = <span class="built_in">getPerspectiveTransform</span>(in, out);</span><br></pre></td></tr></table></figure><h4 id="4-2-calculate-corresponding-3d-position">4.2 Calculate Corresponding 3D Position</h4><p><code>perspectiveTransformation</code> have already done for us.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; out;</span><br><span class="line">vector&lt;Point2f&gt; in; in.push_back(p2);</span><br><span class="line">cv::perspectiveTransform(in, out, pix23D);</span><br><span class="line">Point3f ret = Point3f(out[0].x,out[0].y,0);</span><br></pre></td></tr></table></figure><hr><h2 id="summary">Summary</h2><p>Till now, the Project is half-way finished and seems pretty simple, only taking advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan.</p><p>The next aim of our project would be human-skeleton detection and action deduction with it. And finally, adding some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.</p>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 记录 </tag>
            
            <tag> KinectV2 </tag>
            
            <tag> Opencv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Source Backup for Disk Update</title>
      <link href="/2022/12/12/Source-Backup-for-Disk-Update/"/>
      <url>/2022/12/12/Source-Backup-for-Disk-Update/</url>
      
        <content type="html"><![CDATA[<h2 id="background">Background</h2><ul><li>作为大学生，只有一台 Thinkpad T14 Gen 1 的笔记本却安装了 Arch Linux，而由于课程需要仍保留了大部分 Windows 11 的系统，长期以来的使用，让 512 GB 的存储空间告急，于是打算更换一块 1T 的固态硬盘。在此对磁盘更换做一个记录。</li></ul><span id="more"></span><h2 id="准备">准备</h2><h3 id="文件分类">文件分类</h3><ul><li><p>第一大类：系统文件，</p><ul><li>如 Windows 等 Operating Systems 的文件</li><li>不考虑保存，但需要先查清除如何对 Windows 11 下的 Office 全家桶进行恢复。</li></ul></li><li><p>第二大类：工程文件；</p><ul><li>2-1. 项目、代码等<ul><li>存入 github 代码库，将不必要保存的 tmp 文件删除</li><li>写好 README 文档</li></ul></li><li>2-2. 安装包、dependency<ul><li>丢弃，实际使用时再安装</li></ul></li></ul></li><li><p>第三大类：资源文件：</p><ul><li>3-1. 课程文件等<ul><li>打包按时间排序</li><li>项目类文件保存两份</li></ul></li><li>3-2. 配置文件<ul><li>重要配置打包或上传 github</li></ul></li><li>3-3. 字体<ul><li>不能打包，但需要确定安装和配置方案</li></ul></li><li>3-3. 其他资源<ul><li>写个 README</li></ul></li></ul></li></ul><h3 id="计划">计划</h3><ol><li><p>文件备份</p><ul><li>根据上述分类进行备份</li><li>2022-12-12 23:38 DONE</li></ul></li><li><p>确定安装和磁盘分区</p><ul><li>安装 512 GB 的 Windows 11 系统 和 512 GB 的 Linux 系统</li><li>Linux 安装 DWM + Xorg （需要先写好安装文档以便查看）</li></ul></li><li><p>购置固态硬盘</p><ul><li>选取三星 “980 Pro PCIe 4.0 NVMe M.2”</li><li>2022-12-12 23:38 次日到貨</li></ul></li><li><p>对需要安装的系统和软件、配置写教程</p></li></ol><table><thead><tr><th>待安装（配置）的内容</th><th>教程</th><th>备注</th></tr></thead><tbody><tr><td>Arch Linux</td><td><a href="https://www.viseator.com/2017/05/17/arch_install/">viseator的博客</a></td><td></td></tr><tr><td>dwm</td><td></td><td>拷贝文件即可, 应当包括 autostart</td></tr><tr><td>dmenu</td><td></td><td>同上</td></tr><tr><td>vim / nvim</td><td></td><td>同上</td></tr><tr><td>中文输入法</td><td><a href="https://www.viseator.com/2017/07/02/arch_more/">中文輸入法等安裝</a></td><td></td></tr><tr><td>yay</td><td><a href="https://aur.archlinux.org/yay.git">yay git repo</a></td><td></td></tr><tr><td>clash</td><td><a href="https://chrisvicky.github.io/2022/12/09/Setup-Systemd-for-clash-Proxy/">我的博客</a></td><td></td></tr><tr><td>navicat</td><td></td><td>已經完成拷貝</td></tr><tr><td>apifox</td><td></td><td></td></tr><tr><td>vivado</td><td><a href="https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/vivado-design-tools/archive.html">Vivado HLx 2019.2: All OS installer Single-File Download (TAR/GZIP - 26.55 GB)</a></td><td></td></tr><tr><td>libfreenect2</td><td><a href="https://github.com/OpenKinect/libfreenect2">libfreenect2 Github</a>!!</td><td>十分重要</td></tr><tr><td>opencv</td><td></td><td>yay 将就用一下先</td></tr><tr><td>virtualbox</td><td><a href="https://chrisvicky.github.io/2022/11/15/deploy-hadoop/">我的博客</a></td><td></td></tr></tbody></table><hr>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 记录 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Setup Systemd for clash Proxy</title>
      <link href="/2022/12/09/Setup-Systemd-for-clash-Proxy/"/>
      <url>/2022/12/09/Setup-Systemd-for-clash-Proxy/</url>
      
        <content type="html"><![CDATA[<ul><li>Since I live in China mainland, some specific websites can’t get accessed without a <code>Vpn</code> service running on the laptop. I managed to get one using <a href="https://github.com/Dreamacro/clash">clash</a> and <a href="ikuuu.live">configuration</a>. However, I have to run <code>./clash -d .</code> manually every-time I need access, which is inconvenient. Since I’ve been using Linux, I did some search and managed to set up a system task that runs automatically after booting. Here is the Memo</li></ul><span id="more"></span><ol><li>Add <code>clash@.service</code> in <code>/usr/lib/systemd/system/</code> or in <code>/etc/systemd/system/</code></li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2022-12-09-20-23-23.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Example</div></center><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=A rule based proxy in Go for %i.</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=%i</span><br><span class="line">Restart=on-abort</span><br><span class="line">ExecStart=/home/christopher/.config/clash/clash</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Use <code>systemctl</code> to enable and start mission.</li></ol><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2022-12-09-20-38-10.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Systemd Management</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> VPN </tag>
            
            <tag> Clash </tag>
            
            <tag> Systemd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSH Penetration Setup Memo</title>
      <link href="/2022/12/01/ssh/"/>
      <url>/2022/12/01/ssh/</url>
      
        <content type="html"><![CDATA[<h2 id="background">Background</h2><ul><li>We have deployed a web application demo on a computer whose network access is limited within school and cannot be accessed from devices outside. Plus that we know from the man page of ssh that ssh -R could perform a proxy that transfers data stream from one port to another, thus, we decide to make use of a server bought from <a href="https://aliyun.com/">Aliyun</a> as a repeater, or bridge, that connects our clients to the application.</li></ul><span id="more"></span><h2 id="how-to-do-it">How to do it</h2><ul><li><p>For specification, we call the computer with the deployment of our web application as <code>A</code> and its port as <code>AP</code> while the server is called <code>B</code>, <code>BP1</code>, <code>BP2</code>.</p></li><li><p>First on computer <code>A</code></p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -fCNR `BP1`:localhost:`AP` -o ServerAliveInterval=60 serverName@serverIP</span><br></pre></td></tr></table></figure><ul><li>On Server</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -fCNL 0.0.0.0:`BP2`:localhost:`BP1` localhost</span><br></pre></td></tr></table></figure><ul><li>It works like this</li></ul><!-- ``` --><!--                       Server B                                    Our Computer in School --><!--                 ┌──────────────────────┐                    ┌───────────────────────────┐ --><!--                 │                      │                    │                           │ --><!--       ─────► ┌──┴──┐ ──────────────► ┌─┴───┐ ──────────► ┌──┴─┐  bind                   │ --><!-- requests     │ BP2 │  inside Server  │ BP1 │  Via SSH    │ AP │ ◄────► Web Application  │ --><!--       ◄───── └──┬──┘ ◄────────────── └─┬───┘ ◄────────── └──┬─┘                         │ --><!--                 │                      │                    │                           │ --><!--                 └──────────────────────┘                    └───────────────────────────┘ --><!----><!-- ``` --><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-12-01-16-33-31.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Graph</div></center><ul><li>Access through internet</li></ul><center>  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"    src="/img/image_2022-12-01-16-23-35.png"><br>  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Access</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh </tag>
            
            <tag> learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add Extra Memory to Thinkpad T14</title>
      <link href="/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/"/>
      <url>/2022/11/18/Add-Extra-Memory-to-Thinkpad-T14/</url>
      
        <content type="html"><![CDATA[<p>Working on hard projects with Hadoop cluster really runs out the limit of my poorly configured laptop. Therefore, I decide to add an extra memory card to it to boost the performance.</p><span id="more"></span><h2 id="1-check-the-hardware-information">1. Check the hardware information</h2><ul><li>Before cracking down the shell, we should figure out whether my laptop has a spare set for the extra memory and what kinds of memory card should I buy.</li><li>To do that, we perform the following command to monitor the hardware status</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dmidecode memory &gt; memory.log</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-12-06-21-38-11.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">command output</div></center><blockquote><p>Note: I’ve accidentally removed <code>memory.log</code> which generated before adding the extra memory, and this screenshot is taken after the card is inserted.</p></blockquote><ul><li>According to the output, the maximum Capacity of Memory of my laptop is 32 GB, meaning that it is capable of accepting an extra memory of 16 GB.</li><li>Another important information here is the speed. Because both memory cards will be accessed with the same bus, the slower speed could be a bottle net.</li></ul><blockquote><p>I bought a memory card from Samsung with the same speed and data width for 300 RMB on Taobao.</p></blockquote><h2 id="2-opening-the-laptop">2. Opening the Laptop</h2><ul><li>Using tools from the shop, the laptop is rather easy to tear. The only thing to mention here is that, be gentle not to break the machine down and after insertion, do not fix the screw before the evaluation process succeeds.</li></ul><blockquote><p>In fact, the work can be down with a screwdriver, and a student ID card.</p></blockquote><h2 id="3-evaluation">3.  Evaluation</h2><ul><li>After insertion, boot the machine and enter the Operating System to check whether the extra Memory works well.</li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-18-11-20-48.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Successfully Boot up Heavy Virtual Machines after Installation of the Additional 16GB Memory Card</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DIY </tag>
            
            <tag> Hardware </tag>
            
            <tag> Performance Improve </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deploy Hadoop with VirtualBox</title>
      <link href="/2022/11/15/deploy-hadoop/"/>
      <url>/2022/11/15/deploy-hadoop/</url>
      
        <content type="html"><![CDATA[<ul><li>This post is a reminder of hadoop deployment on Arch Linux and virtualbox (with vagrant)</li><li><a href="https://github.com/ChrisVicky/hadoop-vm">Related Github Repo contains resource:</a></li></ul><span id="more"></span><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-16-00-30-34.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">system information</div></center><hr><h2 id="pre-requirement">Pre-Requirement</h2><ol><li>VirtualBox</li><li>vagrant</li></ol><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-23-47-12.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">virtualbox and vagrant</div></center><ol start="3"><li>Access to Internet<ul><li>The base OS we choose to deploy Hadoop Cluster is Ubuntu 16, thus we should be able to fetch “ubuntu/xenial64” from <a href="https://app.vagrantup.com/boxes/search">vagrant cloud</a>.</li><li>Also, the first step in your vm is to update and install necessary applications such as <code>ssh</code>, <code>rsync</code> and <code>vim</code>.</li></ul></li></ol><h2 id="installation">Installation</h2><h3 id="1-ingredients-checkup">1. Ingredients checkup</h3><ul><li>Make sure the Directory has the following structure</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── cache                               -- Files to replace in VM</span><br><span class="line">│   ├── core-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/core-site.xml in VM</span><br><span class="line">│   ├── hadoop-2.9.0.tar.gz             -- hadoop </span><br><span class="line">│   ├── hdfs-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/hdfs-site.xml in VM </span><br><span class="line">│   ├── hosts                           -- replace /usr/hosts in VM</span><br><span class="line">│   ├── jdk-8u161-linux-x64.tar.gz      -- jdk package</span><br><span class="line">│   ├── mapred-site.xml                 -- replace /usr/local/hadoop/etc/hadoop/mapred-site.xml</span><br><span class="line">│   ├── scala-2.11.8.tgz                -- scala package</span><br><span class="line">│   ├── sources.list                    -- replace /etc/apt/sources.list -&gt; from https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</span><br><span class="line">│   ├── spark-2.3.0-bin-hadoop2.7.tgz   -- spark package</span><br><span class="line">│   └── yarn-site.xml                   -- replace /usr/local/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">├── hadoop+spark集群平台搭建.pptx       -- ppt instruction for Hadoop + spark on VMware</span><br><span class="line">├── Hadoop集群安装手册.pdf              -- PDF instruction for Hadoop on VMware based on CentOS</span><br><span class="line">├── hadoop集群搭建.pptx                 -- ppt instruction for Hadoop on VMware based on Ubuntu -- Instruction for this Virtualbox Version</span><br><span class="line">├── img                                 -- IMGs in this readme file</span><br><span class="line">├── init.sh                             -- Scripts to execute when VM first starts</span><br><span class="line">├── README.md                           -- This file</span><br><span class="line">└── Vagrantfile                         -- VM Configurations</span><br><span class="line"></span><br><span class="line">2 directories, 26 files</span><br></pre></td></tr></table></figure><h3 id="2-check-init-sh-and-vagrantfile-for-certain-configurations">2. Check <a href="http://init.sh">init.sh</a> and Vagrantfile for certain configurations</h3><ol><li>The default configuration for Virtual Machines is written in <code>Vagrantfile</code>. Modifications can be made by changing the code directly.</li><li>Check up <a href="http://init.sh">init.sh</a> for more setups.</li></ol><h3 id="3-execute-vagrant-up">3. Execute Vagrant Up</h3><ul><li>In the directory shown above, execute <code>vagrant up</code> to setup and boot your vm.</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant up </span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-04-25.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">Commands to Execute</div></center><ul><li>It should take a while. So have a cup of tea and when everything is settled, check your virutal machine with either <code>vagrant status</code> or <code>virtualbox user interface</code></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant status</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-07-10.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">vagrant status</div></center><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-08-01.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">virtualbox user interface</div></center><h3 id="4-ssh-configurations">4. ssh configurations</h3><ol><li>use <code>vagrant ssh master</code> to enter master virtual machine.</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh master</span><br></pre></td></tr></table></figure><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-11-20.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">vagrant ssh master</div></center><ol start="2"><li>Append public keys to authorized_keys by <code>cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys</code></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /vagrant/cache/authorized_keys &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><blockquote><p>Note that all three public keys have been generated and pasted in <code>/vagrant/cache/authorized_keys</code> by commands in <code>init.sh</code> and <code>Vagrantfile</code>.</p></blockquote><ol start="3"><li><p>Ssh configuration should be done in both slaves as well.</p></li><li><p>Varify ssh configuration by executing <code>ssh slave1</code> in <code>master</code> virtual machine. You sohuld log into slave1 without entering password.</p></li></ol><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-22-03.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">log into slave1 without password after ssh configuration</div></center><h2 id="deploy-and-varify-hadoop">Deploy and Varify Hadoop</h2><h3 id="1-deploy-hadoop">1. Deploy Hadoop</h3><ul><li>run <code>hadoop namenode -format</code> in <code>master</code> virtual machine to configure node information.</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure><ul><li>run <code>start-all.sh</code> to start deployment. Always remember to run <code>stop-all.sh</code> before virtual machine shutdown.</li></ul><h3 id="2-varification">2. Varification</h3><ul><li>run <code>hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5</code> in directory <code>/usr/local/hadoop/share/hadoop/mapreduce</code> to varify.</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hadoop/share/hadoop/mapreduce</span><br><span class="line">hadoop jar hadoop-mapreduce-examples-2.9.0.jar pi 5 5</span><br></pre></td></tr></table></figure><blockquote><p>A number relatively close to pi is then presented. To be more accurate on the result, try running <code>pi 10 10000</code> which takes a longer period.</p></blockquote><h3 id="3-user-interface">3. User Interface</h3><ul><li>Hadoop Environment Configuration on <code>IP:50070</code> where IP is the static IP for Master. And on <code>IP:8088</code> where IP is the static IP of Master and <code>8088</code> can be configured in those <code>.xml</code> files.</li></ul><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-33-19.png"><br><br>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-15-22-33-51.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">UI</div></center>]]></content>
      
      
      <categories>
          
          <category> Post </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Big Data </tag>
            
            <tag> 課程作業 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About Me</title>
      <link href="/2022/11/12/aboutme/"/>
      <url>/2022/11/12/aboutme/</url>
      
        <content type="html"><![CDATA[<p>Hello, and welcome to my blog. An introduction about myself is written here with my scarce English vocabulary.</p><p>I am a 20-year-old college student majoring in Computer Science who dreamed about designing a computer architecture that would contribute to the development of advanced technology. I once was a Windows user but was obsessed with Linux immediately after my very first installation of Ubuntu for my scientific research requirement. After about 3 months, when I learnt about the architecture and components of computers as well as the Operating System in school, I started to switch from Ubuntu to Arch Linux, which, until now, is my first choice every time I turn the power on.</p><span id="more"></span><center>    <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"        src="/img/image_2022-11-14-12-20-13.png"><br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">My Arch Linux & Dwm</div></center><p>I said I was passionate and wanted to design computer architectures. However, as a college student with limited choices of topic and professors, the project that I have conducted is related to another area, UAV (unmanned aerial vehicle) swam control, in which we proposed a de-centralised method and algorithm that enable a group of drones to explore an unknown environment and make decisions based on the algorithm that is embedded in every individual.</p><p>Right now, I am involved in a project that develops a system with multiple technologies, which, from my point of view, shall be better for industrial rather than for scientific purposes since the idea and method we conducted are nothing special but rather old-school while the system that we design is somehow new to the industrial world to the best of our knowledge.</p><p>All those experience of ‘doing lab’ and ‘debuging’ seems, from my point of view, to be a scientific skill training. That is, the knowledge is not the centric but the habit and lessons related to the insight of ‘doing lab’ is what I gains through it.</p><p>Other experiences includes ACM and Web Backend Development will be written later.</p>]]></content>
      
      
      <categories>
          
          <category> Diary </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
